# íŒŒì¼: Search_Dewey.py )
"""ë²„ì „: v1.1.0
ìˆ˜ì • ë‚´ì—­:
    DeweyClientê°€ DB ì¡°íšŒ ì‹œ DatabaseManager ëŒ€ì‹  SearchQueryManagerë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì˜ì¡´ì„± êµ¬ì¡°ë¥¼ ì˜¬ë°”ë¥´ê²Œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
    Negative Cache ë¡œì§ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.
        API ì¡°íšŒ ì‹œ WebDeweyì— ì—†ëŠ” ë²ˆí˜¸ì¼ ê²½ìš°, {"exists": false} í˜•íƒœë¡œ ìºì‹œì— ê¸°ë¡í•©ë‹ˆë‹¤.
        ìºì‹œ ì¡°íšŒ ì‹œ 'ì—†ìŒ' ê¸°ë¡ì´ 3ê°œì›” ì´ë‚´ì¼ ê²½ìš° API í˜¸ì¶œì„ ê±´ë„ˆë›°ë„ë¡ í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.( 'Time To Live' ë˜ëŠ” ì¤„ì—¬ì„œ 'TTL)
ì•„ë˜ëŠ” DLD APIê°€ ë³´ë‚´ì˜¨ JSON ìƒ˜í”Œ
{"id":"https://id.oclc.org/worldcat/ddc/E37yQ6jKcj8VqkypgYVCyj3b63","altLabel":{"sv":["Internetresurser","Webbplatser--bibliografier","Webbplatser--informationssystem","Webbdatabaser--informationssystem"],"de":["Websites--Informationssysteme","Webdatenbanken--Informationssysteme","Websites--Bibliografien","Internetquellen"],"no":["Internettressurser","Nettsteder--bibliografier","Webdatabaser--informasjonssystemer","Nettsteder--informasjonssystemer"],"en":["Web sites--bibliographies","Web sites--information systems","Web databases--information systems","Internet resources"],"it":["Siti web--sistemi informativi","Risorse di Internet","Siti web--bibliografie","Database web--sistemi informativi"],"fr":["Sites Web--systÃ¨mes d'information","Ressources Internet","Sites Web--bibliographies","Bases de donnÃ©es Web--systÃ¨mes d'information","Portails Web--bibliographies"]},"related":["https://id.oclc.org/worldcat/ddc/E3pdK7wtKYVRbJKdFTyrg9rTd8","https://id.oclc.org/worldcat/ddc/E3h74Y87yHXrmHRPwqhrghtKvH"],"scopeNote":{"en":["Class here directories of web sites, portals"],"no":["Her: Registre over nettsteder; nettportaler"],"sv":["Klassificera hÃ¤r register Ã¶ver webbplatser, nÃ¤tportaler"],"de":["Hier auch: Webverzeichnisse, Portale"],"fr":["Classer ici les rÃ©pertoires de sites Webâ€¯; les portails"],"it":["Classificare qui le cartelle dei siti web, i portali"]},"prefLabel":{"fr":"Sites Web","it":"Siti web","sv":"Webbplatser","no":"Nettsteder","de":"Websites","en":"Web sites"},"notation":"025.0422","historyNote":{"de":["Erweitert aus 025.04, 2008-08, Edition 22"],"sv":["Webbplatser under 025.04, 2008-08, Edition 22"],"no":["Nettsteder utvidet fra 025.04, 2008-08, Edition 22"],"en":["Web sites continued from 025.04, 2008-08, Edition 22"],"it":["I siti web specificati da 025.04, 2008-08, Edition 22"],"fr":["Sites Web prolongÃ© Ã  partir de 025.04, 2008-08, Edition 22"]},"type":"Concept","modified":"2021-01-19T07:51:53Z","inScheme":"https://id.oclc.org/worldcat/ddc/","created":"2008-08-28","broader":"https://id.oclc.org/worldcat/ddc/E3BfQcQb8xjtVxb8Br6p8xyRPP","narrower":"https://id.oclc.org/worldcat/ddc/E3M6jGBd49y8kFHCxpwTXMP9Jr","@context":"https://id.oclc.org/worldcat/ddc/context.json"}
"""
import base64
import time
import json
import logging
import re
from typing import Dict, List, Tuple, Optional
import requests
import threading
from collections import OrderedDict
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from search_query_manager import SearchQueryManager
from text_utils import clean_ksh_search_input

TOKEN_URL = "https://oauth.oclc.org/token"
URL_MAP_API = "https://id.oclc.org/worldcat/ddc/api/url?ddc={ddc}"
DEFAULT_TIMEOUT = 15
RETRY_SECONDS = 0.6

log = logging.getLogger(__name__)
log.setLevel(logging.INFO)


class DeweyClient:
    def __init__(self, db_manager):
        self.db = db_manager
        self.query_manager = SearchQueryManager(db_manager)
        self._token_cache: Tuple[Optional[str], float] = (None, 0.0)
        # -------------------
        # âœ… ì„¸ì…˜ LRU (notation â†’ payload) + ìŠ¤ë ˆë“œ ë½
        self._lru_capacity = 256
        self._lru_cache: "OrderedDict[str, dict]" = OrderedDict()
        self._lru_lock = threading.Lock()
        # ë¶€ëª¨ ì½”ë“œ ê³„ì‚° ë©”ëª¨ì´ì œì´ì…˜(ê°€ë²¼ì›€)
        self._parent_cache: dict[str, list[str]] = {}
        # âœ… í† í° ë°œê¸‰ ë™ì‹œì„± ì œì–´ìš© ë½
        self._token_lock = threading.Lock()
        # -------------------

    # --- OAuth ---
    def _fetch_token(self) -> str:
        cid, secret = self.db.get_dewey_api_credentials()
        if not cid or not secret:
            raise ConnectionRefusedError("Dewey API ìê²©ì¦ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        basic = base64.b64encode(f"{cid}:{secret}".encode("utf-8")).decode("ascii")
        headers = {
            "Authorization": f"Basic {basic}",
            "Content-Type": "application/x-www-form-urlencoded",
        }
        data = {"grant_type": "client_credentials", "scope": "deweyLinkedData"}

        resp = requests.post(
            TOKEN_URL, headers=headers, data=data, timeout=DEFAULT_TIMEOUT
        )
        resp.raise_for_status()
        payload = resp.json()
        token = payload.get("access_token")
        if not token:
            raise RuntimeError("Dewey í† í° ì‘ë‹µì— access_tokenì´ ì—†ìŠµë‹ˆë‹¤.")
        return token

    def _get_token(self) -> str:
        # âœ… [ë™ì‹œì„± ê°œì„ ] Double-checked locking íŒ¨í„´
        # ë¨¼ì € ë½ ì—†ì´ í™•ì¸ (ë¹ ë¥¸ ê²½ë¡œ)
        token, ts = self._token_cache
        if token and (time.time() - ts) < 2580:  # 43ë¶„
            return token

        # í† í°ì´ ë§Œë£Œë˜ì—ˆê±°ë‚˜ ì—†ìœ¼ë©´ ë½ì„ íšë“í•˜ê³  ì¬í™•ì¸
        with self._token_lock:
            # ë‹¤ë¥¸ ì“°ë ˆë“œê°€ ì´ë¯¸ í† í°ì„ ë°œê¸‰ë°›ì•˜ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¬í™•ì¸
            token, ts = self._token_cache
            if token and (time.time() - ts) < 2580:
                return token

            # ì‹¤ì œ í† í° ë°œê¸‰ (ë½ ì•ˆì—ì„œë§Œ í•œ ë²ˆ ì‹¤í–‰)
            try:
                token = self._fetch_token()
                self._token_cache = (token, time.time())
                log.info("ìƒˆë¡œìš´ DDC API í† í° ë°œê¸‰ ì™„ë£Œ")
                return token
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 401:
                    log.error("DDC API ìê²©ì¦ëª… ì˜¤ë¥˜ - Client ID/Secret í™•ì¸ í•„ìš”")
                else:
                    log.error(f"DDC API í† í° ë°œê¸‰ HTTP ì˜¤ë¥˜: {e.response.status_code}")
                self._token_cache = (None, 0.0)
                raise
            except Exception as e:
                log.error(f"DDC API í† í° ë°œê¸‰ ì‹¤íŒ¨: {e}")
                self._token_cache = (None, 0.0)
                raise

    def _get_json(self, url: str) -> dict:
        # -------------------
        # 1) notation íŒŒì‹± (ì¿¼ë¦¬ ?ddc=, ê²½ë¡œ /ddc/api/url?ddc=, ì¶”í›„ payload í´ë°±)
        notation = None
        try:
            m = re.search(r"[?&]ddc=([\d.]+)", url) or re.search(
                r"/ddc/[^?]*\?ddc=([\d.]+)", url
            )
            if m:
                notation = m.group(1)
        except Exception:
            notation = None

        # 2) LRU ì¡°íšŒ
        if notation:
            with self._lru_lock:
                if notation in self._lru_cache:
                    payload = self._lru_cache.pop(notation)
                    self._lru_cache[notation] = payload
                    return payload

        # 3) DB ìºì‹œ(DDC ì½”ë“œ) ìš°ì„  ì¡°íšŒ
        if notation:
            # âœ… [ìˆ˜ì •] self.query_managerë¥¼ í†µí•´ ìºì‹œ ì¡°íšŒ
            raw = self.query_manager.get_dewey_by_notation(notation)
            if raw:
                try:
                    payload = json.loads(raw)
                    with self._lru_lock:
                        self._lru_cache[notation] = payload
                        if len(self._lru_cache) > self._lru_capacity:
                            self._lru_cache.popitem(last=False)
                    return payload
                except Exception as e:
                    log.warning(f"DDC ìºì‹œ JSON íŒŒì‹± ì‹¤íŒ¨({notation}): {e}")

        # 4) ê¸°ì¡´ IRI/URL í‚¤ ê¸°ë°˜ ìºì‹œ (í•˜ìœ„ í˜¸í™˜)
        # âœ… [ìˆ˜ì •] self.query_managerë¥¼ í†µí•´ ìºì‹œ ì¡°íšŒ
        cached_json = self.query_manager.get_dewey_from_cache(url)
        if cached_json:
            try:
                payload = json.loads(cached_json)
                n2 = payload.get("notation")
                if n2:
                    with self._lru_lock:
                        self._lru_cache[n2] = payload
                        if len(self._lru_cache) > self._lru_capacity:
                            self._lru_cache.popitem(last=False)
                return payload
            except Exception as e:
                log.warning(f"IRI ìºì‹œ JSON íŒŒì‹± ì‹¤íŒ¨({url}): {e}")

        # 5) API í˜¸ì¶œ - ğŸ¯ 401 ì˜¤ë¥˜ ìë™ ë³µêµ¬ ì¶”ê°€
        # ğŸ”¥ í† í°ì€ API í˜¸ì¶œ ì§ì „ì—ë§Œ ìš”ì²­ (ìºì‹œ íˆíŠ¸ ì‹œ ë¶ˆí•„ìš”í•œ í† í° ìš”ì²­ ë°©ì§€)
        headers = {"Authorization": f"Bearer {self._get_token()}"}
        for attempt in range(3):
            try:
                r = requests.get(url, headers=headers, timeout=DEFAULT_TIMEOUT)
                r.raise_for_status()
                payload = r.json()
                try:
                    ddc_code = payload.get("notation", "") or notation
                    iri = payload.get("@id", url)
                    if ddc_code and iri:
                        # âœ… [ìˆ˜ì •] self.query_managerë¥¼ í†µí•´ ìºì‹œ ì €ì¥
                        self.query_manager.save_dewey_to_cache(iri, ddc_code, r.text)
                    if ddc_code:
                        with self._lru_lock:
                            self._lru_cache[ddc_code] = payload
                            if len(self._lru_cache) > self._lru_capacity:
                                self._lru_cache.popitem(last=False)
                except Exception as e:
                    log.warning(f"Dewey ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
                return payload
            except requests.exceptions.HTTPError as e:
                # ğŸ¯ 401 ì˜¤ë¥˜ ì‹œ í† í° ê°±ì‹  í›„ ì¬ì‹œë„
                if e.response.status_code == 401:
                    log.warning(
                        f"401 Unauthorized ì˜¤ë¥˜ ê°ì§€ - í† í° ê°±ì‹  í›„ ì¬ì‹œë„ (ì‹œë„ {attempt + 1}/3)"
                    )
                    # í† í° ìºì‹œ ê°•ì œ ì´ˆê¸°í™”
                    self._token_cache = (None, 0.0)
                    # ìƒˆë¡œìš´ í† í°ìœ¼ë¡œ í—¤ë” ê°±ì‹ 
                    try:
                        headers = {"Authorization": f"Bearer {self._get_token()}"}
                        log.info("í† í° ê°±ì‹  ì™„ë£Œ - ì¬ì‹œë„")
                    except Exception as token_error:
                        log.error(f"í† í° ê°±ì‹  ì‹¤íŒ¨: {token_error}")
                        if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                            raise
                    if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œë„ 401ì´ë©´ ì˜ˆì™¸ ë°œìƒ
                        log.error("3íšŒ ì‹œë„ í›„ì—ë„ 401 ì˜¤ë¥˜ ì§€ì† - ìê²©ì¦ëª… í™•ì¸ í•„ìš”")
                        raise
                elif e.response.status_code == 429:
                    # Rate limit ì˜¤ë¥˜ - UI í”„ë¦¬ì§• ë°©ì§€ë¥¼ ìœ„í•´ ì¬ì‹œë„ ì—†ì´ ì¦‰ì‹œ ì‹¤íŒ¨
                    log.warning(
                        f"429 Rate Limit ì˜¤ë¥˜ - API ì œí•œ ë„ë‹¬ (DDC: {notation or url})"
                    )
                    raise  # ì¦‰ì‹œ ì˜ˆì™¸ ë°œìƒ
                else:
                    # ë‹¤ë¥¸ HTTP ì˜¤ë¥˜
                    if attempt == 2:
                        raise
                time.sleep(RETRY_SECONDS * (attempt + 1))
            except requests.exceptions.RequestException as e:
                # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“± ê¸°íƒ€ ìš”ì²­ ì˜¤ë¥˜
                if attempt == 2:
                    raise
                time.sleep(RETRY_SECONDS * (attempt + 1))
        return {}

    def _get_parent_codes(self, ddc: str) -> List[str]:
        parts = []
        current = ddc
        while True:
            parent = ""
            if "." in current:
                parent = current.rsplit(".", 1)[0]
            elif len(current) == 3:
                if current.endswith("00"):
                    parent = ""
                elif current.endswith("0"):
                    parent = current[0] + "00"
                else:
                    parent = current[:2] + "0"
            if parent and parent not in parts:
                parts.append(parent)
                current = parent
            else:
                break
        return parts

    def _fetch_url_job(self, url, result_list):
        try:
            if isinstance(url, str) and url.startswith("http"):
                result_list.append(self._get_json(url))
            elif isinstance(url, dict):
                result_list.append(url)
        except Exception as e:
            log.error(f"Concurrent fetch failed for {url}: {e}")

    def _fetch_many_concurrent(self, links: list) -> List[dict]:
        if not links:
            return []

        results = []
        threads = []
        lock = threading.Lock()

        def job_wrapper(url, res_list, thread_lock):
            try:
                data = self._get_json(url)
                with thread_lock:
                    if data:
                        res_list.append(data)
            except Exception as e:
                # 429 Rate Limit ë“±ì˜ ì˜¤ë¥˜ëŠ” ì¡°ìš©íˆ ë¬´ì‹œ (ì´ë¯¸ ë¡œê¹…ë¨)
                pass

        for link in links:
            if isinstance(link, str) and link.startswith("http"):
                thread = threading.Thread(
                    target=job_wrapper, args=(link, results, lock)
                )
                threads.append(thread)
                thread.start()
            elif isinstance(link, dict):
                results.append(link)

        for thread in threads:
            thread.join(timeout=20)

        return results

    def get_dewey_context_by_iri(self, iri: str) -> dict:
        return {"main": self._get_json(iri)}

    def _fetch_chain_upwards(self, concept: dict) -> List[dict]:
        """
        [ìµœì¢… ì•ˆì •í™” ë²„ì „] broader ì†ì„±ì„ ì¶”ì í•˜ì—¬ ìƒìœ„ ê³„ì¸µì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        """
        chain = []
        seen_ids = set()
        current_concept = concept
        for _ in range(10):  # ë¬´í•œ ë£¨í”„ ë°©ì§€
            if not current_concept or not isinstance(current_concept, dict):
                break
            concept_id = current_concept.get("@id")
            if not concept_id or concept_id in seen_ids:
                break

            seen_ids.add(concept_id)
            chain.append(current_concept)
            broader = current_concept.get("broader")

            if isinstance(broader, str) and broader.startswith("http"):
                try:
                    current_concept = self._get_json(broader)
                except Exception:
                    break
            elif isinstance(broader, dict):
                current_concept = broader
            else:
                break
        return list(reversed(chain))

    def get_dewey_context(self, ddc: str) -> dict:
        """
        [ìµœì¢… ì•ˆì •í™” ë²„ì „] broader ì¶”ì  ë°©ì‹ìœ¼ë¡œ DDC ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        (Negative Cache 3ê°œì›” ìœ íš¨ê¸°ê°„ ì ìš©)
        """
        from datetime import datetime, timedelta
        import json

        # 1ï¸âƒ£ DB ìºì‹œì—ì„œ ì „ì²´ ë°ì´í„° ë¨¼ì € ì¡°íšŒ (Positive & Negative Cache)
        iri = None
        main = None

        # âœ… [í•µì‹¬ ë¡œì§ 1] Negative Cache í™•ì¸
        cached_entry = self.query_manager.get_dewey_cache_entry(ddc)
        if cached_entry:
            cached_json, last_updated = cached_entry
            try:
                payload = json.loads(cached_json)
                if payload.get("exists") is False:
                    # Negative Cache í•­ëª© ë°œê²¬. ì €ì¥ ì‹œê° í™•ì¸
                    three_months_ago = datetime.now() - timedelta(days=90)
                    updated_time = datetime.fromisoformat(last_updated.split(".")[0])

                    if updated_time > three_months_ago:
                        # 3ê°œì›” ì´ë‚´ì˜ ìœ íš¨í•œ 'ì—†ìŒ' ìºì‹œ. API í˜¸ì¶œ ìƒëµ
                        log.info(
                            f"âœ… DDC {ddc}: Negative Cache HIT (3ê°œì›” ì´ë‚´). API í˜¸ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤."
                        )
                        raise ValueError(
                            f"'{ddc}' ì— ëŒ€í•œ DLD URLì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (Negative Cache)"
                        )
                    else:
                        # 3ê°œì›” ì§€ë‚œ 'ì—†ìŒ' ìºì‹œ. ì¬ê²€ì¦ í•„ìš”
                        log.warning(
                            f"âš ï¸ DDC {ddc}: Negative Cache ë§Œë£Œ (3ê°œì›” ì´ˆê³¼). APIë¡œ ì¬ê²€ì¦í•©ë‹ˆë‹¤."
                        )
                else:
                    # Positive Cache í•­ëª©. ì •ìƒ ì²˜ë¦¬
                    main = payload
                    iri = main.get("@id")
                    log.info(
                        f"âœ… DDC {ddc} ì „ì²´ ë°ì´í„°ë¥¼ DB ìºì‹œì—ì„œ ì¡°íšŒ (API í˜¸ì¶œ ì—†ìŒ)"
                    )
            except (json.JSONDecodeError, TypeError, ValueError) as e:
                # JSON íŒŒì‹± ì˜¤ë¥˜ ë˜ëŠ” Negative Cache íˆíŠ¸ ì‹œì˜ ValueErrorëŠ” ì •ìƒ íë¦„
                if "Negative Cache" in str(e):
                    raise e  # Negative cache íˆíŠ¸ëŠ” ë‹¤ì‹œ ì˜ˆì™¸ ë°œìƒì‹œì¼œ UIë¡œ ì „ë‹¬
                log.warning(f"âš ï¸ DDC {ddc} DB ìºì‹œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                pass  # ìºì‹œê°€ ì†ìƒë˜ì—ˆê±°ë‚˜ ë¬¸ì œê°€ ìˆìœ¼ë©´ APIë¡œ ì¬ì¡°íšŒ

        # 2ï¸âƒ£ DBì— ì—†ê±°ë‚˜ ë§Œë£Œë˜ì—ˆìœ¼ë©´ APIë¡œ ì¡°íšŒ
        if not main:
            if not iri:
                try:
                    map_url = URL_MAP_API.format(ddc=ddc)
                    iri_map = self._get_json(map_url)
                    iri = iri_map.get(ddc)
                    if not iri:
                        # âœ… [í•µì‹¬ ë¡œì§ 2] Negative Cache ê¸°ë¡
                        log.warning(
                            f"DDC {ddc}ëŠ” WebDeweyì— ì—†ìŒ. Negative Cacheì— ê¸°ë¡í•©ë‹ˆë‹¤."
                        )
                        not_found_payload = json.dumps({"exists": False})
                        self.query_manager.save_dewey_to_cache(
                            map_url, ddc, not_found_payload
                        )
                        raise ValueError(f"'{ddc}' ì— ëŒ€í•œ DLD URLì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    raise e  # ì—¬ê¸°ì„œ ë°œìƒí•œ ì˜ˆì™¸ëŠ” ìƒìœ„ë¡œ ì „ë‹¬

            main = self._get_json(iri)
            if not main:
                return {}

        # broader ì¶”ì  ë°©ì‹ìœ¼ë¡œ hierarchyë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        hierarchy = self._fetch_chain_upwards(main)

        # -------------------
        # âœ… [í•µì‹¬ ìˆ˜ì •] ê°’ì´ ë¬¸ìì—´ì¼ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì£¼ëŠ” í—¬í¼ í•¨ìˆ˜
        def _to_list(value):
            if value is None:
                return []
            if isinstance(value, list):
                return value
            return [value]

        narrower = self._fetch_many_concurrent(_to_list(main.get("narrower")))
        related = self._fetch_many_concurrent(_to_list(main.get("related")))
        # -------------------

        return {
            "iri": iri,
            "main": main,
            "hierarchy": hierarchy,
            "narrower": narrower,
            "related": related,
        }

    def _get_parent_codes_memo(self, ddc: str) -> list[str]:
        if not ddc:
            return []
        if ddc in self._parent_cache:
            return self._parent_cache[ddc]
        parts = []
        current = ddc
        while True:
            parent = ""
            if "." in current:
                parent = current.rsplit(".", 1)[0]
            elif len(current) == 3:
                if current.endswith("00"):
                    parent = ""
                elif current.endswith("0"):
                    parent = current[0] + "00"
                else:
                    parent = current[:2] + "0"
            if parent and parent not in parts:
                parts.append(parent)
                current = parent
            else:
                break
        self._parent_cache[ddc] = parts
        return parts

    def get_ddc_with_parents(self, ddc: str) -> dict:
        """
        ì…ë ¥ DDCì™€ ìƒìœ„ì½”ë“œë¥¼ LRU/DBì—ì„œ ìš°ì„  ì¡°íšŒí•˜ê³ ,
        ìºì‹œ ë¯¸ìŠ¤ë§Œ APIë¡œ ê°€ì ¸ì˜¨ ë’¤ LRUë¥¼ ì±„ìš´ë‹¤. ë°˜í™˜ê°’ì€ 'ì…ë ¥ DDC'ì˜ payload.
        """
        if not ddc:
            return {}
        # 1) í˜„ì¬ + ìƒìœ„ ëª©ë¡
        codes = [ddc] + self._get_parent_codes_memo(ddc)

        # 2) LRU hit / DB hit ë¨¼ì €
        results: dict[str, dict] = {}
        to_fetch_urls: list[str] = []
        for code in codes:
            # LRU
            with self._lru_lock:
                if code in self._lru_cache:
                    results[code] = self._lru_cache[code]
                    continue
            # DB
            # âœ… [ìˆ˜ì •] self.query_managerë¥¼ í†µí•´ ìºì‹œ ì¡°íšŒ
            raw = self.query_manager.get_dewey_by_notation(code)
            if raw:
                try:
                    payload = json.loads(raw)
                    results[code] = payload
                    with self._lru_lock:
                        self._lru_cache[code] = payload
                        if len(self._lru_cache) > self._lru_capacity:
                            self._lru_cache.popitem(last=False)
                    continue
                except Exception as e:
                    log.warning(f"DDC ìºì‹œ JSON íŒŒì‹± ì‹¤íŒ¨({code}): {e}")

            # ë¯¸ìŠ¤ â†’ API ëŒ€ìƒ
            to_fetch_urls.append(URL_MAP_API.format(ddc=code))

        # 3) API í˜¸ì¶œ(ë¯¸ìŠ¤ë§Œ)
        for url in to_fetch_urls:
            payload = self._get_json(url)  # ë‚´ë¶€ì—ì„œ ì €ì¥ ë° LRU ì ì¬ë¨
            n = payload.get("notation")
            if n:
                results[n] = payload

        # 4) ìµœì¢… ë°˜í™˜: ì…ë ¥ DDC
        return results.get(ddc, {})


# ========================================
# Helper Functions (from qt_TabView_Dewey.py)
# ========================================


def extract_all_ksh_concept_ids(subject_text):
    """KSH ë§ˆí¬ì—…ì—ì„œ ëª¨ë“  KSH Concept ID (nlk:KSH...)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        matches = re.findall(r"â–¼0(KSH\d+)â–²", subject_text)
        return [f"nlk:{ksh_code}" for ksh_code in matches]
    except:
        return []


def format_ksh_content_for_preview(content_list, max_items=None):
    """ë¯¸ë¦¬ë³´ê¸°ìš©ìœ¼ë¡œ KSH ê´€ë ¨ì–´ ëª©ë¡ì˜ í˜•ì‹ì„ ì§€ì •í•©ë‹ˆë‹¤."""
    if not content_list:
        return ""

    if max_items is None:
        return "\n".join(content_list)
    else:
        limited_list = content_list[:max_items]
        result = "\n".join(limited_list)
        if len(content_list) > max_items:
            result += f"\n... ì™¸ {len(content_list) - max_items}ê°œ"
        return result


def normalize_ddc_code(ddc_code: str) -> str:
    """DDC ì½”ë“œ ì •ê·œí™”"""
    if not ddc_code:
        return ""

    code = re.sub(r"[^0-9.\- ]", "", str(ddc_code)).strip()

    if "-" in code:
        parts = code.split("-")
        normalized_parts = []
        for part in parts:
            part = part.strip()
            if "." in part:
                base, decimal = part.split(".", 1)
                decimal = decimal.rstrip("0")
                normalized_parts.append(base if not decimal else f"{base}.{decimal}")
            else:
                normalized_parts.append(part)
        return "-".join(normalized_parts)

    if "." in code:
        base, decimal = code.split(".", 1)
        decimal = decimal.rstrip("0")
        return base if not decimal else f"{base}.{decimal}"
    return code


def get_parent_code(code: str) -> str:
    """ìƒìœ„ DDC ì½”ë“œ ê³„ì‚°"""
    if not code or code == "000":
        return ""

    if "-" in str(code):
        code = str(code).split("-", 1)[0].strip()

    normalized_code = normalize_ddc_code(code)

    if "." in normalized_code:
        base, decimal = normalized_code.split(".", 1)
        trimmed = decimal[:-1] if len(decimal) > 1 else ""
        if not trimmed or set(trimmed) == {"0"}:
            return base
        return f"{base}.{trimmed}"

    if len(normalized_code) == 3:
        if normalized_code.endswith("00"):
            return ""
        if normalized_code.endswith("0"):
            return normalized_code[0] + "00"
        return normalized_code[:2] + "0"

    return ""


def is_table_notation(raw: str) -> bool:
    """ë³´ì¡°í‘œ í‘œê¸° í™•ì¸"""
    s = str(raw).strip()
    return bool(re.fullmatch(r"\d{4,}", s)) and s[0] in "123456"


def format_table_notation(raw: str) -> str:
    """ë³´ì¡°í‘œ í¬ë§·íŒ…"""
    s = str(raw).strip()
    t = s[0]
    rest = s[1:].zfill(4)
    return f"T{t}--{rest}"


def format_ddc_for_display(code: str) -> str:
    """í‘œì‹œìš© DDC ì½”ë“œ í¬ë§·"""
    if not code:
        return ""

    if "." in code:
        return code

    if re.fullmatch(r"\d{1,3}", code):
        return code

    if is_table_notation(code):
        return format_table_notation(code)

    m = re.match(r"^(\d)--(\d+)$", code)
    if m:
        table_no, rest = m.groups()
        return f"T{table_no}--{rest.zfill(4)}"

    return code


def should_lazy_expand(code: str) -> bool:
    """ì§€ì—° ë¡œë”© ëŒ€ìƒ íŒì •"""
    if not code:
        return False

    if is_table_notation(code):
        return False

    if "." in code:
        return True

    if "-" in code:
        return True

    return len(code) == 3 and code.isdigit()


def preprocess_dewey_description_for_ksh(db_manager, text: str) -> str:
    if not text:
        return ""

    sqm = SearchQueryManager(db_manager)

    def replace_parentheses(match):
        main_term, parentheses_term = match.group(1).strip(), match.group(2).strip()
        if re.match(r"^[\\d\s\-â€“â€”,]+$", parentheses_term):
            return main_term
        ref_patterns = [
            r"^(see\s+also|cf\.?,|etc\.?,|e\.g\.?,|i\.e\.?)?",
            r"^(ì°¸ì¡°|ì°¸ê³ |ì˜ˆ|ì¦‰)",
        ]
        if any(re.match(p, parentheses_term.lower()) for p in ref_patterns):
            return main_term
        return f"{main_term}, {parentheses_term}"

    processed_text = text
    for _ in range(3):
        if not re.search(r"([^()]+?)\s*\(([^()]+?)\)", processed_text):
            break
        processed_text = re.sub(
            r"([^()]+?)\s*\(([^()]+?)\)", replace_parentheses, processed_text
        )

    text = processed_text.strip().replace("--", ", ")
    text = re.sub(r"(,\s*)?(\.\s*){2,}$", "", text).strip()
    text = re.sub(r"\s+(and|&)\s+", ", ", text, flags=re.IGNORECASE)
    text = re.sub(r"\b\d{4}[-â€âˆ’â€“â€”]?\d*[-â€âˆ’â€“â€”]?\b", "", text)
    text = re.sub(r"[0-9\-â€âˆ’â€“â€”]+", "", text)
    text = re.sub(r"[;:]+", "", text)
    text = re.sub(r",+", ",", text)
    text = re.sub(r"\s*,\s*$", "", text).strip()
    text = re.sub(r"^\s*,\s*", "", text).strip()
    text = re.sub(r"\s+", " ", text).strip()
    return sqm._singularize_search_term(text)


def get_ksh_detailed_info(concept_id, db_manager, log_message_func):
    """KSH ê°œë… IDë¡œë¶€í„° ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    result = {
        "definition": "",
        "pref": "",
        "related": [],
        "broader": [],
        "narrower": [],
        "synonyms": [],
        "ksh_link_url": "",
    }
    try:
        conn = db_manager._get_concepts_connection()
        cursor = conn.cursor()
        cursor.execute(
            "SELECT prop, value FROM literal_props WHERE concept_id = ? ORDER BY prop",
            (concept_id,),
        )
        for prop, value in cursor.fetchall():
            if prop == "definition":
                result["definition"] = value
            elif prop == "prefLabel":
                result["pref"] = value
            elif prop == "altLabel":
                result["synonyms"].append(value)

        cursor.execute(
            "SELECT prop, target FROM uri_props WHERE concept_id = ?", (concept_id,)
        )
        for prop, target_id in cursor.fetchall():
            if target_id and target_id.startswith("nlk:KSH"):
                cursor.execute(
                    "SELECT value FROM literal_props WHERE concept_id = ? AND prop = 'prefLabel' LIMIT 1",
                    (target_id,),
                )
                label_result = cursor.fetchone()
                if label_result:
                    label, ksh_code = label_result[0], target_id.replace("nlk:", "")
                    formatted = f"â–¼a{label}â–¼0{ksh_code}â–²"
                    if prop in result:
                        result[prop].append(formatted)

        if concept_id.startswith("nlk:"):
            ksh_code = concept_id.replace("nlk:", "")
            result["ksh_link_url"] = (
                f"https://librarian.nl.go.kr/LI/contents/L20201000000.do?controlNo={ksh_code}"
            )
        conn.close()
    except Exception as e:
        log_message_func(f"ì˜¤ë¥˜: KSH ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ - {e}", level="ERROR")
    return result


def search_ksh_for_dewey_tab(sqm, search_term, is_cancelled_callback=None):
    """
    Dewey íƒ­ì˜ KSH ê²€ìƒ‰ íŒ¨ë„ì—ì„œ ì‹¤í–‰ë˜ëŠ” ëª¨ë“  ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì´ê´„í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” QThreadì˜ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """
    # ----------------------------------------------------------------------------------
    # [ì‚¬ì „ ì¤€ë¹„ 1ë‹¨ê³„] - í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬(pandas) ë¡œë“œ í™•ì¸
    # ----------------------------------------------------------------------------------
    # ì´ í”„ë¡œê·¸ë¨ì€ ë°ì´í„°ë¥¼ DataFrame í˜•íƒœë¡œ ë‹¤ë£¨ë¯€ë¡œ, pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìˆ˜ì ì…ë‹ˆë‹¤.
    # ë§Œì•½ pandasê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´, ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•Šê³  ì¡°ìš©íˆ í•¨ìˆ˜ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.
    try:
        import pandas as pd
    except ImportError:
        # pandasê°€ ì—†ëŠ” í™˜ê²½ì—ì„œëŠ” ì•„ë¬´ ì‘ì—…ë„ í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        print("Pandas is not installed. Returning empty DataFrame.")
        return None

    # ----------------------------------------------------------------------------------
    # [ì‚¬ì „ ì¤€ë¹„ 2ë‹¨ê³„] - ì‚¬ìš©ì ì…ë ¥ê°’ ì „ì²˜ë¦¬
    # ----------------------------------------------------------------------------------
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê²€ìƒ‰ì–´(ì˜ˆ: "  íŒŒì´ì¬   í”„ë¡œê·¸ë˜ë°  ")ì— í¬í•¨ëœ ë¶ˆí•„ìš”í•œ ê³µë°±ì´ë‚˜
    # íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•˜ì—¬ ìˆœìˆ˜í•œ ê²€ìƒ‰ í‚¤ì›Œë“œë§Œ ë‚¨ê¹ë‹ˆë‹¤. (ì˜ˆ: "íŒŒì´ì¬í”„ë¡œê·¸ë˜ë°")
    cleaned_search_term = clean_ksh_search_input(search_term)

    # ----------------------------------------------------------------------------------
    # [ë©”ì¸ ê²€ìƒ‰ 1ë‹¨ê³„] - KSH ê°œë… DB ë° ì„œì§€ DB í†µí•© ê²€ìƒ‰
    # ----------------------------------------------------------------------------------
    # ì‚¬ìš©ìê°€ 'ê²€ìƒ‰ ì·¨ì†Œ' ë²„íŠ¼ì„ ëˆŒë €ëŠ”ì§€ í™•ì¸í•˜ê³ , ì·¨ì†Œí–ˆë‹¤ë©´ ì¦‰ì‹œ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
    if is_cancelled_callback and is_cancelled_callback():
        return pd.DataFrame()

    # search_integrated_ksh í•¨ìˆ˜ëŠ” ì…ë ¥ëœ ê²€ìƒ‰ì–´ë¥¼ ë¶„ì„í•˜ì—¬ 'DDC', 'KSH ì½”ë“œ', 'í‚¤ì›Œë“œ' ë“±
    # ìœ í˜•ì„ ìë™ìœ¼ë¡œ íŒë³„í•˜ê³ , KSH ê°œë… DBì™€ ì„œì§€ DBì—ì„œ ê´€ë ¨ëœ ëª¨ë“  ì •ë³´ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜µë‹ˆë‹¤.
    # ì´ í•¨ìˆ˜ëŠ” (ê°œë… DataFrame, ì„œì§€ DataFrame, ê²€ìƒ‰ì–´ íƒ€ì…) ì„¸ ê°€ì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    final_concepts, final_bibliographic, search_type = sqm.search_integrated_ksh(
        search_term=cleaned_search_term
    )

    # ê²€ìƒ‰ ê²°ê³¼ê°€ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬, None ëŒ€ì‹  ë¹ˆ DataFrameìœ¼ë¡œ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    # ì´ë ‡ê²Œ í•˜ë©´ ì´í›„ì˜ ì½”ë“œì—ì„œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ ì˜¤ë¥˜ ì—†ì´ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    final_concepts = final_concepts if final_concepts is not None else pd.DataFrame()
    final_bibliographic = (
        final_bibliographic if final_bibliographic is not None else pd.DataFrame()
    )

    # ----------------------------------------------------------------------------------
    # [ê²°ê³¼ ì·¨í•© ì¤€ë¹„] - ìµœì¢… ê²°ê³¼ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ìˆœì„œ ë° ì´ë¦„ ì •ì˜
    # ----------------------------------------------------------------------------------
    # í™”ë©´ì— í‘œì‹œë  í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ìˆœì„œì™€ í—¤ë” ì´ë¦„ì„ ë¯¸ë¦¬ ì •ì˜í•©ë‹ˆë‹¤.
    # ì´ ìˆœì„œëŒ€ë¡œ ë°ì´í„°ê°€ ì¡°í•©ë˜ì–´ ìµœì¢… DataFrameì´ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤.
    biblio_cols = [
        "ksh",
        "title",
        "ddc",
        "ddc_label",
        "kdc",
        "publication_year",
        "identifier",
        "data_type",
        "source_file",
    ]

    # ì—¬ëŸ¬ ê²€ìƒ‰ ì†ŒìŠ¤(ì„œì§€, ì»¨ì…‰, DDC Cache)ì˜ ê²°ê³¼ë¥¼ ë‹´ì„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    all_data = []

    # ----------------------------------------------------------------------------------
    # [ê²°ê³¼ ì·¨í•© 1ë‹¨ê³„] - ì„œì§€ DB ê²€ìƒ‰ ê²°ê³¼(final_bibliographic) ì²˜ë¦¬
    # ----------------------------------------------------------------------------------
    # ì„œì§€ DBì—ì„œ ê°€ì ¸ì˜¨ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    if not final_bibliographic.empty:
        # ê° í–‰ì„ ìˆœíšŒí•˜ë©´ì„œ, ë¯¸ë¦¬ ì •ì˜í•œ ì»¬ëŸ¼(biblio_cols) í˜•ì‹ì— ë§ê²Œ ë°ì´í„°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
        for _, row in final_bibliographic.iterrows():
            all_data.append(
                {
                    "ksh": row.get("ksh_labeled", row.get("ksh", "")),
                    "title": row.get("title", ""),
                    "ddc": row.get("ddc", ""),
                    "ddc_label": row.get("ddc_label", ""),
                    "kdc": row.get("kdc", ""),
                    "publication_year": row.get("publication_year", ""),
                    "identifier": row.get("identifier", ""),
                    "data_type": "ì„œì§€",  # ì´ ë°ì´í„°ì˜ ì¶œì²˜ê°€ 'ì„œì§€ DB'ì„ì„ ëª…ì‹œí•©ë‹ˆë‹¤.
                    "source_file": row.get("source_file", ""),
                }
            )

    # ----------------------------------------------------------------------------------
    # [ê²°ê³¼ ì·¨í•© 2ë‹¨ê³„] - KSH ê°œë… DB ê²€ìƒ‰ ê²°ê³¼(final_concepts) ì²˜ë¦¬
    # ----------------------------------------------------------------------------------
    # KSH ê°œë… DBì—ì„œ ê°€ì ¸ì˜¨ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    if not final_concepts.empty:
        # ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´, ê²°ê³¼ì— í¬í•¨ëœ ëª¨ë“  DDC ì½”ë“œì˜ ë ˆì´ë¸”ì„ DBì—ì„œ í•œ ë²ˆì— ì¡°íšŒí•©ë‹ˆë‹¤.
        # ì´ë ‡ê²Œ í•˜ë©´ ê° í–‰ë§ˆë‹¤ DBì— ì ‘ê·¼í•˜ëŠ” ë¹„íš¨ìœ¨ì„ ë§‰ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        unique_ddcs = final_concepts["DDC"].dropna().unique().tolist()
        ddc_label_map = {}
        if unique_ddcs:
            ddc_label_map = sqm.get_all_ddc_labels_bulk(unique_ddcs)

        # ê° í–‰ì„ ìˆœíšŒí•˜ë©´ì„œ, ë¯¸ë¦¬ ì •ì˜í•œ ì»¬ëŸ¼ í˜•ì‹ì— ë§ê²Œ ë°ì´í„°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
        for _, row in final_concepts.iterrows():
            ddc_value = row.get("DDC", "")
            # ìœ„ì—ì„œ ë¯¸ë¦¬ ë§Œë“¤ì–´ ë‘” DDC ë ˆì´ë¸” ë§µì—ì„œ í•´ë‹¹ DDCì˜ ì„¤ëª…ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            ddc_label_value = ddc_label_map.get(ddc_value, "") if ddc_value else ""

            all_data.append(
                {
                    "ksh": row.get("ì£¼ì œëª…", ""),
                    "title": row.get("Matched", ""),
                    "ddc": ddc_value,
                    "ddc_label": ddc_label_value,
                    "kdc": row.get("KDC-Like", ""),
                    "publication_year": "",  # ì»¨ì…‰ ë°ì´í„°ì—ëŠ” ë°œí–‰ë…„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.
                    "identifier": row.get("_concept_id", ""),
                    "data_type": "ì»¨ì…‰",  # ì´ ë°ì´í„°ì˜ ì¶œì²˜ê°€ 'ì»¨ì…‰ DB'ì„ì„ ëª…ì‹œí•©ë‹ˆë‹¤.
                    "source_file": row.get("ì£¼ì œëª¨ìŒ", ""),
                }
            )

    # ----------------------------------------------------------------------------------
    # [ë©”ì¸ ê²€ìƒ‰ 2ë‹¨ê³„] - DDC Cache DB ë³´ì¡° ê²€ìƒ‰ (í•µì‹¬ ìˆ˜ì • ì§€ì )
    # ----------------------------------------------------------------------------------
    # ì´ì „ ë‹¨ê³„ì—ì„œ 'ìˆ«ì/DDC í˜•ì‹'ì´ ì•„ë‹Œ 'ìˆœìˆ˜ í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ'ë§Œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    # ì´ ë¡œì§ì€ ë¶ˆí•„ìš”í•œ ì¤‘ë³µ/ì—ëŸ¬ ê²€ìƒ‰ì„ ë§‰ëŠ” í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.
    keywords_only = []
    original_keywords = [
        kw.strip() for kw in re.split(r"[,;]", search_term) if kw.strip()
    ]
    for kw in original_keywords:
        # ì •ê·œì‹ì„ ì‚¬ìš©í•´ ìˆ«ìì™€ ì (.)ë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ DDC í˜•ì‹ì˜ í‚¤ì›Œë“œë¥¼ ê±¸ëŸ¬ëƒ…ë‹ˆë‹¤.
        if not re.fullmatch(r"[\d\.]+", kw):
            keywords_only.append(kw)

    # ë¶„ë¦¬ëœ 'ìˆœìˆ˜ í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ'ê°€ ìˆì„ ê²½ìš°ì—ë§Œ DDC Cache DB ë³´ì¡° ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    if keywords_only:
        # ê²€ìƒ‰í•  í‚¤ì›Œë“œë“¤ì„ ë‹¤ì‹œ ì‰¼í‘œë¡œ ì—°ê²°ëœ ë¬¸ìì—´ë¡œ ë§Œë“­ë‹ˆë‹¤. (ì˜ˆ: "Python, Java")
        supplemental_search_term = ", ".join(keywords_only)

        # DDC Cache DBì—ì„œ í…ìŠ¤íŠ¸ í‚¤ì›Œë“œë¡œ DDC ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        ddc_keyword_results = sqm.search_ddc_by_multiple_keywords(
            supplemental_search_term, max_results_per_level=5
        )

        # ë³´ì¡° ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆë‹¤ë©´, ìµœì¢… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸(all_data)ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        if ddc_keyword_results:
            # ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´, ì°¾ì€ DDC ì½”ë“œë“¤ì˜ ë ˆì´ë¸”ì„ í•œ ë²ˆì— ì¡°íšŒí•©ë‹ˆë‹¤.
            found_ddcs = [res["ddc"] for res in ddc_keyword_results]
            ddc_label_map = sqm.get_all_ddc_labels_bulk(found_ddcs)

            for result in ddc_keyword_results:
                ddc_val = result.get("ddc", "")
                label_text = ddc_label_map.get(ddc_val, "")
                all_data.append(
                    {
                        "ksh": f"DDC: {ddc_val}",
                        "title": label_text,
                        "ddc": ddc_val,
                        "ddc_label": label_text,
                        "kdc": "",
                        "publication_year": "",
                        "identifier": "",
                        "data_type": "DDC Cache",  # ì´ ë°ì´í„°ì˜ ì¶œì²˜ê°€ 'DDC Cache'ì„ì„ ëª…ì‹œí•©ë‹ˆë‹¤.
                        "source_file": "DDC Cache DB",
                    }
                )

    # ----------------------------------------------------------------------------------
    # [ìµœì¢… ë§ˆë¬´ë¦¬] - í†µí•©ëœ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
    # ----------------------------------------------------------------------------------
    # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì·¨í•©í•œ all_data ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´, ë¹ˆ DataFrameì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    if not all_data:
        return pd.DataFrame()

    # all_data ë¦¬ìŠ¤íŠ¸ì™€ ë¯¸ë¦¬ ì •ì˜í•œ ì»¬ëŸ¼ ìˆœì„œ(biblio_cols)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… DataFrameì„ ìƒì„±í•©ë‹ˆë‹¤.
    combined_df = pd.DataFrame(all_data, columns=biblio_cols)
    # ìƒì„±ëœ DataFrameì„ í˜¸ì¶œí•œ ìª½(DeweyKshSearchThread)ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    return combined_df


def dewey_get_safe(data, key):
    """ë”•ì…”ë„ˆë¦¬ ì•ˆì „ get"""
    return data.get(key, "") if isinstance(data, dict) else ""


def dewey_pick_label(value):
    """ë‹¤êµ­ì–´ ë ˆì´ë¸” ì„ íƒ"""
    if isinstance(value, dict):
        return value.get("en") or value.get("ko") or value.get("label") or ""
    return str(value) if value else ""


def search_dewey_hierarchy(dewey_client, ddc_code, is_cancelled_callback=None):
    """
    DDC ê³„ì¸µ ê²€ìƒ‰ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§.
    DeweySearchThreadë¡œë¶€í„° ë¶„ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed

    if is_cancelled_callback and is_cancelled_callback():
        return None

    main_ctx = dewey_client.get_dewey_context(ddc_code)

    if is_cancelled_callback and is_cancelled_callback():
        return None

    hierarchy_data = {}

    raw_main_code = dewey_get_safe(main_ctx.get("main", {}), "notation")
    main_code = normalize_ddc_code(raw_main_code)
    main_label = dewey_pick_label(main_ctx.get("main", {}).get("prefLabel"))
    if main_code:
        hierarchy_data[main_code] = main_label or "Label not found"

    for item in main_ctx.get("hierarchy", []):
        if is_cancelled_callback and is_cancelled_callback():
            return None
        code = normalize_ddc_code(dewey_get_safe(item, "notation"))
        if code:
            label = dewey_pick_label(item.get("prefLabel"))
            hierarchy_data[code] = label or "Label not found"

    path_codes = []
    current_code = main_code
    while current_code:
        if is_cancelled_callback and is_cancelled_callback():
            return None
        path_codes.append(normalize_ddc_code(current_code))
        current_code = get_parent_code(current_code)

    if path_codes:
        top_code = path_codes[-1]
        if len(top_code) == 3 and not top_code.endswith("00"):
            path_codes.append(top_code[0] + "00")
    path_codes = sorted(list(set(path_codes)))

    missing_codes = [code for code in path_codes if code not in hierarchy_data]
    if missing_codes and (not is_cancelled_callback or not is_cancelled_callback()):

        def fetch_label(code):
            if is_cancelled_callback and is_cancelled_callback():
                return code, "Label not found"
            if not code or code == "000":
                return code, "Label not found"
            try:
                missing_ctx = dewey_client.get_dewey_context(code)
                label = dewey_pick_label(missing_ctx.get("main", {}).get("prefLabel"))
                return code, label or "Label not found"
            except Exception:
                return code, "Label not found"

        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = {
                executor.submit(fetch_label, code): code for code in missing_codes
            }
            for future in as_completed(futures):
                if is_cancelled_callback and is_cancelled_callback():
                    executor.shutdown(wait=False, cancel_futures=True)
                    return None
                code, label = future.result()
                hierarchy_data[code] = label

    if is_cancelled_callback and is_cancelled_callback():
        return None

    return {
        "main_ctx": main_ctx,
        "hierarchy_data": hierarchy_data,
        "path_codes": path_codes,
    }
