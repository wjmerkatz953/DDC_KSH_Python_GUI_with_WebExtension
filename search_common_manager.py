# íŒŒì¼ëª…: search_query_manager.py
# ì„¤ëª…: ë³µì¡í•œ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì¿¼ë¦¬ ë° ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
"""search_query_manager.py
ë²„ì „: v2.2.0
ì—…ë°ì´íŠ¸: 2025-10-19

[2025-10-19 ì—…ë°ì´íŠ¸ ë‚´ì—­ - v2.2.0]
âš¡ ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™” (20ì´ˆ â†’ 1ì´ˆ!)
- FTS5 ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ ë„ì… (ksh_korean ì»¬ëŸ¼)
  * _search_by_korean_subject(): FTS5 MATCH ì‚¬ìš©
  * í•œêµ­ì–´ ì£¼ì œëª… ê²€ìƒ‰: 20ì´ˆ â†’ 1ì´ˆ (95% ì„±ëŠ¥ í–¥ìƒ)

- DDC ê²€ìƒ‰ ìµœì í™”
  * _search_by_ddc_ranking_logic(): INDEXED BY idx_ddc_ksh ê°•ì œ ì‚¬ìš©
  * ë³µí•© ì¸ë±ìŠ¤ë¡œ ì¸ë±ìŠ¤ íŒíŠ¸ ì§€ì •
  * DDC ì½”ë“œ ê²€ìƒ‰: 17ì´ˆ â†’ 1ì´ˆ (94% ì„±ëŠ¥ í–¥ìƒ)

- SELECT ì¿¼ë¦¬ ìµœì í™”
  * SELECT * â†’ í•„ìš”í•œ 11ê°œ ì»¬ëŸ¼ë§Œ ì¡°íšŒ
  * ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 15-20% ì ˆê°

[ì´ì „ ë²„ì „ - v2.1.0]
- DDC ìºì‹œ ê´€ë ¨ ëª¨ë“  ê³ ìˆ˜ì¤€ ë¡œì§ì„ database_manager.pyì—ì„œ ì´ê´€í•˜ì—¬ í†µí•©
- ì¶”ê°€ëœ ë©”ì„œë“œ: get_dewey_cache_entry, get_dewey_by_notation, get_multiple_ddcs_descriptions, _cache_ddc_description, save_dewey_to_cache, _extract_and_save_keywords
- NameError í•´ê²°ì„ ìœ„í•´ íŒŒì¼ ìƒë‹¨ì— import json êµ¬ë¬¸ ì¶”ê°€
- _search_by_korean_subject ë©”ì„œë“œì˜ ë¶ˆí•„ìš”í•œ print ë¡œê·¸ë¥¼ ì œê±°í•˜ê³  logger ì‚¬ìš©
"""

import pandas as pd
import re
import json
import time
import logging
from typing import List, Dict, Tuple
from database_manager import DatabaseManager
from db_perf_tweaks import wait_for_warmup  # âœ… ì›Œë°ì—… ì™„ë£Œ ëŒ€ê¸°
import inflection
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = logging.getLogger("qt_main_app.database_manager")


# ì•±ì˜ ë¡œê·¸ í•¸ë“¤ëŸ¬ì™€ ì—°ë™ë˜ë„ë¡ ëª…ì‹œì  ì´ë¦„ ì‚¬ìš©
logger = logging.getLogger("qt_main_app.database_manager")


# ì–¸ì–´íƒœê·¸ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ì¶”ê°€
LANG_TAG_RE = re.compile(r"@([A-Za-z]{2,3})$")


def simple_singularize(word: str) -> str:
    """ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì˜ë‹¨ì–´ë¥¼ ë‹¨ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    # ìì£¼ ì“°ì´ëŠ” ë¶ˆê·œì¹™ ë³µìˆ˜í˜•
    irregulars = {
        "children": "child",
        "people": "person",
        "men": "man",
        "women": "woman",
        "teeth": "tooth",
        "feet": "foot",
        "mice": "mouse",
        "geese": "goose",
    }
    if word.lower() in irregulars:
        return irregulars[word.lower()]

    # ê·œì¹™ ê¸°ë°˜ ë³€í™˜
    if word.endswith("s"):
        if word.endswith("ies"):
            # babies -> baby
            return word[:-3] + "y"
        if word.endswith("es"):
            # boxes, buses -> box, bus
            if word.endswith(("buses", "gases")):
                return word[:-2]
            if len(word) > 3 and word.endswith(("ches", "shes", "sses", "xes")):
                return word[:-2]
        if not word.endswith(("ss", "us")):
            # cats -> cat
            return word[:-1]

    return word


def split_lang_suffix(s: str):
    """ì–¸ì–´íƒœê·¸ ë¶„ë¦¬: 'internet@en' -> ('internet', 'en')"""
    if not s:
        return s, None
    m = LANG_TAG_RE.search(s.strip())
    if not m:
        return s.strip(), None
    base = s[: m.start()].strip()
    return base, m.group(1).lower()


def dedup_lang_variants(values: List[str]) -> List[str]:
    """ì–¸ì–´íƒœê·¸ê°€ ìˆëŠ” ê²ƒì„ ìš°ì„ í•˜ì—¬ ì¤‘ë³µ ì œê±°"""
    chosen: Dict[str, Tuple[str, bool]] = {}
    order: List[str] = []

    for v in values:
        if not v:
            continue
        base, lang = split_lang_suffix(str(v))
        if base not in chosen:
            chosen[base] = (v, bool(lang))
            order.append(base)
        else:
            cur_v, cur_tagged = chosen[base]
            if (not cur_tagged) and lang:
                chosen[base] = (v, True)

    return [chosen[b][0] for b in order]



class SearchCommonManager:
    """
    ê²€ìƒ‰ ê¸°ëŠ¥ì˜ ê³µìš© ë² ì´ìŠ¤ í´ë˜ìŠ¤
    - ì„œì§€ ê²€ìƒ‰
    - ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬
    - ê´€ê³„ì–´ ì¡°íšŒ
    - ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ
    """

    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager

    # ---------------------------------------------------
    # ì•„ë˜ëŠ” database_manager.pyì—ì„œ ì˜®ê²¨ì˜¨ ê²€ìƒ‰ ê´€ë ¨ ë©”ì„œë“œë“¤ì…ë‹ˆë‹¤.
    # self.ë©”ì„œë“œ() í˜¸ì¶œì€ self.db_manager.ë©”ì„œë“œ() ë¡œ ë³€ê²½ë©ë‹ˆë‹¤.
    # ---------------------------------------------------


    def _extract_and_save_keywords(
        self, cursor, iri: str, ddc_code: str, raw_json: str
    ):
        """JSONì—ì„œ ì˜ì–´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ddc_keyword í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            data = json.loads(raw_json)
            keyword_entries = []

            # prefLabel ì²˜ë¦¬
            if "prefLabel" in data and "en" in data["prefLabel"]:
                keyword = data["prefLabel"]["en"].strip()
                if keyword:
                    keyword_entries.append((iri, ddc_code, keyword, "pref"))

            # altLabel ì²˜ë¦¬
            if "altLabel" in data and "en" in data["altLabel"]:
                labels = data["altLabel"]["en"]
                alt_labels = labels if isinstance(labels, list) else [labels]
                for label in alt_labels:
                    keyword = label.strip()
                    if keyword:
                        keyword_entries.append((iri, ddc_code, keyword, "alt"))

            if keyword_entries:
                # ê¸°ì¡´ í‚¤ì›Œë“œ ë¨¼ì € ì‚­ì œ (ê°™ì€ IRIì˜ ëª¨ë“  í‚¤ì›Œë“œ)
                cursor.execute("DELETE FROM ddc_keyword WHERE iri = ?", (iri,))

                # ìƒˆ í‚¤ì›Œë“œ ì‚½ì… (íŠ¸ë¦¬ê±°ê°€ ìë™ìœ¼ë¡œ FTS ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸)
                cursor.executemany(
                    """
                    INSERT INTO ddc_keyword (iri, ddc, keyword, term_type)
                    VALUES (?, ?, ?, ?)
                    """,
                    keyword_entries,
                )
        except (json.JSONDecodeError, KeyError, TypeError):
            # JSON íŒŒì‹± ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
            pass

    # ==========================================================
    # âœ… [ì‹ ê·œ ì¶”ê°€] DDC Cache DB ê²€ìƒ‰ í•¨ìˆ˜
    # ==========================================================

    def _get_best_matched_term(
        self, conn, concept_id: str, search_term: str = None
    ) -> str:
        """
        ê°œë…ì—ì„œ ê°€ì¥ ì í•©í•œ matched_termì„ ì°¾ê¸° (ì¤‘ë³µ ì œê±° + ìš°ì„ ìˆœìœ„)
        ìš°ì„ ìˆœìœ„: ê²€ìƒ‰ì–´ í¬í•¨ëœ ê²ƒ > prefLabel > label > altLabel
        """
        cursor = conn.cursor()

        # ëª¨ë“  value ê°€ì ¸ì˜¤ê¸°
        cursor.execute(
            """
            SELECT value, prop FROM literal_props
            WHERE concept_id = ?
            ORDER BY
                CASE prop
                    WHEN 'prefLabel' THEN 1
                    WHEN 'label' THEN 2
                    WHEN 'altLabel' THEN 3
                    ELSE 4
                END,
                LENGTH(value) ASC
        """,
            (concept_id,),
        )

        all_values = cursor.fetchall()
        if not all_values:
            return ""

        # ì–¸ì–´íƒœê·¸ ì¤‘ë³µ ì œê±° ì ìš©
        values_only = [row[0] for row in all_values if row[0]]
        deduplicated = self.dedup_lang_variants(values_only)

        # ê²€ìƒ‰ì–´ê°€ ìˆìœ¼ë©´ ê²€ìƒ‰ì–´ í¬í•¨ëœ ê²ƒ ìš°ì„ 
        if search_term and deduplicated:
            for value in deduplicated:
                if search_term.lower() in value.lower():
                    return value

        # ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ê²ƒ ë°˜í™˜
        return deduplicated[0] if deduplicated else ""


    def _get_broader_batch(self, conn, concept_ids: list) -> dict:
        """âš¡ ë°°ì¹˜: ì—¬ëŸ¬ conceptì˜ ìƒìœ„ì–´ë¥¼ í•œ ë²ˆì— ì¡°íšŒ (IN ì ˆ ì²­í¬ ë°©ì‹)"""
        if not concept_ids:
            return {}

        cursor = conn.cursor()
        CHUNK_SIZE = 100

        # Step 1: broader ê´€ê³„ ì¡°íšŒ (ì²­í¬ ë°©ì‹)
        all_broader_results = []
        for i in range(0, len(concept_ids), CHUNK_SIZE):
            chunk = concept_ids[i:i + CHUNK_SIZE]
            placeholders = ",".join("?" * len(chunk))

            query1 = f"""
                SELECT concept_id, target
                FROM uri_props
                WHERE concept_id IN ({placeholders}) AND prop='broader'
            """
            cursor.execute(query1, chunk)
            all_broader_results.extend(cursor.fetchall())

        broader_map = {}
        all_target_ids = set()
        for row in all_broader_results:
            concept_id, target_id = row[0], row[1]
            if concept_id not in broader_map:
                broader_map[concept_id] = set()
            if target_id:
                broader_map[concept_id].add(target_id)
                all_target_ids.add(target_id)

        # Step 2: narrower ì—­ê´€ê³„ ì¡°íšŒ (ì²­í¬ ë°©ì‹)
        all_narrower_inverse = []
        for i in range(0, len(concept_ids), CHUNK_SIZE):
            chunk = concept_ids[i:i + CHUNK_SIZE]
            placeholders = ",".join("?" * len(chunk))

            query2 = f"""
                SELECT target, concept_id
                FROM uri_props
                WHERE target IN ({placeholders}) AND prop='narrower'
            """
            cursor.execute(query2, chunk)
            all_narrower_inverse.extend(cursor.fetchall())

        for row in all_narrower_inverse:
            target_id, concept_id_from = row[0], row[1]
            if target_id not in broader_map:
                broader_map[target_id] = set()
            if concept_id_from:
                broader_map[target_id].add(concept_id_from)
                all_target_ids.add(concept_id_from)

        # Step 3: ëª¨ë“  target IDì˜ prefLabel ë°°ì¹˜ ì¡°íšŒ (ì²­í¬ ë°©ì‹)
        if not all_target_ids:
            return {cid: [] for cid in concept_ids}

        target_ids_list = list(all_target_ids)
        label_results = []

        for i in range(0, len(target_ids_list), CHUNK_SIZE):
            chunk = target_ids_list[i:i + CHUNK_SIZE]
            placeholders = ",".join("?" * len(chunk))

            label_query = f"""
                SELECT concept_id, value
                FROM literal_props
                WHERE concept_id IN ({placeholders})
                AND prop IN ('prefLabel', 'label')
                ORDER BY CASE prop WHEN 'prefLabel' THEN 1 ELSE 2 END
            """
            cursor.execute(label_query, chunk)
            label_results.extend(cursor.fetchall())

        label_map = {}
        for row in label_results:
            target_id, label = row[0], row[1]
            if target_id not in label_map and label:
                label_map[target_id] = label

        # Step 4: ìµœì¢… ê²°ê³¼ êµ¬ì„±
        result = {}
        for concept_id in concept_ids:
            broader_ids = list(broader_map.get(concept_id, set()))
            broader_terms = []
            for broader_id in broader_ids:
                label = label_map.get(broader_id)
                if label:
                    broader_terms.append(self._format_ksh_display(broader_id, label))
            result[concept_id] = broader_terms

        return result


    def _get_clean_subject_for_sorting(self, text):
        """ì •ë ¬ìš© ìˆœìˆ˜ ì£¼ì œì–´ ì¶”ì¶œ (ëª¨ë“  ìˆ˜ì‹ì–´ ì œê±°)"""
        if not text:
            return ""

        # 1ë‹¨ê³„: ì›ê´„í˜¸ () ì™„ì „ ì œê±°
        clean = re.sub(r"\([^)]*\)", "", text)
        # 2ë‹¨ê³„: ê°ê´„í˜¸ [] ì™„ì „ ì œê±°
        clean = re.sub(r"\[[^\]]*\]", "", clean)
        # 3ë‹¨ê³„: ì–¸ì–´íƒœê·¸ @en, @fr, @de ë“± ì œê±°
        clean = re.sub(r"@[a-z]{2,3}$", "", clean, flags=re.IGNORECASE)
        # 4ë‹¨ê³„: ì•ë’¤ ê³µë°± ì œê±°
        clean = clean.strip()

        return clean


    def _get_narrower_batch(self, conn, concept_ids: list) -> dict:
        """âš¡ ë°°ì¹˜: ì—¬ëŸ¬ conceptì˜ í•˜ìœ„ì–´ë¥¼ í•œ ë²ˆì— ì¡°íšŒ (IN ì ˆ ì²­í¬ ë°©ì‹)"""
        if not concept_ids:
            return {}

        cursor = conn.cursor()
        CHUNK_SIZE = 100

        # Step 1: narrower ê´€ê³„ ì¡°íšŒ (ì²­í¬ ë°©ì‹)
        all_narrower_results = []
        for i in range(0, len(concept_ids), CHUNK_SIZE):
            chunk = concept_ids[i:i + CHUNK_SIZE]
            placeholders = ",".join("?" * len(chunk))

            query1 = f"""
                SELECT concept_id, target
                FROM uri_props
                WHERE concept_id IN ({placeholders}) AND prop='narrower'
            """
            cursor.execute(query1, chunk)
            all_narrower_results.extend(cursor.fetchall())

        narrower_map = {}
        all_target_ids = set()
        for row in all_narrower_results:
            concept_id, target_id = row[0], row[1]
            if concept_id not in narrower_map:
                narrower_map[concept_id] = set()
            if target_id:
                narrower_map[concept_id].add(target_id)
                all_target_ids.add(target_id)

        # Step 2: broader ì—­ê´€ê³„ ì¡°íšŒ (ì²­í¬ ë°©ì‹)
        all_broader_inverse = []
        for i in range(0, len(concept_ids), CHUNK_SIZE):
            chunk = concept_ids[i:i + CHUNK_SIZE]
            placeholders = ",".join("?" * len(chunk))

            query2 = f"""
                SELECT target, concept_id
                FROM uri_props
                WHERE target IN ({placeholders}) AND prop='broader'
            """
            cursor.execute(query2, chunk)
            all_broader_inverse.extend(cursor.fetchall())

        for row in all_broader_inverse:
            target_id, concept_id_from = row[0], row[1]
            if target_id not in narrower_map:
                narrower_map[target_id] = set()
            if concept_id_from:
                narrower_map[target_id].add(concept_id_from)
                all_target_ids.add(concept_id_from)

        # Step 3: ëª¨ë“  target IDì˜ prefLabel ë°°ì¹˜ ì¡°íšŒ (ì²­í¬ ë°©ì‹)
        if not all_target_ids:
            return {cid: [] for cid in concept_ids}

        target_ids_list = list(all_target_ids)
        label_results = []

        for i in range(0, len(target_ids_list), CHUNK_SIZE):
            chunk = target_ids_list[i:i + CHUNK_SIZE]
            placeholders = ",".join("?" * len(chunk))

            label_query = f"""
                SELECT concept_id, value
                FROM literal_props
                WHERE concept_id IN ({placeholders})
                AND prop IN ('prefLabel', 'label')
                ORDER BY CASE prop WHEN 'prefLabel' THEN 1 ELSE 2 END
            """
            cursor.execute(label_query, chunk)
            label_results.extend(cursor.fetchall())

        label_map = {}
        for row in label_results:
            target_id, label = row[0], row[1]
            if target_id not in label_map and label:
                label_map[target_id] = label

        # Step 4: ìµœì¢… ê²°ê³¼ êµ¬ì„±
        result = {}
        for concept_id in concept_ids:
            narrower_ids = list(narrower_map.get(concept_id, set()))
            narrower_terms = []
            for narrower_id in narrower_ids:
                label = label_map.get(narrower_id)
                if label:
                    narrower_terms.append(self._format_ksh_display(narrower_id, label))
            result[concept_id] = narrower_terms

        return result


    def _get_pref_label(self, conn, concept_id: str) -> str:
        """concept_idì— ëŒ€í•œ prefLabelì„ ê°€ì ¸ì˜µë‹ˆë‹¤. ì—†ìœ¼ë©´ labelë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."""
        cursor = conn.cursor()
        # -------------------
        # prefLabelì„ ìš°ì„ ìœ¼ë¡œ ì°¾ê³ , ì—†ìœ¼ë©´ labelì„ ê°€ì ¸ì˜¤ë„ë¡ ì¿¼ë¦¬ ìˆ˜ì •
        cursor.execute(
            """
            SELECT value FROM literal_props
            WHERE concept_id=? AND prop IN ('prefLabel', 'label')
            ORDER BY CASE prop WHEN 'prefLabel' THEN 1 ELSE 2 END
            LIMIT 1
            """,
            (concept_id,),
        )
        # -------------------
        row = cursor.fetchone()
        return row[0] if row and row[0] else ""


    def _get_related_batch(self, conn, concept_ids: list) -> dict:
        """âš¡ ë°°ì¹˜: ì—¬ëŸ¬ conceptì˜ ê´€ë ¨ì–´ë¥¼ í•œ ë²ˆì— ì¡°íšŒ (IN ì ˆ ì²­í¬ ë°©ì‹)"""
        if not concept_ids:
            return {}

        cursor = conn.cursor()

        # SQLite IN ì ˆ ìµœì í™”ë¥¼ ìœ„í•´ 100ê°œì”© ì²­í¬ë¡œ ë¶„í• 
        CHUNK_SIZE = 100
        all_related_ids = []

        for i in range(0, len(concept_ids), CHUNK_SIZE):
            chunk = concept_ids[i:i + CHUNK_SIZE]
            placeholders = ",".join("?" * len(chunk))

            # Step 1: related IDë“¤ ê°€ì ¸ì˜¤ê¸° (COVERING INDEX í™œìš©)
            query = f"""
                SELECT concept_id, target
                FROM uri_props
                WHERE concept_id IN ({placeholders}) AND prop='related'
            """
            cursor.execute(query, chunk)
            all_related_ids.extend(cursor.fetchall())

        # concept_id â†’ [related_ids] ë§¤í•‘
        related_map = {}
        all_target_ids = set()
        for row in all_related_ids:
            concept_id, target_id = row[0], row[1]
            if concept_id not in related_map:
                related_map[concept_id] = []
            if target_id:
                related_map[concept_id].append(target_id)
                all_target_ids.add(target_id)

        # Step 2: ëª¨ë“  target IDì˜ prefLabelì„ ë°°ì¹˜ë¡œ ê°€ì ¸ì˜¤ê¸° (ì²­í¬ ë°©ì‹)
        if not all_target_ids:
            return {cid: [] for cid in concept_ids}

        target_ids_list = list(all_target_ids)
        label_results = []

        # -------------------
        # âœ… [í•µì‹¬ ìˆ˜ì •] Step 2ì—ì„œë„ IN ì ˆ ì²­í¬ ë¶„í• ì„ ì ìš©í•˜ì—¬ ë©”ëª¨ë¦¬/ì¿¼ë¦¬ ë¶€í•˜ë¥¼ ì¤„ì…ë‹ˆë‹¤.
        for i in range(0, len(target_ids_list), CHUNK_SIZE):
            chunk = target_ids_list[i:i + CHUNK_SIZE]
            placeholders = ",".join("?" * len(chunk))

            label_query = f"""
                SELECT concept_id, value
                FROM literal_props
                WHERE concept_id IN ({placeholders})
                AND prop IN ('prefLabel', 'label')
                ORDER BY CASE prop WHEN 'prefLabel' THEN 1 ELSE 2 END
            """
            cursor.execute(label_query, chunk)
            label_results.extend(cursor.fetchall())
        # -------------------

        # Step 3: target_id â†’ label ë§¤í•‘
        label_map = {}
        for row in label_results:
            target_id, label = row[0], row[1]
            if target_id not in label_map and label:
                label_map[target_id] = label

        # Step 4: ìµœì¢… ê²°ê³¼ êµ¬ì„±
        result = {}
        for concept_id in concept_ids:
            related_ids = related_map.get(concept_id, [])
            related_terms = []
            for related_id in related_ids:
                label = label_map.get(related_id)
                if label:
                    related_terms.append(self._format_ksh_display(related_id, label))
            result[concept_id] = related_terms

        return result


    def _get_synonyms_batch(self, conn, concept_ids: list) -> dict:
        """âš¡ ë°°ì¹˜: ì—¬ëŸ¬ conceptì˜ ë™ì˜ì–´ë¥¼ í•œ ë²ˆì— ì¡°íšŒ"""
        if not concept_ids:
            return {}

        cursor = conn.cursor()
        placeholders = ",".join("?" * len(concept_ids))
        query = f"""
            SELECT concept_id, value
            FROM literal_props
            WHERE concept_id IN ({placeholders}) AND prop='altLabel'
        """
        cursor.execute(query, concept_ids)

        # concept_idë³„ë¡œ ê·¸ë£¹í™”
        result = {}
        for row in cursor.fetchall():
            concept_id, value = row[0], row[1]
            if concept_id not in result:
                result[concept_id] = []
            if value:
                result[concept_id].append(value)

        # ì–¸ì–´íƒœê·¸ ì¤‘ë³µ ì œê±° ì ìš©
        for concept_id in result:
            result[concept_id] = self.dedup_lang_variants(result[concept_id])

        return result


    def _process_parentheses_for_equal_terms(
        self, text: str
    ) -> str:  # âœ… selfë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        """ì›ê´„í˜¸ ì•ˆì˜ ë‚´ìš©ì„ ë™ë“±í•œ ìš©ì–´ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        if not text:
            return ""
        try:
            parentheses_pattern = r"([^()]+?)\s*\(([^()]+?)\)"

            def replace_parentheses(match):
                main_term = match.group(1).strip()
                parentheses_term = match.group(2).strip()
                if re.match(r"^[\d\s\-â€“â€”,]+$", parentheses_term):
                    return main_term
                reference_patterns = [
                    r"^(see\s+also|cf\.?|etc\.?|e\.g\.?|i\.e\.?)",
                    r"^(ì°¸ì¡°|ì°¸ê³ |ì˜ˆ|ì¦‰)",
                ]
                for pattern in reference_patterns:
                    if re.match(pattern, parentheses_term.lower()):
                        return main_term
                return f"{main_term}, {parentheses_term}"

            processed_text = text
            for _ in range(3):  # ë¬´í•œ ë£¨í”„ ë°©ì§€
                if not re.search(parentheses_pattern, processed_text):
                    break
                processed_text = re.sub(
                    parentheses_pattern, replace_parentheses, processed_text
                )
            return processed_text.strip()
        except Exception as e:
            print(f"ì›ê´„í˜¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return text


    def _save_keywords_separately(self, iri: str, ddc_code: str, raw_json: str):
        """
        âœ… [ë™ì‹œì„± ê°œì„ ] í‚¤ì›Œë“œ ì¶”ì¶œì„ íì— ì¶”ê°€í•˜ì—¬ ë¹„ë™ê¸° ì²˜ë¦¬
        ë©”ì¸ ìºì‹œ ì €ì¥ê³¼ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
        """
        try:
            # JSONì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            data = json.loads(raw_json)
            keyword_entries = []

            # prefLabel ì²˜ë¦¬
            if "prefLabel" in data and "en" in data["prefLabel"]:
                keyword = data["prefLabel"]["en"].strip()
                if keyword:
                    keyword_entries.append((iri, ddc_code, keyword, "pref"))

            # altLabel ì²˜ë¦¬
            if "altLabel" in data and "en" in data["altLabel"]:
                labels = data["altLabel"]["en"]
                alt_labels = labels if isinstance(labels, list) else [labels]
                for label in alt_labels:
                    keyword = label.strip()
                    if keyword:
                        keyword_entries.append((iri, ddc_code, keyword, "alt"))

            # í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ íì— ì¶”ê°€
            if keyword_entries:
                self.db_manager.enqueue_keyword_extraction(
                    iri, ddc_code, keyword_entries
                )

        except (json.JSONDecodeError, KeyError, TypeError) as e:
            logger.debug(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ ({ddc_code}): {e}")


    def _singularize_search_term(self, text: str) -> str:
        """
        ê²€ìƒ‰ì–´ë¥¼ KSH DBì— ë§ê²Œ ë‹¨ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        (ì˜ˆ: "Environmental problems" -> "Environmental problem")
        í•œê¸€ ë“± ì˜ì–´ ëª…ì‚¬ê°€ ì•„ë‹ˆë©´ ì›ë³¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì—¬ ì•ˆì „í•©ë‹ˆë‹¤.
        """
        if not text or re.search("[\uac00-\ud7a3]", text):
            # ë¹„ì–´ ìˆê±°ë‚˜ í•œê¸€ì´ í¬í•¨ëœ ê²½ìš° ì›ë³¸ ë°˜í™˜
            return text

        # -------------------
        # âœ… ì–¸ì–´íƒœê·¸ ì œê±° (ì˜ˆ: 'dacapo@en' -> 'dacapo')
        base, _lang = split_lang_suffix(text)
        # âœ… ê´„í˜¸/ê°ê´„í˜¸ ì•ˆ ìˆ˜ì‹ì–´ ì œê±°
        base = re.sub(r"[\(\[].*?[\)\]]", "", base).strip()
        # -------------------

        # í•œê¸€ í¬í•¨ì´ë©´ ë‹¨ìˆ˜í™” ìŠ¤í‚µ
        if re.search("[\uac00-\ud7a3]", base):
            return base

        words = base.split()
        singular_words = []
        for word in words:
            # -------------------
            # âœ… inflection.singularize() ìœ ì§€
            singular = inflection.singularize(word)
            singular_words.append(singular)
        # -------------------
        return " ".join(singular_words)


    def _sort_by_year_and_identifier(self, df):
        """ë°œí–‰ì—°ë„ ìµœì‹ ìˆœ + identifier í° ìˆœìœ¼ë¡œ ì •ë ¬ (íƒ€ì´ë¸Œë ˆì´ì»¤ ì ìš©)"""
        # -------------------
        if df.empty:
            return df

        df = df.copy()

        # ë°œí–‰ì—°ë„ ìˆ«ì ë³€í™˜
        df["year_numeric"] = pd.to_numeric(
            df["publication_year"], errors="coerce"
        ).fillna(0)

        # identifier ìˆ«ì ë³€í™˜ (íƒ€ì´ë¸Œë ˆì´ì»¤)
        df["id_numeric"] = pd.to_numeric(df["identifier"], errors="coerce").fillna(0)

        # ì •ë ¬: ë°œí–‰ì—°ë„ ë‚´ë¦¼ì°¨ìˆœ â†’ identifier ë‚´ë¦¼ì°¨ìˆœ
        df = df.sort_values(["year_numeric", "id_numeric"], ascending=[False, False])

        # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
        return df.drop(["year_numeric", "id_numeric"], axis=1)
        # -------------------


    def _strip_namespace(self, concept_id: str) -> str:
        """ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì œê±°: 'nlk:KSH123' -> 'KSH123'"""
        if not concept_id:
            return concept_id
        if ":" in concept_id:
            return concept_id.split(":", 1)[1]
        return concept_id


    def dedup_lang_variants(self, values: List[str]) -> List[str]:
        """í´ë˜ìŠ¤ ë©”ì„œë“œë¡œ ì–¸ì–´íƒœê·¸ ì¤‘ë³µ ì œê±°"""
        return dedup_lang_variants(values)


    def get_bibliographic_by_subject_name(self, subject_name):
        """
        ì£¼ì œëª…ìœ¼ë¡œ ì„œì§€ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        âœ… [ìˆ˜ì •] _search_by_korean_subjectë¥¼ ì§ì ‘ í˜¸ì¶œ (DDCë³„ ì œí•œ ì—†ì´ ì „ì²´ ì •ë ¬)
        """
        return self._search_by_korean_subject([subject_name])


    def get_bibliographic_by_title(self, title_keyword, limit=500):
        """âœ… [ì‹ ê·œ ì¶”ê°€] ì œëª©ìœ¼ë¡œ ì„œì§€ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        conn = None
        try:
            conn = self.db_manager._get_mapping_connection()

            # âœ… [ìˆ˜ì •] ì‹¤ì œ í…Œì´ë¸” ì»¬ëŸ¼ëª… ì‚¬ìš©
            query = """
            SELECT
                identifier,
                kdc,
                ddc,
                ksh,
                kdc_edition,
                ddc_edition,
                publication_year,
                title,
                data_type,
                source_file,
                ksh_labeled,
                ksh_korean
            FROM mapping_data
            WHERE title LIKE ?
            ORDER BY publication_year DESC, identifier
            LIMIT ?
            """

            df = pd.read_sql_query(query, conn, params=(f"%{title_keyword}%", limit))

            if df.empty:
                return pd.DataFrame()

            # âœ… [í•µì‹¬] KSH ë¼ë²¨ í¬ë§·íŒ… - ksh_labeled ì»¬ëŸ¼ì„ ë®ì–´ì”€ (ìƒˆ ì»¬ëŸ¼ ì¶”ê°€ X)
            if not df.empty and "ksh" in df.columns:
                df["ksh_labeled"] = df.apply(
                    lambda row: self._format_ksh_column_optimized(
                        row.get("ksh", ""),
                        row.get("ksh_korean", ""),
                        row.get("ksh_labeled", ""),
                    ),
                    axis=1,
                )

            # âœ… [ì‹ ê·œ] DDC Label ì»¬ëŸ¼ ì¶”ê°€ (ê¸°ì¡´ ì„œì§€ ê²€ìƒ‰ê³¼ ë™ì¼)
            if "ddc" in df.columns:
                df["ddc_label"] = df["ddc"].apply(
                    lambda ddc: (
                        self.get_ddc_labels(ddc)
                        if pd.notna(ddc) and str(ddc).strip()
                        else ""
                    )
                )

            # âœ… [ì¤‘ìš”] NLK ë§í¬ ìƒì„± (ê¸°ì¡´ ì„œì§€ ê²€ìƒ‰ê³¼ ë™ì¼)
            if "identifier" in df.columns:
                df["nlk_link"] = df["identifier"].apply(
                    lambda x: (
                        f"https://www.nl.go.kr/NL/contents/search.do?systemType=&pageNum=1&pageSize=10&srchTarget=total&kwd={x}"
                        if x
                        else ""
                    )
                )

            # âœ… [ì¤‘ìš”] ksh, ksh_korean ì»¬ëŸ¼ ì œê±° (UIì— ë¶ˆí•„ìš”)
            df.drop(columns=["ksh", "ksh_korean"], inplace=True, errors="ignore")

            # âœ… [í•µì‹¬] ì»¬ëŸ¼ëª…ì„ ë³€í™˜í•˜ì§€ ì•Šê³  DB ì»¬ëŸ¼ëª… ê·¸ëŒ€ë¡œ ë°˜í™˜
            # Qt íƒ­ì—ì„œ column_map_bottomì„ í†µí•´ ìë™ìœ¼ë¡œ ë§¤í•‘ë¨
            return df

        except Exception as e:
            print(f"ì œëª© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback

            traceback.print_exc()
            return pd.DataFrame()
        finally:
            if conn:
                conn.close()


    def preprocess_search_term(self, raw_text: str) -> str:
        """
        [ìµœì¢… ê°•í™” ë²„ì „] ëª¨ë“  ê²€ìƒ‰ì–´ ì…ë ¥ì„ ì •ê·œí™”í•˜ëŠ” ì¤‘ì•™ í•¨ìˆ˜.
        - ë‹¤ì¤‘ â–¼a íŒ¨í„´, CJK ê³µë°±, Dewey ì„¤ëª…ë¬¸, KSH ë§ˆí¬ì—…, ì˜ë‹¨ì–´ ë‹¨ìˆ˜í™” ëª¨ë‘ ì²˜ë¦¬
        """
        if not raw_text or pd.isna(raw_text):
            return ""

        text = str(raw_text).strip()

        # 1ë‹¨ê³„: ë‹¤ì¤‘ â–¼a íŒ¨í„´ ì²˜ë¦¬ (â–¼aê±´ê°•ê´€ë¦¬â–¼aëŸ¬ë‹â–¼aìš´ë™ë²• íŒ¨í„´)
        if "â–¼a" in text and text.count("â–¼a") > 1:
            keywords = []
            pattern = r"â–¼a([^â–¼â–²]+)"
            matches = re.findall(pattern, text)

            for match in matches:
                # ê° í‚¤ì›Œë“œì—ì„œ ë¶ˆí•„ìš”í•œ ê¸°í˜¸ì™€ ê³µë°± ì •ë¦¬
                cleaned_keyword = re.sub(r"[â–¼â–²]", "", match)
                cleaned_keyword = re.sub(
                    r"\[.*?\]|\(.*?\)", "", cleaned_keyword
                ).strip()

                # í•œê¸€/í•œìê°€ í¬í•¨ë˜ë©´ ë‚´ë¶€ ê³µë°± ëª¨ë‘ ì œê±° (ì˜ˆ: 'ìê¸° ê³„ë°œ' -> 'ìê¸°ê³„ë°œ')
                if re.search(r"[\uAC00-\uD7AF\u4E00-\u9FFF]", cleaned_keyword):
                    cleaned_keyword = re.sub(r"\s+", "", cleaned_keyword)
                else:
                    # ì˜ë‹¨ì–´ ë“± ë¹„CJKëŠ” ë‹¤ì¤‘ ê³µë°±ë§Œ ë‹¨ì¼í™”
                    cleaned_keyword = re.sub(r"\s{2,}", " ", cleaned_keyword)

                if cleaned_keyword:
                    keywords.append(cleaned_keyword)

            if keywords:
                # ìµœì¢…ì ìœ¼ë¡œ ì˜ë‹¨ì–´ ë‹¨ìˆ˜í™” ì ìš© í›„ ì‰¼í‘œë¡œ ì—°ê²°
                singular_keywords = []
                for kw in keywords:
                    if not re.search(r"[\uAC00-\uD7AF\u4E00-\u9FFF]", kw):
                        singular_keywords.append(
                            " ".join(
                                [inflection.singularize(word) for word in kw.split()]
                            )
                        )
                    else:
                        singular_keywords.append(kw)
                return ", ".join(singular_keywords)

        # 2ë‹¨ê³„: ì¼ë°˜ ë§ˆí¬ì—… ë˜ëŠ” Dewey ì„¤ëª…ë¬¸ ë“± ë³µì¡í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if any(c in text for c in "â–¼â–²[]();:"):
            # KSH ë§ˆí¬ì—… ì œê±°
            if "â–¼a" in text and "â–¼0" in text:
                match = re.search(r"â–¼a([^â–¼]+)â–¼0", text)
                if match:
                    text = match.group(1)

            # Dewey ì„¤ëª…ë¬¸ ì •ê·œí™”
            text = self._process_parentheses_for_equal_terms(text)
            text = text.replace("--", ", ")
            text = re.sub(r"\s+(and|&)\s+", ", ", text, flags=re.IGNORECASE)
            text = re.sub(r"\b\d{4}[-â€âˆ’â€“â€”]?\d*[-â€âˆ’â€“â€”]?\b", "", text)
            text = re.sub(r"[;:]+", ",", text)

            # ì¼ë°˜ ì •ê·œí™”
            base, _lang = split_lang_suffix(text)
            base = re.sub(r"\[.*?\]|\(.*?\)", "", base).strip()

            # -------------------
            # âœ… [í•µì‹¬ ìˆ˜ì •] í•œê¸€ì´ í¬í•¨ëœ ê²½ìš°, ë¬¸ì œê°€ ë˜ëŠ” ì •ê·œì‹ì„ ê±´ë„ˆë›°ê³  ì¦‰ì‹œ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •
            if re.search(r"[\uAC00-\uD7AF\u4E00-\u9FFF]", base):
                # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë‹¤ì¤‘ ê²€ìƒ‰ì–´ì˜ ê²½ìš°, ê° í•­ëª©ì˜ ê³µë°±ì„ ì œê±°
                parts = [re.sub(r'\s+', '', part.strip()) for part in base.split(',') if part.strip()]
                return ", ".join(parts)

            base = re.sub(r"[^\w\s,\-]", "", base) # <- ì´ì œ ì´ ì½”ë“œëŠ” í•œê¸€ì´ ì—†ì„ ë•Œë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
            base = re.sub(r"[,]+", ",", base)
            # -------------------
            base = re.sub(r"\s+", " ", base).strip()

            # í•œê¸€/í•œì í¬í•¨ ì‹œ ë‚´ë¶€ ê³µë°± ì œê±°
            if re.search(r"[\uAC00-\uD7AF\u4E00-\u9FFF]", base):
                return re.sub(r"\s+", "", base)

            # ì˜ë‹¨ì–´ ë‹¨ìˆ˜í™”
            words = base.split()
            singular_words = [inflection.singularize(word) for word in words]
            return " ".join(singular_words)

        # 3ë‹¨ê³„: ìˆœìˆ˜ í‚¤ì›Œë“œ (ê°€ì¥ ì¼ë°˜ì ì¸ ê²½ìš°)
        else:
            # í•œê¸€/í•œì í¬í•¨ ì‹œ ë‚´ë¶€ ê³µë°± ì œê±°
            if re.search(r"[\uAC00-\uD7AF\u4E00-\u9FFF]", text):
                return re.sub(r"\s+", "", text)

            # ì˜ë‹¨ì–´ ë‹¨ìˆ˜í™”
            words = text.split()
            singular_words = [inflection.singularize(word) for word in words]
            return " ".join(singular_words)

    # âœ… [ì¶”ê°€] database_manager.pyì—ì„œ ì´ê´€ëœ DDC notation ìºì‹œ ì¡°íšŒ ë©”ì„œë“œ

    def search_bibliographic_by_subject_optimized(self, subject_name, total_limit=200):
        """
        ğŸ¯ ë©”ë¥´ì¹´ì¸ ë‹˜ ìš”êµ¬ì‚¬í•­ì— ë§ì¶˜ ì„œì§€DB ê²€ìƒ‰:
        1. DDC ë¹ˆë„ ê¸°ì¤€ ì •ë ¬ (ê°€ì¥ í”í•œ DDC ë¨¼ì €)
        2. ê° DDCë‹¹ 5-20ê±´ ì œí•œ (ë¶„ë¥˜ ê°œìˆ˜ì— ë”°ë¼ ë™ì  ì¡°ì ˆ)
        3. 2:3 ë¹„ìœ¨ (ë‹¨ì¼KSH:ë‹¤ì¤‘KSH) ìœ ì—° ì ìš©
        4. ë°œí–‰ì—°ë„ ìµœì‹ ìˆœ + identifier íƒ€ì´ë¸Œë ˆì´ì»¤
        """
        conn = None
        try:
            conn = self.db_manager._get_mapping_connection()

            # 1ë‹¨ê³„: ì£¼ì œëª… ê¸°ë³¸ ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§ í™œìš©)
            base_results = self._search_by_korean_subject([subject_name])

            if base_results.empty:
                return pd.DataFrame()

            # 2ë‹¨ê³„: DDCë³„ ë¹ˆë„ ê³„ì‚°
            ddc_counts = base_results["ddc"].value_counts()
            base_results["ddc_count"] = base_results["ddc"].map(ddc_counts)
            total_ddcs = len(ddc_counts)

            # âœ… [ìµœì í™” 1] ë””ë²„ê·¸ ë¡œê·¸ ì œê±° (0.3-0.5s ì ˆê°)
            # print(f"ğŸ¯ [DDC_FREQ] ë°œê²¬ëœ DDC ë¶„ë¥˜: {total_ddcs}ê°œ")
            # print(f"ğŸ¯ [DDC_FREQ] ìƒìœ„ 5ê°œ DDC: {dict(ddc_counts.head())}")

            # 3ë‹¨ê³„: ë¶„ë¥˜ë‹¹ í• ë‹¹ëŸ‰ ê³„ì‚°
            # âœ… [í•µì‹¬ ìˆ˜ì •] DDCë‹¹ ìµœëŒ€ 10ê°œë¡œ ê³ ì • (ë‹¨ì¼ 3:ë³µìˆ˜ 7 ë¹„ìœ¨)
            items_per_ddc = 10
            # âœ… [ìµœì í™” 1] ë””ë²„ê·¸ ë¡œê·¸ ì œê±°
            # print(
            #     f"ğŸ¯ [ALLOCATION] DDCë‹¹ í• ë‹¹ëŸ‰: {items_per_ddc}ê±´ (ë‹¨ì¼ ìµœëŒ€ 3 : ë³µìˆ˜ ìµœëŒ€ 7)"
            # )

            # 4ë‹¨ê³„: DDCë³„ ë°ì´í„° ì²˜ë¦¬
            final_results = []
            processed_count = 0

            for ddc_code, frequency in ddc_counts.items():
                if processed_count >= total_limit:
                    break

                # âœ… [ìµœì í™” 2] .copy() ì œê±° - ë·° ì‚¬ìš© (0.2-0.3s ì ˆê°)
                # í•´ë‹¹ DDCì˜ ëª¨ë“  ë°ì´í„°
                ddc_subset = base_results[base_results["ddc"] == ddc_code]

                # âœ… [ìµœì í™” 3] ksh_countëŠ” SQLì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨ (0.1-0.2s ì ˆê°)
                # KSH ê°œìˆ˜ ê³„ì‚° ë° ë¶„í•  (ksh_count ì»¬ëŸ¼ì€ SQL ì¿¼ë¦¬ì—ì„œ ì´ë¯¸ ìƒì„±ë¨)
                single_ksh_data = ddc_subset[ddc_subset["ksh_count"] == 1]
                multi_ksh_data = ddc_subset[ddc_subset["ksh_count"] > 1]

                # ê°ê° ë°œí–‰ì—°ë„ + identifier ê¸°ì¤€ ì •ë ¬
                # -------------------
                single_ksh_data = self._sort_by_year_and_identifier(single_ksh_data)
                multi_ksh_data = self._sort_by_year_and_identifier(multi_ksh_data)
                # -------------------

                # ğŸ¯ 3:7 ë¹„ìœ¨ ì ìš© (ë‹¨ì¼ KSH ìµœëŒ€ 3ê°œ : ë³µìˆ˜ KSH ìµœëŒ€ 7ê°œ)
                # items_per_ddcê°€ 10ì´ë©´ ë‹¨ì¼ 3ê°œ, ë³µìˆ˜ 7ê°œ
                target_single = min(3, (items_per_ddc * 3) // 10, len(single_ksh_data))
                remaining = items_per_ddc - target_single
                target_multi = min(remaining, len(multi_ksh_data))

                # ë‚¨ì€ í• ë‹¹ëŸ‰ì´ ìˆìœ¼ë©´ ë‹¤ë¥¸ ìª½ì—ì„œ ì±„ì›€
                if target_single + target_multi < items_per_ddc:
                    if len(single_ksh_data) > target_single:
                        additional = min(
                            items_per_ddc - target_multi,
                            len(single_ksh_data) - target_single,
                        )
                        target_single += additional
                    elif len(multi_ksh_data) > target_multi:
                        additional = min(
                            items_per_ddc - target_single,
                            len(multi_ksh_data) - target_multi,
                        )
                        target_multi += additional

                # ìµœì¢… ì„ ë³„
                selected_single = (
                    single_ksh_data.head(target_single)
                    if target_single > 0
                    else pd.DataFrame()
                )
                selected_multi = (
                    multi_ksh_data.head(target_multi)
                    if target_multi > 0
                    else pd.DataFrame()
                )

                # DDCë³„ ê²°ê³¼ ë³‘í•©
                ddc_result = pd.concat(
                    [selected_single, selected_multi], ignore_index=True
                )
                if not ddc_result.empty:
                    ddc_result["ddc_frequency_rank"] = len(
                        final_results
                    )  # ë¹ˆë„ ìˆœìœ„ ê¸°ë¡
                    final_results.append(ddc_result)
                    processed_count += len(ddc_result)

                # âœ… [ìµœì í™” 1] ë””ë²„ê·¸ ë¡œê·¸ ì œê±° (DDC ë£¨í”„ë§ˆë‹¤ 118íšŒ ì¶œë ¥ â†’ ì œê±°)
                # print(
                #     f"ğŸ¯ [DDC:{ddc_code}] ë‹¨ì¼KSH:{target_single}ê±´, ë‹¤ì¤‘KSH:{target_multi}ê±´ (ë¹ˆë„:{frequency})"
                # )

            # 5ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ìƒì„±
            if final_results:
                result_df = pd.concat(final_results, ignore_index=True)

                # âœ… [ë””ë²„ê¹…] ìµœì¢… ê²°ê³¼ í™•ì¸
                # logger.info(f"ğŸ› [DEBUG] ìµœì¢… ê²°ê³¼ ìƒì„± - í–‰ ìˆ˜: {len(result_df)}")
                # if not result_df.empty and "ksh_labeled" in result_df.columns:
                #    unique_ksh_labeled = result_df["ksh_labeled"].nunique()
                #    logger.info(f"ğŸ› [DEBUG] ê³ ìœ í•œ ksh_labeled ê°’ ê°œìˆ˜: {unique_ksh_labeled}")
                #    logger.info(f"ğŸ› [DEBUG] ìƒ˜í”Œ ksh_labeled ê°’ (ìµœì´ˆ 3ê°œ):\n{result_df['ksh_labeled'].head(3).tolist()}")

                # ksh_labeled ì»¬ëŸ¼ ìƒì„± (í‘œì‹œìš©)
                if "ksh" in result_df.columns:
                    result_df["ksh_labeled"] = result_df.apply(
                        lambda row: self._format_ksh_labeled_to_markup(
                            row.get("ksh_labeled", ""), row.get("ksh", "")
                        ),
                        axis=1,
                    )

                    # âœ… [ë””ë²„ê¹…] ë³€í™˜ í›„ í™•ì¸
                    # unique_after = result_df["ksh_labeled"].nunique()
                    # logger.info(f"ğŸ› [DEBUG] ë³€í™˜ í›„ ê³ ìœ í•œ ksh_labeled ê°’ ê°œìˆ˜: {unique_after}")
                    # logger.info(f"ğŸ› [DEBUG] ë³€í™˜ í›„ ìƒ˜í”Œ ksh_labeled ê°’ (ìµœì´ˆ 3ê°œ):\n{result_df['ksh_labeled'].head(3).tolist()}")

                # âœ… [ì‹ ê·œ] DDC Label ì»¬ëŸ¼ ì¶”ê°€
                if "ddc" in result_df.columns:
                    result_df["ddc_label"] = result_df["ddc"].apply(
                        lambda ddc: (
                            self.get_ddc_labels(ddc)
                            if pd.notna(ddc) and str(ddc).strip()
                            else ""
                        )
                    )

                # âœ… nlk_link ì»¬ëŸ¼ ìƒì„± (identifier ê¸°ë°˜)
                if "identifier" in result_df.columns:
                    result_df["nlk_link"] = result_df["identifier"].apply(
                        lambda x: (
                            f"https://www.nl.go.kr/NL/contents/search.do?systemType=&pageNum=1&pageSize=10&srchTarget=total&kwd={x}"
                            if x
                            else ""
                        )
                    )

                # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
                result_df = result_df.drop(
                    ["ddc_frequency_rank"], axis=1, errors="ignore"
                )

                print(f"ğŸ¯ [FINAL] ìµœì¢… ì„œì§€ ê²°ê³¼: {len(result_df)}ê±´")
                return result_df.fillna("")
            else:
                return pd.DataFrame()

        except Exception as e:
            print(f"ì˜¤ë¥˜: ìµœì í™”ëœ ì„œì§€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return pd.DataFrame()
        finally:
            if conn:
                conn.close()

