# Search_Gemini.py (ê³„ì¸µì  ê²€ì¦ ì‹œìŠ¤í…œ)
# -*- coding: utf-8 -*-
import os
import json

# âœ… [ì¶”ê°€] PyInstaller í™˜ê²½ì—ì„œ SSL ì¸ì¦ì„œ ê²½ë¡œ ì„¤ì •
from ssl_cert_utils import configure_ssl_certificates
configure_ssl_certificates()

import requests
import logging
import re
from typing import Any, Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from database_manager import DatabaseManager
from search_query_manager import SearchQueryManager  # âœ… ìµœì‹  DDC ìºì‹œ/ì¡°íšŒ ê²½ìœ  ê³„ì¸µ


logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

JSON_BLOCK_RE = re.compile(r"```(?:json)?\s*(\{.*?\})\s*```", re.DOTALL | re.IGNORECASE)


def _strip_code_fences(text: str) -> str:
    if not text:
        return ""
    m = JSON_BLOCK_RE.search(text)
    if m:
        return m.group(1).strip()
    # ë°±í‹± ì—†ëŠ” ê²½ìš°: ì²˜ìŒ '{'ë¡œ ì‹œì‘í•˜ëŠ” JSON ë¸”ë¡ë§Œ ì¶”ì¶œ
    first = text.find("{")
    last = text.rfind("}")
    if first != -1 and last != -1 and last > first:
        candidate = text[first : last + 1].strip()
        return candidate
    return text.strip()


def _safe_json_loads(text: str) -> Optional[dict]:
    if not text:
        return None
    s = _strip_code_fences(text)
    try:
        return json.loads(s)
    except Exception:
        # í”í•œ íŒ¨í„´: íŠ¸ë ˆì¼ë§ ì½¤ë§ˆ, ì˜ëª»ëœ ë”°ì˜´í‘œ â†’ ë³´ì • ì‹œë„
        s2 = re.sub(r",\s*}", "}", s)
        s2 = re.sub(r",\s*]", "]", s2)
        try:
            return json.loads(s2)
        except Exception:
            return None


def _fallback_hierarchy_from_text(review_text: str) -> dict:
    """
    ëª¨ë¸ JSON íŒŒì‹±ì´ ì‹¤íŒ¨í–ˆì„ ë•Œ ìµœì†Œ ë™ì‘ ë³´ì¥ì„ ìœ„í•œ í´ë°±.
    - ë§ˆì¹¨í‘œ/ì‰¼í‘œ/ê°œí–‰ ê¸°ì¤€ìœ¼ë¡œ í† í°í™” í›„, ê¸¸ì´ 1~8ì˜ í•œê¸€/ì˜ë¬¸ ë‹¨ì–´ ì¶”ì¶œ
    - ìƒìœ„: ë¹ˆë„ ìƒìœ„ 2~3ê°œ, ì¤‘ê°„: ë‹¤ìŒ 3~4ê°œ, í•˜ìœ„: ëª…ì‚¬/ê¸°ìˆ ì–´ë¡œ ì¶”ì •ë˜ëŠ” 2~3ê°œ
    """
    txt = (review_text or "").strip()
    tokens = re.findall(r"[A-Za-zê°€-í£0-9\-]{2,}", txt)
    # ê°„ë‹¨ ë¹ˆë„ ì§‘ê³„
    freq = {}
    for t in tokens:
        t = t.lower()
        freq[t] = freq.get(t, 0) + 1
    ranked = [w for w, _ in sorted(freq.items(), key=lambda kv: (-kv[1], kv[0]))]
    broad = ranked[:3]
    specific = ranked[3:7]
    technical = [
        w
        for w in ranked
        if re.search(
            r"(í•™|ë¡ |ë²•|ëª¨í˜•|ëª¨ë¸|ì•Œê³ ë¦¬ì¦˜|ë¶„ì„|ì‹¤í—˜|ë°ì´í„°|ë¯¸ë””ì–´|ì‹¬ë¦¬|ì‚¬íšŒ|ê³µí•™|neural|model|algorithm|analysis)$",
            w,
        )
    ][:3]
    if not technical:
        technical = ranked[7:10]
    return {
        "broad": broad or ["ì¼ë°˜"],
        "specific": specific or broad,
        "technical": technical or specific or broad,
    }


class SearchGemini:
    """
    Gemini APIë¥¼ ì‚¬ìš©í•œ ê³„ì¸µì  DDC ë¶„ë¥˜ + ì˜ë¯¸ ê²€ì¦ ì‹œìŠ¤í…œ
    """

    def __init__(self, db_manager: DatabaseManager, app_instance=None):
        """
        SearchGemini í´ë˜ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        self.db_manager = db_manager
        self.app_instance = app_instance
        self.query_manager = SearchQueryManager(
            db_manager
        )  # âœ… ìºì‹œ/ì¿¼ë¦¬ ì¼ì›í™” ì§„ì…ì 
        self.api_key = self._get_gemini_api_key()
        self.api_url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"

    def _log(self, message: str, level: str = "INFO"):
        """ì•±ê³¼ ì½˜ì†” ëª¨ë‘ì— ë¡œê·¸ ì¶œë ¥"""
        logging_func = getattr(logging, level.lower(), logging.info)
        logging_func(message)
        if self.app_instance and hasattr(self.app_instance, "log_message"):
            self.app_instance.log_message(message, level)

    def _get_gemini_api_key(self):
        """DatabaseManagerì—ì„œ Gemini API í‚¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        api_key = self.db_manager.get_gemini_api_key()
        if not api_key:
            logging.error("Gemini API í‚¤ê°€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            raise ConnectionRefusedError(
                "Gemini API í‚¤ê°€ ë°ì´í„°ë² ì´ìŠ¤ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'API í‚¤ ì„¤ì •'ì„ í†µí•´ ë¨¼ì € ì„¤ì •í•´ì£¼ì„¸ìš”."
            )
        return api_key

    def _extract_hierarchical_keywords(self, bundle_text: str) -> dict:
        """
        ê³„ì¸µì  í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ì¼ ì…ë ¥ì°½: ì €ìì •ë³´/ëª©ì°¨/ì„œí‰ í˜¼í•© í…ìŠ¤íŠ¸):
        - ì‚¬ìš©ìëŠ” ì €ìì •ë³´/ëª©ì°¨/ì„œí‰ì„ í•œ ë²ˆì— ë¶™ì—¬ë„£ëŠ”ë‹¤ (í‘œì œê°€ ì—†ì–´ë„ ë‚´ìš© ë‹¨ì„œë¡œ êµ¬ë¶„ ì‹œë„)
        - Gemini ì‘ë‹µì´ JSONì´ ì•„ë‹ ë•Œ(ë¹ˆ ë¬¸ìì—´/ì½”ë“œíœìŠ¤/í”„ë¦¬ì•°ë¸”/HTML)ë„ ê²¬ê³ í•˜ê²Œ ì²˜ë¦¬
        - íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ í´ë°±ìœ¼ë¡œ í•­ìƒ {"broad":[], "specific":[], "technical":[]} ë°˜í™˜
        """

        # ----- ë‚´ë¶€ í—¬í¼: ë²ˆë“¤ í…ìŠ¤íŠ¸ ì„¹ì…˜ ì¶”ì • ë¶„ë¦¬ -----
        def _split_bundle_sections(txt: str) -> dict:
            """
            ë²ˆë“¤ í…ìŠ¤íŠ¸ì—ì„œ (ê°€ëŠ¥í•˜ë©´) ì €ìì •ë³´/ëª©ì°¨/ì„œí‰ì„ ì¶”ì • ë¶„ë¦¬.
            - í‘œì œê°€ ì—†ëŠ” ê²½ìš°ë„ ê³ ë ¤: ë‹¨ì„œ ê¸°ë°˜ ê°„ì´ ê·œì¹™
            - ë°˜í™˜: {"author": "...", "toc": "...", "review": "..."}
            """
            s = txt or ""
            # 1) ëª…ì‹œì  ë¼ë²¨ ì‹œë„ (ì˜ˆ: ì €ìì •ë³´: ... / ëª©ì°¨: ... / ì„œí‰: ...)
            pat = re.compile(
                r"(ì €ì\s*ì •ë³´|ì €ìì •ë³´|ì €ì|ëª©ì°¨|ì„œí‰)\s*[:ï¼š]\s*", re.IGNORECASE
            )
            parts = pat.split(s)
            sections = {"author": "", "toc": "", "review": ""}
            if len(parts) >= 3:
                buf = {
                    "ì €ìì •ë³´": "author",
                    "ì €ì": "author",
                    "ì €ì ì •ë³´": "author",
                    "ëª©ì°¨": "toc",
                    "ì„œí‰": "review",
                }
                it = iter(parts[1:])  # [label1, text1, label2, text2, ...]
                for label, chunk in zip(it, it):
                    key = buf.get(label.strip().lower().replace(" ", ""), None)
                    if key:
                        sections[key] += chunk.strip() + "\n"
                if any(v.strip() for v in sections.values()):
                    return sections

            # 2) ë‹¨ì„œ ê¸°ë°˜ ì¶”ì •
            lines = [ln.strip() for ln in s.splitlines() if ln.strip()]
            author_lines, toc_lines, review_lines = [], [], []
            for ln in lines:
                if re.search(r"(ì €ì|ì•½ë ¥|êµìˆ˜|ì—°êµ¬ì›|ì†Œì†|ë°•ì‚¬|ì„ì‚¬|ê²½ë ¥)", ln):
                    author_lines.append(ln)
                    continue
                if re.search(
                    r"(ëª©ì°¨|^ì œ\s*\d+\s*ì¥|^chapter\s*\d+|^part\s*\d+|\.\.\.\s*\d+$)",
                    ln,
                    re.IGNORECASE,
                ):
                    toc_lines.append(ln)
                    continue
                review_lines.append(ln)
            sections["author"] = "\n".join(author_lines).strip()
            sections["toc"] = "\n".join(toc_lines).strip()
            sections["review"] = "\n".join(review_lines).strip()
            return sections

        # ----- í´ë°±: ì„¹ì…˜ ê°€ì¤‘ì¹˜ ê¸°ë°˜ n-gram ë¹ˆë„ -----
        def _fallback_hierarchy_from_text(txt: str) -> dict:
            sec = _split_bundle_sections(txt)
            # ì„¹ì…˜ë³„ ê°€ì¤‘ì¹˜: ëª©ì°¨(1.2), ì„œí‰(1.0), ì €ìì •ë³´(0.8)
            weighted_tokens: list[str] = []
            for src, w in (("toc", 1.2), ("review", 1.0), ("author", 0.8)):
                tokens = re.findall(r"[A-Za-zê°€-í£0-9\-]{2,}", sec.get(src, ""))
                # ê°€ì¤‘ì¹˜ë§Œí¼ ì¤‘ë³µ ì‚½ì… â†’ ì´í›„ ì¹´ìš´íŒ…ì—ì„œ ìì—° ë°˜ì˜
                for t in tokens:
                    n = max(1, int(w * 5))
                    weighted_tokens.extend([t] * n)

            freq: dict[str, int] = {}
            for t in weighted_tokens:
                key = t.lower()
                freq[key] = freq.get(key, 0) + 1

            ranked = [
                w for w, _ in sorted(freq.items(), key=lambda kv: (-kv[1], kv[0]))
            ]
            broad = ranked[:3]
            specific = ranked[3:7]
            tech_guess = [
                w
                for w in ranked
                if re.search(
                    r"(í•™|ë¡ |ë²•|ëª¨í˜•|ëª¨ë¸|ì•Œê³ ë¦¬ì¦˜|ë¶„ì„|ì‹¤í—˜|ë°ì´í„°|ë¯¸ë””ì–´|ì‹¬ë¦¬|ì‚¬íšŒ|ê³µí•™|neural|model|algorithm|analysis)$",
                    w,
                )
            ]
            technical = tech_guess[:3] if tech_guess else ranked[7:10]
            # âœ… [ìˆ˜ì •] ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ê²Œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
            return {
                "broad": {"korean": broad or ["ì¼ë°˜"], "english": []},
                "specific": {"korean": specific or broad or ["ì¼ë°˜"], "english": []},
                "technical": {"korean": technical or specific or broad or ["ì¼ë°˜"], "english": []},
            }

        def _norm_list(v) -> list[str]:
            """ë¬¸ìì—´/ë¦¬ìŠ¤íŠ¸ ì–´ë–¤ í˜•ì‹ì´ ì™€ë„ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ê·œí™”"""
            if v is None:
                return []
            if isinstance(v, str):
                return [x.strip() for x in v.split(",") if x.strip()]
            if isinstance(v, list):
                return [str(x).strip() for x in v if str(x).strip()]
            return []

        # ----- í”„ë¡¬í”„íŠ¸ êµ¬ì„± (í•œêµ­ì–´/ì˜ì–´ í‚¤ì›Œë“œ ë¶„ë¦¬ ì¶”ì¶œ) -----
        try:
            if hasattr(self, "_build_hierarchy_prompt"):
                prompt = self._build_hierarchy_prompt(bundle_text)
            else:
                prompt = (
                    "ë‹¤ìŒ 'ë‹¨ì¼ ì…ë ¥'ì—ì„œ ì €ìì •ë³´/ëª©ì°¨/ì„œí‰ì„ ê°€ëŠ¥í•œ í•œ ì‹ë³„/ì¶”ë¡ í•˜ê³ , "
                    "ì„¸ ì„¹ì…˜ ëª¨ë‘ë¥¼ ë°˜ì˜í•´ ê³„ì¸µì  í‚¤ì›Œë“œë¥¼ í•œêµ­ì–´ì™€ ì˜ì–´ë¡œ ê°ê° ì¶”ì¶œí•˜ì—¬ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.\n\n"
                    "ì‘ë‹µ í˜•ì‹:\n"
                    "{\n"
                    '  "broad": {"korean": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...ìµœëŒ€5ê°œ], "english": ["keyword1", "keyword2", ...ìµœëŒ€5ê°œ]},\n'
                    '  "specific": {"korean": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...ìµœëŒ€5ê°œ], "english": ["keyword1", "keyword2", ...ìµœëŒ€5ê°œ]},\n'
                    '  "technical": {"korean": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...ìµœëŒ€5ê°œ], "english": ["keyword1", "keyword2", ...ìµœëŒ€5ê°œ]}\n'
                    "}\n\n"
                    "ìš”êµ¬ì‚¬í•­:\n"
                    "- ê° ë¶„ë¥˜(broad, specific, technical)ë‹¹ í•œêµ­ì–´ í‚¤ì›Œë“œ ìµœëŒ€ 5ê°œ, ì˜ì–´ í‚¤ì›Œë“œ ìµœëŒ€ 5ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.\n"
                    "- í‘œì œê°€ ì—†ì–´ë„ ë‚´ìš© ë‹¨ì„œ(ì•½ë ¥/ì†Œì†/ê²½ë ¥, ì¥/ì ˆ/í˜ì´ì§€, ë…ìí‰/ë‚´ìš©ìš”ì•½)ë¥¼ í™œìš©í•´ êµ¬ë¶„í•˜ì„¸ìš”.\n"
                    "- ì˜ì–´ í‚¤ì›Œë“œëŠ” DDC(ë“€ì´ì‹­ì§„ë¶„ë¥˜ë²•) ê²€ìƒ‰ì— ì í•©í•œ ìš©ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n\n"
                    f"ë‹¨ì¼ ì…ë ¥:\n{bundle_text}\n"
                )
        except Exception as e:
            logging.error(f"[Gemini] í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì‹¤íŒ¨: {e}")
            return _fallback_hierarchy_from_text(bundle_text)

        # ----- ëª¨ë¸ í˜¸ì¶œ -----
        try:
            raw = self._call_gemini_api(prompt)
        except Exception as e:
            logging.error(f"[Gemini] ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return _fallback_hierarchy_from_text(bundle_text)

        if not raw or not str(raw).strip():
            logging.error("[Gemini] í‚¤ì›Œë“œ ì¶”ì¶œ ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return _fallback_hierarchy_from_text(bundle_text)

        # ----- JSON íŒŒì‹± (ê²¬ê³ ) - í•œêµ­ì–´/ì˜ì–´ ë¶„ë¦¬ êµ¬ì¡° -----
        parsed = _safe_json_loads(str(raw))
        if not parsed:
            logging.error(
                f"[Gemini] í‚¤ì›Œë“œ ì¶”ì¶œ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨. ì›ë¬¸ ì¼ë¶€: {str(raw)[:200]!r}"
            )
            return _fallback_hierarchy_from_text(bundle_text)

        # ìƒˆë¡œìš´ êµ¬ì¡°: {"broad": {"korean": [...], "english": [...]}, ...}
        hierarchy = {}
        for level in ["broad", "specific", "technical"]:
            level_data = parsed.get(level, {})
            if isinstance(level_data, dict):
                hierarchy[level] = {
                    "korean": _norm_list(level_data.get("korean", []))[:5],  # ìµœëŒ€ 5ê°œ
                    "english": _norm_list(level_data.get("english", []))[
                        :5
                    ],  # ìµœëŒ€ 5ê°œ
                }
            else:
                # êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¥´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸
                hierarchy[level] = {"korean": [], "english": []}

        # ìµœì†Œ ë³´ì¥: ë¹„ì–´ ìˆìœ¼ë©´ í´ë°±
        all_empty = all(not (v["korean"] or v["english"]) for v in hierarchy.values())
        if all_empty:
            logging.warning("[Gemini] íŒŒì‹± ê²°ê³¼ê°€ ë¹„ì–´ ìˆì–´ í´ë°± ì‚¬ìš©")
            # âœ… [ìˆ˜ì •] fallbackì´ ì´ë¯¸ ì˜¬ë°”ë¥¸ êµ¬ì¡°ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ì§ì ‘ ì‚¬ìš©
            hierarchy = _fallback_hierarchy_from_text(bundle_text)

        return hierarchy

    def _search_korean_keyword(
        self, keywords: str, level_name: str, max_results: int = 3
    ) -> list:
        results = []
        error_occurred = False  # ì˜¤ë¥˜ ë°œìƒ ì—¬ë¶€ í”Œë˜ê·¸

        try:
            # ì½¤ë§ˆë¡œ ë¶„ë¦¬
            keyword_list = [kw.strip() for kw in keywords.split(",") if kw.strip()]
            if not keyword_list:
                self._log(
                    f"âš ï¸ [í•œêµ­ì–´ ê²€ìƒ‰] '{keywords}': ìœ íš¨í•œ í‚¤ì›Œë“œ ì—†ìŒ", "WARNING"
                )
                return results

            self._log(
                f"ğŸ” [í•œêµ­ì–´ ê²€ìƒ‰] {level_name}: {len(keyword_list)}ê°œ í‚¤ì›Œë“œ ë™ì‹œ ê²€ìƒ‰ ì‹œì‘",
                "INFO",
            )
            self._log(f"   ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keyword_list)}", "INFO")

            ddc_aggregation = {}

            # âœ… [ì„±ëŠ¥ ê°œì„ ] ì—¬ëŸ¬ í‚¤ì›Œë“œë¥¼ í•œ ë²ˆì— ì¡°íšŒí•˜ëŠ” ìµœì í™” ë©”ì„œë“œ í˜¸ì¶œ
            from Search_KSH_Local import KshLocalSearcher
            import pandas as pd

            searcher = KshLocalSearcher(self.db_manager)
            df_all = searcher.search_biblio_by_multiple_subjects(keyword_list)

            if df_all is None or getattr(df_all, "empty", True):
                self._log(f"   âš ï¸ ëª¨ë“  í‚¤ì›Œë“œì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ", "WARNING")
            else:
                self._log(f"   ğŸ” í†µí•© ê²€ìƒ‰ìœ¼ë¡œ {len(df_all)}ê°œ ê²°ê³¼ ë°œê²¬", "INFO")
                # âœ… [ìˆ˜ì •] ì—„ê²©í•œ ë‹¨ì¼ KSH í•„í„°ë§ ë¡œì§ì„ ì œê±°í•˜ì—¬ ëª¨ë“  ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½
                df = df_all.copy()

                if getattr(df, "empty", True):
                    self._log("   âš ï¸ ë‹¨ì¼ KSH ì„œì§€ ì—†ìŒ", "WARNING")
                else:
                    self._log(
                        f"   ğŸ¯ í•„í„°ë§ í›„ {len(df)}ê°œ", "INFO"
                    )  # ë¡œê·¸ ë©”ì‹œì§€ ê°„ì†Œí™”
                    for _, row in df.iterrows():
                        ddc = row.get("ddc") or ""
                        keyword = row.get("matched_keyword")
                        if not ddc or not keyword:
                            continue

                        if ddc not in ddc_aggregation:
                            ddc_aggregation[ddc] = {
                                "count": 0,
                                "titles": [],
                                "ksh_list": [],
                                "keywords": [],
                            }

                        ddc_aggregation[ddc]["count"] += 1
                        if len(ddc_aggregation[ddc]["titles"]) < 1:
                            ddc_aggregation[ddc]["titles"].append(
                                row.get("title") or ""
                            )
                        if len(ddc_aggregation[ddc]["ksh_list"]) < 1:
                            ddc_aggregation[ddc]["ksh_list"].append(
                                row.get("ksh_labeled") or ""
                            )
                        if keyword not in ddc_aggregation[ddc]["keywords"]:
                            ddc_aggregation[ddc]["keywords"].append(keyword)

            if not ddc_aggregation:
                # ê²°ê³¼ ì—†ìŒ ë¡œê·¸ëŠ” í•¨ìˆ˜ ë§ˆì§€ë§‰ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” returnë§Œ
                return results

            # âœ… [ì‹ ê·œ] 1. ìµœì¢… ê²°ê³¼ì— í¬í•¨ë  ëª¨ë“  DDC ë²ˆí˜¸ë¥¼ ë¯¸ë¦¬ ìˆ˜ì§‘
            all_ddc_numbers = list(ddc_aggregation.keys())

            # âœ… [ì‹ ê·œ] 2. ìƒˆë¡œìš´ ëŒ€ëŸ‰ ì¡°íšŒ í•¨ìˆ˜ë¥¼ ë‹¨ í•œ ë²ˆë§Œ í˜¸ì¶œ
            all_labels_map = self.query_manager.get_all_ddc_labels_bulk(all_ddc_numbers)

            keyword_results = {kw: [] for kw in keyword_list}
            for ddc, info in ddc_aggregation.items():
                for keyword in info["keywords"]:
                    if keyword in keyword_results:
                        keyword_results[keyword].append((ddc, info))

            for keyword in keyword_list:
                kw_ddcs = keyword_results.get(keyword, [])
                if not kw_ddcs:
                    continue

                kw_ddcs_sorted = sorted(kw_ddcs, key=lambda x: (-x[1]["count"], x[0]))[
                    :max_results
                ]

                for rank, (ddc, info) in enumerate(kw_ddcs_sorted, 1):
                    ddc_clean = re.sub(r"[^\d.]", "", ddc)
                    ddc_label = all_labels_map.get(ddc_clean, "")
                    item = {
                        "level": level_name,
                        "keyword": keyword,
                        "language": "í•œêµ­ì–´",
                        "rank": rank,
                        "ddc": ddc,
                        "ddc_count": info["count"],
                        "ddc_label": ddc_label,
                        "title": info["titles"][0][:120] if info["titles"] else "",
                        "ksh": info["ksh_list"][0] if info["ksh_list"] else "",
                    }
                    results.append(item)

        # -------------------
        # âœ… [í•µì‹¬ ìˆ˜ì •] ëˆ„ë½ë˜ì—ˆë˜ except ë¸”ë¡ ì¶”ê°€
        except Exception as e:
            error_occurred = True
            self._log(f"âŒ [í•œêµ­ì–´ ê²€ìƒ‰] '{keywords}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}", "ERROR")
        # -------------------

        # ìµœì¢… ê²°ê³¼ì— ë”°ë¼ ë¡œê·¸ ê¸°ë¡
        if results:
            self._log(
                f"âœ… [í•œêµ­ì–´ ê²€ìƒ‰] {level_name}: í‚¤ì›Œë“œ {len(keyword_list)}ê°œ Ã— ìµœëŒ€ {max_results}ê°œ = ì´ {len(results)}ê°œ ê²°ê³¼",
                "INFO",
            )
        elif not error_occurred:
            self._log(
                f"âš ï¸ [í•œêµ­ì–´ ê²€ìƒ‰] {level_name}: ëª¨ë“  í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ", "WARNING"
            )

        return results

    def _search_english_keyword(
        self, keywords: str, level_name: str, max_results: int = 3
    ) -> list:
        results = []
        conn = None  # 1. DB ì—°ê²° ë³€ìˆ˜ë¥¼ Noneìœ¼ë¡œ ì´ˆê¸°í™”
        error_occurred = False  # ì˜¤ë¥˜ ë°œìƒ ì—¬ë¶€ í”Œë˜ê·¸

        try:
            keyword_list = [kw.strip() for kw in keywords.split(",") if kw.strip()]
            if not keyword_list:
                self._log(f"âš ï¸ [ì˜ì–´ ê²€ìƒ‰] '{keywords}': ìœ íš¨í•œ í‚¤ì›Œë“œ ì—†ìŒ", "WARNING")
                return results

            self._log(
                f"ğŸ” [ì˜ì–´ ê²€ìƒ‰] {level_name}: {len(keyword_list)}ê°œ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œì‘",
                "INFO",
            )
            self._log(f"   ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keyword_list)}", "INFO")

            conn = self.db_manager._get_dewey_connection()  # 2. try ë¸”ë¡ ì•ˆì—ì„œ ì—°ê²°
            cursor = conn.cursor()

            # ê° í‚¤ì›Œë“œë³„ë¡œ ê°œë³„ ê²€ìƒ‰ í›„ ìµœëŒ€ 3ê°œì”© ì„ íƒ (ì´í•˜ ë¡œì§ì€ ë³€ê²½ ì—†ìŒ)
            for keyword in keyword_list:
                fts_query = (
                    f'"{keyword}"' if "-" in keyword or " " in keyword else keyword
                )

                cursor.execute(
                    """
                    SELECT ddc, keyword, term_type
                    FROM ddc_keyword_fts
                    WHERE keyword MATCH ?
                    ORDER BY rank
                    LIMIT 100
                    """,
                    (fts_query,),
                )
                rows = cursor.fetchall()

                if not rows:
                    self._log(f"   âš ï¸ '{keyword}': ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ", "WARNING")
                    continue

                self._log(f"   ğŸ” '{keyword}': {len(rows)}ê°œ ê²°ê³¼ ë°œê²¬", "INFO")

                ddc_aggregation = {}
                for ddc, matched_kw, term_type in rows:
                    if ddc not in ddc_aggregation:
                        ddc_aggregation[ddc] = {
                            "count": 0,
                            "matched_keywords": [],
                            "term_types": set(),
                        }
                    ddc_aggregation[ddc]["count"] += 1
                    if matched_kw not in ddc_aggregation[ddc]["matched_keywords"]:
                        ddc_aggregation[ddc]["matched_keywords"].append(matched_kw)
                    ddc_aggregation[ddc]["term_types"].add(term_type)

                sorted_ddcs = sorted(
                    ddc_aggregation.items(), key=lambda x: (-x[1]["count"], x[0])
                )[:max_results]

                # ê²°ê³¼ í¬ë§·íŒ… (ì´í•˜ ë¡œì§ì€ ë³€ê²½ ì—†ìŒ)
                for rank, (ddc, info) in enumerate(sorted_ddcs, 1):
                    ddc_label = self.query_manager.get_ddc_description_cached(ddc) or ""
                    results.append(
                        {
                            "level": level_name,
                            "search_keyword": keyword,
                            "language": "ì˜ì–´",
                            "rank": rank,
                            "ddc": ddc,
                            "ddc_count": info["count"],
                            "ddc_label": ddc_label,
                            "keyword": ", ".join(info["matched_keywords"][:3]),
                            "term_type": ", ".join(sorted(info["term_types"])),
                        }
                    )
        except Exception as e:
            error_occurred = True
            self._log(f"âŒ [ì˜ì–´ ê²€ìƒ‰] '{keywords}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}", "ERROR")
        finally:
            # 3. try ë¸”ë¡ì˜ ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ í•­ìƒ DB ì—°ê²°ì„ ë‹«ìŒ
            if conn:
                conn.close()

        # 4. ëª¨ë“  ë¡œì§ì´ ëë‚œ í›„, ìµœì¢… ê²°ê³¼ì— ë”°ë¼ ë¡œê·¸ë¥¼ í•œ ë²ˆë§Œ ê¸°ë¡
        if results:
            self._log(
                f"âœ… [ì˜ì–´ ê²€ìƒ‰] {level_name}: í‚¤ì›Œë“œ {len(keyword_list)}ê°œ Ã— ìµœëŒ€ {max_results}ê°œ = ì´ {len(results)}ê°œ ê²°ê³¼",
                "INFO",
            )
        elif not error_occurred:  # ì˜¤ë¥˜ ì—†ì´ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ "ê²°ê³¼ ì—†ìŒ" ë¡œê·¸ ê¸°ë¡
            self._log(
                f"âš ï¸ [ì˜ì–´ ê²€ìƒ‰] {level_name}: ëª¨ë“  í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ", "WARNING"
            )

        return results

    def _perform_hierarchical_search(self, hierarchy: dict) -> list:
        """
        2ë‹¨ê³„: ê³„ì¸µë³„ í•œêµ­ì–´/ì˜ì–´ í‚¤ì›Œë“œ DB ë³‘ë ¬ ê²€ìƒ‰ (ë³µí•© í‚¤ì›Œë“œ ê²€ìƒ‰)
        - ê° ë¶„ë¥˜ ë ˆë²¨(ëŒ€/ì¤‘/ì†Œ)ë‹¹ í•œêµ­ì–´/ì˜ì–´ ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰ (ìµœëŒ€ 6ê°œ ë™ì‹œ ì‘ì—…)
        - í•œêµ­ì–´: 5ê°œ í‚¤ì›Œë“œë¥¼ ì½¤ë§ˆë¡œ ì—°ê²°í•˜ì—¬ 1íšŒ ê²€ìƒ‰ â†’ KSH DB
        - ì˜ì–´: 5ê°œ í‚¤ì›Œë“œë¥¼ ì½¤ë§ˆë¡œ ì—°ê²°í•˜ì—¬ 1íšŒ ê²€ìƒ‰ â†’ DDC cache DB (FTS5)
        """
        all_search_results: list[dict] = []

        with ThreadPoolExecutor(
            max_workers=6, thread_name_prefix="gemini_search"
        ) as executor:
            futures = []
            for level, keywords_dict in hierarchy.items():
                level_name = {
                    "broad": "ëŒ€ë¶„ë¥˜",
                    "specific": "ì¤‘ë¶„ë¥˜",
                    "technical": "ì†Œë¶„ë¥˜",
                }.get(level, level)

                # í•œêµ­ì–´ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‘ì—… ì œì¶œ
                korean_keywords = keywords_dict.get("korean", [])
                korean_keywords_filtered = [
                    kw.strip() for kw in korean_keywords if kw and str(kw).strip()
                ]
                if korean_keywords_filtered:
                    korean_keywords_str = ", ".join(korean_keywords_filtered)
                    self._log(f"ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë³µí•© ê²€ìƒ‰ ì œì¶œ: {level_name}", "INFO")
                    futures.append(
                        executor.submit(
                            self._search_korean_keyword,
                            korean_keywords_str,
                            level_name,
                            max_results=3,
                        )
                    )

                # ì˜ì–´ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‘ì—… ì œì¶œ
                english_keywords = keywords_dict.get("english", [])
                english_keywords_filtered = [
                    kw.strip() for kw in english_keywords if kw and str(kw).strip()
                ]
                if english_keywords_filtered:
                    english_keywords_str = ", ".join(english_keywords_filtered)
                    self._log(f"ğŸ‡ºğŸ‡¸ ì˜ì–´ ë³µí•© ê²€ìƒ‰ ì œì¶œ: {level_name}", "INFO")
                    futures.append(
                        executor.submit(
                            self._search_english_keyword,
                            english_keywords_str,
                            level_name,
                            max_results=3,
                        )
                    )

            # ì™„ë£Œë˜ëŠ” ì‘ì—… ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result:
                        all_search_results.extend(result)
                except Exception as e:
                    self._log(f"âŒ ë³‘ë ¬ ê²€ìƒ‰ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", "ERROR")

        self._log(
            f"ğŸ“Š ê³„ì¸µë³„ ë³‘ë ¬ ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(all_search_results)}ê°œ ê²°ê³¼", "INFO"
        )
        return all_search_results

    def _build_gemini_search_summary(
        self, hierarchy: dict, search_results: list
    ) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ëŒ€/ì¤‘/ì†Œ ë¶„ë¥˜ë¡œ êµ¬ë¶„í•˜ì—¬ Gemini APIì— ì „ë‹¬í•  í…ìŠ¤íŠ¸ ìƒì„±
        """
        summary_lines = []

        for level in ["broad", "specific", "technical"]:
            level_name = {
                "broad": "ëŒ€ë¶„ë¥˜",
                "specific": "ì¤‘ë¶„ë¥˜",
                "technical": "ì†Œë¶„ë¥˜",
            }.get(level, level)

            level_data = hierarchy.get(level, {})
            korean_keywords = level_data.get("korean", [])
            english_keywords = level_data.get("english", [])

            summary_lines.append(f"\n{'='*60}")
            summary_lines.append(f"ã€{level_name}ã€‘")
            summary_lines.append(f"{'='*60}")

            # ì¶”ì¶œëœ í‚¤ì›Œë“œ í‘œì‹œ
            summary_lines.append(f"\nì¶”ì¶œ í‚¤ì›Œë“œ:")
            summary_lines.append(
                f"  - í•œêµ­ì–´: {', '.join(korean_keywords) if korean_keywords else 'ì—†ìŒ'}"
            )
            summary_lines.append(
                f"  - ì˜ì–´: {', '.join(english_keywords) if english_keywords else 'ì—†ìŒ'}"
            )

            # í•œêµ­ì–´ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼
            if korean_keywords:
                summary_lines.append(f"\ní•œêµ­ì–´ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼:")
                korean_results = [
                    r
                    for r in search_results
                    if r.get("level") == level_name and r.get("language") == "í•œêµ­ì–´"
                ]
                if korean_results:
                    for result in korean_results:
                        summary_lines.append(f"  í‚¤ì›Œë“œ: {result.get('keyword', '')}")
                        summary_lines.append(f"    - ìˆœìœ„: {result.get('rank', '')}ìœ„")
                        summary_lines.append(f"    - DDC: {result.get('ddc', '')}")
                        summary_lines.append(
                            f"    - DDC ë“±ì¥ íšŸìˆ˜: {result.get('ddc_count', '')}íšŒ"
                        )
                        summary_lines.append(
                            f"    - DDC ì˜ë¯¸: {result.get('ddc_label', '')}"
                        )
                        summary_lines.append(f"    - ì œëª©: {result.get('title', '')}")
                        summary_lines.append(f"    - KSH: {result.get('ksh', '')}")
                        summary_lines.append("")
                else:
                    summary_lines.append("  (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)")

            # ì˜ì–´ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼
            if english_keywords:
                summary_lines.append(f"\nì˜ì–´ í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼:")
                english_results = [
                    r
                    for r in search_results
                    if r.get("level") == level_name and r.get("language") == "ì˜ì–´"
                ]
                if english_results:
                    for result in english_results:
                        summary_lines.append(
                            f"  ê²€ìƒ‰ì–´: {result.get('search_keyword', '')}"
                        )
                        summary_lines.append(f"    - ìˆœìœ„: {result.get('rank', '')}ìœ„")
                        summary_lines.append(f"    - DDC: {result.get('ddc', '')}")
                        summary_lines.append(
                            f"    - DDC ë“±ì¥ íšŸìˆ˜: {result.get('ddc_count', '')}íšŒ"
                        )
                        summary_lines.append(
                            f"    - DDC ë ˆì´ë¸”: {result.get('ddc_label', '')}"
                        )
                        summary_lines.append(
                            f"    - ë§¤ì¹­ í‚¤ì›Œë“œ: {result.get('keyword', '')}"
                        )
                        summary_lines.append(
                            f"    - ìš©ì–´ ìœ í˜•: {result.get('term_type', '')}"
                        )
                        summary_lines.append("")
                else:
                    summary_lines.append("  (ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ)")

        return "\n".join(summary_lines)

    def _final_analysis_with_search_results(
        self, bundle_text: str, hierarchy: dict, search_results: list
    ) -> dict:
        """5ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ Geminiì—ê²Œ ìµœì¢… DDC ì¶”ì²œ ìš”ì²­"""

        # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ìƒì„±
        search_summary = self._build_gemini_search_summary(hierarchy, search_results)

        final_prompt = f"""ì…ë ¥(ì €ìì •ë³´/ëª©ì°¨/ì„œí‰ í˜¼í•© í…ìŠ¤íŠ¸):

{bundle_text}

{search_summary}

ğŸ¯ ì¤‘ìš” ì§€ì‹œì‚¬í•­:
1. ìœ„ì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬, ì…ë ¥ í…ìŠ¤íŠ¸ì— ê°€ì¥ ì í•©í•œ DDC ë²ˆí˜¸ë¥¼ ì¶”ì²œí•˜ì„¸ìš”
2. í•œêµ­ì–´ ê²€ìƒ‰ ê²°ê³¼ì˜ DDCì™€ DDC ì˜ë¯¸, ì˜ì–´ ê²€ìƒ‰ ê²°ê³¼ì˜ DDCë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ì„¸ìš”
3. ê° DDC ë²ˆí˜¸ê°€ ì…ë ¥ í…ìŠ¤íŠ¸(ì €ìì •ë³´/ëª©ì°¨/ì„œí‰)ì™€ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”
4. ì˜ë¯¸ê°€ ì¼ì¹˜í•˜ëŠ” DDCë§Œ ì í•©ë„ ìˆœìœ¼ë¡œ ì¶”ì²œí•˜ì„¸ìš”
5. ê° ì¶”ì²œì— ëŒ€í•´ "ì™œ ê·¸ DDCê°€ ì…ë ¥ í…ìŠ¤íŠ¸ì™€ ì¼ì¹˜í•˜ëŠ”ì§€" ê·¼ê±°ë¥¼ ëª…ì‹œí•˜ì„¸ìš”

ì‘ë‹µì€ ë‹¤ìŒ JSON í˜•ì‹ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì„¸ìš”:

{{
  "overallDescription": "ì¶”ì²œëœ ëª¨ë“  DDC ë¶„ë¥˜ì— ëŒ€í•œ ìµœì¢… ê²°ë¡ ì„ í¬í•¨í•˜ëŠ” ì „ì²´ì ì¸ ì„¤ëª…",
  "classifications": [
    {{
      "ddcNumber": "DDC ë¶„ë¥˜ ë²ˆí˜¸",
      "reason": "ì´ ë¶„ë¥˜ì— ëŒ€í•œ ìƒì„¸í•œ ì¶”ì²œ ì´ìœ  (ì‹¤ì œ ì˜ë¯¸ì™€ì˜ ì¼ì¹˜ì„± í¬í•¨, ì €ìÂ·ëª©ì°¨Â·ì„œí‰ ë‹¨ì„œ ê·¼ê±° ëª…ì‹œ)"
    }}
  ]
}}"""

        try:
            response = self._call_gemini_api(final_prompt)
            if isinstance(response, dict) and "error" in response:
                return response

            # JSON ì‘ë‹µ íŒŒì‹± (ê²¬ê³ í•œ íŒŒì‹±)
            final_result = _safe_json_loads(str(response))
            if not final_result:
                logging.error(f"ìµœì¢… ë¶„ì„ JSON íŒŒì‹± ì‹¤íŒ¨: {str(response)[:200]}")
                return {"error": "ìµœì¢… ë¶„ì„ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜"}

            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if (
                "overallDescription" not in final_result
                or "classifications" not in final_result
            ):
                return {"error": "ìµœì¢… ë¶„ì„ ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜"}

            if not isinstance(final_result["classifications"], list):
                final_result["classifications"] = []

            self._log(
                f"ğŸ‰ ìµœì¢… ë¶„ì„ ì™„ë£Œ: {len(final_result['classifications'])}ê°œ DDC ì¶”ì²œ",
                "INFO",
            )
            return final_result

        except Exception as e:
            self._log(f"âŒ ìµœì¢… ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}", "ERROR")
            return {"error": f"ìµœì¢… ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}"}

    def _call_gemini_api(self, prompt: str) -> str or dict:
        """Gemini API í˜¸ì¶œ ê³µí†µ í•¨ìˆ˜"""
        try:
            payload = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {
                    "temperature": 0.3,
                    "maxOutputTokens": 2048,
                    "responseMimeType": (
                        "application/json" if "JSON" in prompt else "text/plain"
                    ),
                },
            }

            headers = {
                "Content-Type": "application/json",
                "x-goog-api-key": self.api_key,
            }

            response = requests.post(
                self.api_url, headers=headers, json=payload, timeout=30
            )
            response.raise_for_status()

            response_data = response.json()

            if (
                "candidates" in response_data
                and len(response_data["candidates"]) > 0
                and "content" in response_data["candidates"][0]
                and "parts" in response_data["candidates"][0]["content"]
                and len(response_data["candidates"][0]["content"]["parts"]) > 0
            ):
                return response_data["candidates"][0]["content"]["parts"][0].get(
                    "text", ""
                )
            else:
                return {
                    "error": "Gemini API ì‘ë‹µì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }

        except requests.exceptions.HTTPError as e:
            logging.error(
                f"API HTTP ì˜¤ë¥˜: {e.response.status_code} - {e.response.text}"
            )
            return {"error": f"API ì˜¤ë¥˜ ({e.response.status_code}): {e.response.text}"}
        except requests.exceptions.Timeout:
            logging.error("API ìš”ì²­ ì‹œê°„ ì´ˆê³¼")
            return {"error": "API ìš”ì²­ ì‹œê°„ ì´ˆê³¼"}
        except Exception as e:
            logging.error(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {"error": f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}"}

    # ê¸°ì¡´ classify_ddc ë©”ì„œë“œ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
    def classify_ddc(self, bundle_text: str) -> dict:
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ - ìƒˆë¡œìš´ ê³„ì¸µì  ì‹œìŠ¤í…œì„ í˜¸ì¶œ (ë‹¨ì¼ ì…ë ¥)"""
        return self.classify_ddc_with_hierarchical_validation(bundle_text)

    def classify_ddc_with_hierarchical_validation(
        self, bundle_text: str, intermediate_callback=None
    ) -> dict:
        """
        ğŸš€ í•œ ì…ë ¥ì°½ì— ë¶™ì—¬ë„£ì€ ì €ìì •ë³´/ëª©ì°¨/ì„œí‰(í˜¼í•© í…ìŠ¤íŠ¸)ì„ ë°˜ì˜í•œ
        ê³„ì¸µì  ê²€ìƒ‰ + ìµœì¢… DDC ì¶”ì²œ ì‹œìŠ¤í…œ

        1ë‹¨ê³„: Geminiê°€ í•œêµ­ì–´/ì˜ì–´ í‚¤ì›Œë“œë¥¼ ëŒ€/ì¤‘/ì†Œ ë¶„ë¥˜ë³„ë¡œ ì¶”ì¶œ
        2ë‹¨ê³„: í•œêµ­ì–´ í‚¤ì›Œë“œë¡œ KSH DB ê²€ìƒ‰, ì˜ì–´ í‚¤ì›Œë“œë¡œ DDC cache DB ê²€ìƒ‰
        3ë‹¨ê³„: ì¤‘ê°„ ê²°ê³¼ì°½ ì—…ë°ì´íŠ¸
        4ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ë¥¼ Geminiì—ê²Œ ì „ë‹¬í•˜ì—¬ ìµœì¢… DDC ì¶”ì²œ ë°›ê¸°
        """
        try:
            self._log("=" * 80, "INFO")
            self._log("ğŸ¯ 1ë‹¨ê³„: ê³„ì¸µì  í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘ (í•œêµ­ì–´/ì˜ì–´ ë¶„ë¦¬)", "INFO")
            self._log("=" * 80, "INFO")
            hierarchy = self._extract_hierarchical_keywords(bundle_text)

            self._log(f"ğŸ“ ì¶”ì¶œëœ í‚¤ì›Œë“œ:\n{hierarchy}", "INFO")

            self._log("=" * 80, "INFO")
            self._log("ğŸ” 2ë‹¨ê³„: ê³„ì¸µë³„ í•œêµ­ì–´/ì˜ì–´ í‚¤ì›Œë“œ DB ê²€ìƒ‰", "INFO")
            self._log("=" * 80, "INFO")
            search_results = self._perform_hierarchical_search(hierarchy)

            self._log("=" * 80, "INFO")
            self._log("ğŸ“Š 3ë‹¨ê³„: ì¤‘ê°„ ê²°ê³¼ì°½ ì—…ë°ì´íŠ¸", "INFO")
            self._log("=" * 80, "INFO")
            if intermediate_callback:
                intermediate_callback(search_results)

            self._log("=" * 80, "INFO")
            self._log("ğŸ¯ 4ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ìµœì¢… DDC ì¶”ì²œ ìš”ì²­", "INFO")
            self._log("=" * 80, "INFO")
            return self._final_analysis_with_search_results(
                bundle_text, hierarchy, search_results
            )

        except Exception as e:
            self._log(f"âŒ [Gemini] ê³„ì¸µì  ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", "ERROR")
            import traceback

            self._log(traceback.format_exc(), "ERROR")
            return {"error": f"ê³„ì¸µì  ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
