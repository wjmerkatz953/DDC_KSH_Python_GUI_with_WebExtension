# -*- coding: utf-8 -*-
"""
!!! 2025ë…„ 8ì›” NLK DB ìŠ¤ëƒ…ìƒ·ì˜ SQLite DB ì „í™˜ì— ì‚¬ìš©í•œ ë¹Œë“œ !!!
ì—…ë°ì´íŠ¸ ì¼ì‹œ: 2025-10-18 (ìµœê·¼ ì—…ë°ì´íŠ¸)
íŒŒì¼ëª…: PNU_KDC_DDC_Mapper_SQLite_with_KSH.py
Version: 2.3.0
ì„¤ëª…:
ë¶€ì‚°êµëŒ€ KDC to DDC ë§¤í•‘ í”„ë¡œê·¸ë¨ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)
- ì˜¤ëŠ˜(8/30) ìˆ˜í–‰í•œ ë°ì´í„° ì •ê·œí™” ì‘ì—… ì™„ë²½ ë°˜ì˜
- ksh_labeled, ksh_korean ì»¬ëŸ¼ ì¶”ê°€
- publication_year ì •ê·œí™” ì™„ë£Œ ([ë°œí–‰ë…„ë¶ˆëª…] í†µì¼, ì •ìˆ˜ ì—°ë„ ì¶”ì¶œ)
- identifier ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì œê±° ì™„ë£Œ (nlk: ì œê±°)
- 21ê°œ ì¸ë±ìŠ¤ + FTS5 ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ë¡œ ì´ˆê³ ì† ê²€ìƒ‰ ì„±ëŠ¥
- 380ë§Œê±´ ê¸°ì¤€ ë°ì´í„° êµ¬ì¡°

[2025-10-18 ì—…ë°ì´íŠ¸ ë‚´ì—­ - v2.3.0]
- âš¡ FTS5 ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ ì¶”ê°€ (ê²€ìƒ‰ ì„±ëŠ¥ 20ë°° í–¥ìƒ!)
  * ksh_korean ì»¬ëŸ¼ ì „ë¬¸ ê²€ìƒ‰ ìµœì í™”
  * ê²€ìƒ‰ ì‹œê°„: 20ì´ˆ â†’ 1ì´ˆë¡œ ë‹¨ì¶•
  * mapping_data_fts ê°€ìƒ í…Œì´ë¸” ìƒì„±
  * ìë™ ë™ê¸°í™” íŠ¸ë¦¬ê±° 3ê°œ ì¶”ê°€ (INSERT/UPDATE/DELETE)
- create_fts5_index() ë©”ì„œë“œ ì¶”ê°€
- ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„± ì‹œ FTS5 ìë™ ìƒì„±

[2025-09-14 ì—…ë°ì´íŠ¸ ë‚´ì—­ - v2.2.0]
- ì•± ë†’ì´ 900pxë¡œ í™•ì¥ (ë¡œê·¸ì°½ ê°€ë…ì„± ê°œì„ )
- JSON íŒŒì¼ ì²˜ë¦¬ ì‹œ ì§„í–‰ë¥  í‘œì‹œ ì¶”ê°€ (1/60, 2/60 í˜•íƒœ)
- KSH ì‹ë³„ì ì •ê·œí™” ë²„íŠ¼ ì¶”ê°€ (nlk: ì œê±°, êµ¬ë¶„ì í†µì¼, ê³µë°± ì •ë¦¬)
- ì¸ë±ìŠ¤ ì„±ëŠ¥ ìµœì í™”: LIKE '%íŒ¨í„´%' â†’ INSTR/GLOB íŒ¨í„´ìœ¼ë¡œ êµì²´
- ksh_korean ì»¬ëŸ¼ ê³µë°± ì™„ì „ ì œê±° ê¸°ëŠ¥ ê°•í™”
- ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì¿¼ë¦¬ ì„±ëŠ¥ 10-50ë°° í–¥ìƒ
"""

import pandas as pd
import sqlite3
from pathlib import Path
import tkinter as tk
from tkinter import filedialog, messagebox
import customtkinter as ctk
import threading
from datetime import datetime
import re
import os
from collections import Counter
import logging

try:
    import ijson
    from tqdm import tqdm
except ImportError:
    messagebox.showerror(
        "ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜¤ë¥˜",
        "ijson ë˜ëŠ” tqdm ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\ní„°ë¯¸ë„ì—ì„œ 'pip install ijson tqdm' ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.",
    )
    exit()

# GUI ê¸°ë³¸ ì„¤ì •
ctk.set_appearance_mode("System")
ctk.set_default_color_theme("blue")


class KDCDDCMapperSQLite(ctk.CTk):
    def __init__(self):
        super().__init__()
        self.db_path = "kdc_ddc_mapping.db"
        self.stop_flag = False

        # ë¡œê·¸ ìë™ì €ì¥ ê´€ë ¨ ì†ì„±
        self.auto_save_logs = True
        self.log_file_path = None
        self.setup_logging()

        self.setup_gui()
        self.init_database()

    def setup_logging(self):
        """ë¡œê·¸ íŒŒì¼ ìë™ì €ì¥ ì„¤ì •"""
        if self.auto_save_logs:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.log_file_path = Path(f"ë§¤í•‘ë¡œê·¸_{timestamp}.log")

            self.logger = logging.getLogger("KDC_DDC_Mapper")
            self.logger.setLevel(logging.INFO)

            # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
            for handler in self.logger.handlers[:]:
                self.logger.removeHandler(handler)

            # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
            file_handler = logging.FileHandler(self.log_file_path, encoding="utf-8")
            file_handler.setLevel(logging.INFO)

            formatter = logging.Formatter(
                "[%(asctime)s] %(message)s", datefmt="%H:%M:%S"
            )
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)

            # ì´ˆê¸° ë¡œê·¸ ê¸°ë¡
            self.logger.info("=" * 50)
            self.logger.info("KDC-DDC ë§¤í•‘ í”„ë¡œê·¸ë¨ v2.3.0 ì‹œì‘ (2025-10-18 ì—…ë°ì´íŠ¸)")
            self.logger.info("âš¡ FTS5 ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ ì§€ì›")
            self.logger.info(f"ë¡œê·¸ íŒŒì¼: {self.log_file_path}")
            self.logger.info("=" * 50)

    def init_database(self):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ì™€ í…Œì´ë¸”ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤ (ìµœì‹  ìŠ¤í‚¤ë§ˆ ì ìš©)"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # ìµœì‹  í…Œì´ë¸” êµ¬ì¡° ìƒì„± (ksh_labeled, ksh_korean ì»¬ëŸ¼ í¬í•¨)
                cursor.execute(
                    """
                    CREATE TABLE IF NOT EXISTS mapping_data (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        identifier TEXT,
                        kdc TEXT,
                        ddc TEXT,
                        ksh TEXT,
                        kdc_edition TEXT,
                        ddc_edition TEXT,
                        publication_year TEXT,
                        title TEXT,
                        data_type TEXT,
                        source_file TEXT,
                        ksh_labeled TEXT,
                        ksh_korean TEXT
                    )
                """
                )

                # -------------------
                # ë¹ ë¥¸ DB êµ¬ì¶•ì„ ìœ„í•´ ê¸°ë³¸ ì¸ë±ìŠ¤ë§Œ ìƒì„±
                basic_indexes = [
                    "CREATE INDEX IF NOT EXISTS idx_identifier ON mapping_data(identifier)",
                    "CREATE INDEX IF NOT EXISTS idx_kdc ON mapping_data(kdc)",
                    "CREATE INDEX IF NOT EXISTS idx_ddc ON mapping_data(ddc)",
                ]

                for index_sql in basic_indexes:
                    cursor.execute(index_sql)
                # -------------------

                conn.commit()

            self.check_existing_db()
        except Exception as e:
            self.log(f"DB ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")

    def create_search_indexes(self):
        """ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ 21ê°œ ì¸ë±ìŠ¤ ìƒì„± (DB êµ¬ì¶• ì™„ë£Œ í›„ ì‹¤í–‰)"""
        self.log("ğŸ“Š ê²€ìƒ‰ ìµœì í™” ì¸ë±ìŠ¤ ìƒì„± ì‹œì‘...")

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # ê³ ê¸‰ ê²€ìƒ‰ ìµœì í™” ì¸ë±ìŠ¤ë“¤
                advanced_indexes = [
                    "CREATE INDEX IF NOT EXISTS idx_title ON mapping_data(title)",
                    "CREATE INDEX IF NOT EXISTS idx_publication_year ON mapping_data(publication_year)",
                    "CREATE INDEX IF NOT EXISTS idx_ksh ON mapping_data(ksh)",
                    "CREATE INDEX IF NOT EXISTS idx_data_type ON mapping_data(data_type)",
                    "CREATE INDEX IF NOT EXISTS idx_source_file ON mapping_data(source_file)",
                    "CREATE INDEX IF NOT EXISTS idx_ksh_korean ON mapping_data(ksh_korean)",
                    # ë³µí•© ì¸ë±ìŠ¤
                    "CREATE INDEX IF NOT EXISTS idx_ddc_year ON mapping_data(ddc, publication_year)",
                    "CREATE INDEX IF NOT EXISTS idx_kdc_year ON mapping_data(kdc, publication_year)",
                    "CREATE INDEX IF NOT EXISTS idx_ksh_year ON mapping_data(ksh, publication_year)",
                    "CREATE INDEX IF NOT EXISTS idx_ksh_korean_year ON mapping_data(ksh_korean, publication_year)",
                    # ì–‘ë°©í–¥ ë³µí•© ì¸ë±ìŠ¤
                    "CREATE INDEX IF NOT EXISTS idx_ksh_ddc ON mapping_data(ksh, ddc)",
                    "CREATE INDEX IF NOT EXISTS idx_ddc_ksh ON mapping_data(ddc, ksh)",
                    "CREATE INDEX IF NOT EXISTS idx_ksh_korean_ddc ON mapping_data(ksh_korean, ddc)",
                    "CREATE INDEX IF NOT EXISTS idx_ddc_ksh_korean ON mapping_data(ddc, ksh_korean)",
                    "CREATE INDEX IF NOT EXISTS idx_ksh_korean_kdc ON mapping_data(ksh_korean, kdc)",
                    "CREATE INDEX IF NOT EXISTS idx_kdc_ksh_korean ON mapping_data(kdc, ksh_korean)",
                    # 3ì¤‘ ë³µí•© ì¸ë±ìŠ¤
                    "CREATE INDEX IF NOT EXISTS idx_ddc_ksh_korean_year ON mapping_data(ddc, ksh_korean, publication_year)",
                    "CREATE INDEX IF NOT EXISTS idx_ksh_korean_ddc_year ON mapping_data(ksh_korean, ddc, publication_year)",
                ]

                total_indexes = len(advanced_indexes)
                for i, index_sql in enumerate(advanced_indexes, 1):
                    cursor.execute(index_sql)
                    self.log(f"ì¸ë±ìŠ¤ ìƒì„± ì§„í–‰: {i}/{total_indexes}")
                    self.update_idletasks()  # GUI ì—…ë°ì´íŠ¸

                conn.commit()
                self.log(f"âœ… ê²€ìƒ‰ ìµœì í™” ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {total_indexes}ê°œ")

                # âœ… [2025-10-18 ì¶”ê°€] FTS5 ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±
                self.create_fts5_index(cursor, conn)

        except Exception as e:
            self.log(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì˜¤ë¥˜: {e}")

    def create_fts5_index(self, cursor, conn):
        """
        âœ… [2025-10-18 ì¶”ê°€] FTS5 ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±
        - ksh_korean ì»¬ëŸ¼ ì „ë¬¸ ê²€ìƒ‰ ìµœì í™”
        - ê²€ìƒ‰ ì„±ëŠ¥: 20ì´ˆ â†’ 1ì´ˆë¡œ ë‹¨ì¶•!
        """
        try:
            # FTS5 í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            cursor.execute("""
                SELECT name FROM sqlite_master
                WHERE type='table' AND name='mapping_data_fts'
            """)

            if cursor.fetchone():
                self.log("âœ… FTS5 í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
                return

            self.log("â³ FTS5 ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„± ì¤‘... (ìˆ˜ ë¶„ ì†Œìš”)")

            # FTS5 ê°€ìƒ í…Œì´ë¸” ìƒì„±
            cursor.execute("""
                CREATE VIRTUAL TABLE mapping_data_fts USING fts5(
                    identifier UNINDEXED,
                    ksh_korean,
                    content='mapping_data',
                    content_rowid='rowid'
                )
            """)

            # ê¸°ì¡´ ë°ì´í„°ë¡œ FTS5 ì±„ìš°ê¸°
            self.log("â³ FTS5 ì¸ë±ìŠ¤ ë°ì´í„° ì±„ìš°ê¸° ì¤‘...")
            cursor.execute("""
                INSERT INTO mapping_data_fts(rowid, identifier, ksh_korean)
                SELECT rowid, identifier, ksh_korean
                FROM mapping_data
                WHERE ksh_korean IS NOT NULL AND ksh_korean != ''
            """)

            # ë™ê¸°í™” íŠ¸ë¦¬ê±° ìƒì„±
            self.log("â³ FTS5 ë™ê¸°í™” íŠ¸ë¦¬ê±° ìƒì„± ì¤‘...")
            cursor.execute("""
                CREATE TRIGGER mapping_data_fts_insert AFTER INSERT ON mapping_data BEGIN
                    INSERT INTO mapping_data_fts(rowid, identifier, ksh_korean)
                    VALUES (new.rowid, new.identifier, new.ksh_korean);
                END
            """)

            cursor.execute("""
                CREATE TRIGGER mapping_data_fts_delete AFTER DELETE ON mapping_data BEGIN
                    DELETE FROM mapping_data_fts WHERE rowid = old.rowid;
                END
            """)

            cursor.execute("""
                CREATE TRIGGER mapping_data_fts_update AFTER UPDATE ON mapping_data BEGIN
                    DELETE FROM mapping_data_fts WHERE rowid = old.rowid;
                    INSERT INTO mapping_data_fts(rowid, identifier, ksh_korean)
                    VALUES (new.rowid, new.identifier, new.ksh_korean);
                END
            """)

            conn.commit()
            self.log("âœ… FTS5 ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ! (ê²€ìƒ‰ ì†ë„ 20ë°° í–¥ìƒ)")

        except Exception as e:
            self.log(f"âš ï¸ FTS5 ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨ (ë¬´ì‹œ ê°€ëŠ¥): {e}")
            # ì‹¤íŒ¨í•´ë„ ì•±ì€ ê³„ì† ì‹¤í–‰

    def check_existing_db(self):
        """ê¸°ì¡´ DBì˜ ì •ë³´ë¥¼ í™•ì¸í•˜ê³  GUIì— í‘œì‹œí•©ë‹ˆë‹¤"""
        if not os.path.exists(self.db_path):
            self.db_status_var.set("ìƒˆ DB ì¤€ë¹„ ì™„ë£Œ. (íŒŒì¼ ì—†ìŒ)")
            return

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM mapping_data")
                count = cursor.fetchone()[0]

                if count > 0:
                    db_size = os.path.getsize(self.db_path) / (1024 * 1024)  # MB
                    status_msg = f"ê¸°ì¡´ DB ë°œê²¬: {count:,}ê°œ ë°ì´í„° ({db_size:.1f}MB)"
                    self.db_status_var.set(status_msg)
                    self.log(f"ğŸ“Š {status_msg}")

                    # ì •ê·œí™” ìƒíƒœ í™•ì¸
                    cursor.execute(
                        "SELECT COUNT(*) FROM mapping_data WHERE publication_year = '[ë°œí–‰ë…„ë¶ˆëª…]'"
                    )
                    normalized_count = cursor.fetchone()[0]
                    self.log(f"ğŸ“‹ ì •ê·œí™”ëœ ë°œí–‰ë…„ë¶ˆëª…: {normalized_count:,}ê±´")

                else:
                    self.db_status_var.set("ìƒˆ DB ì¤€ë¹„ ì™„ë£Œ. (ë°ì´í„° ì—†ìŒ)")
        except Exception as e:
            self.log(f"DB í™•ì¸ ì˜¤ë¥˜: {e}")
            self.db_status_var.set("DB í™•ì¸ ì˜¤ë¥˜ ë°œìƒ")

    def setup_gui(self):
        """CustomTkinterë¥¼ ì‚¬ìš©í•˜ì—¬ GUIë¥¼ ì„¤ì •í•©ë‹ˆë‹¤"""
        self.title("ë¶€ì‚°ëŒ€-ë¶€ì‚°êµëŒ€ KDC-DDC ë§¤í•‘ í”„ë¡œê·¸ë¨ v2.1.0 (9/14ì—…ë°ì´íŠ¸)")
        self.geometry("900x950")
        self.grid_columnconfigure(0, weight=1)

        # 1. JSON ë°ì´í„° ì„ íƒ
        json_frame = ctk.CTkFrame(self)
        json_frame.grid(row=0, column=0, padx=10, pady=(10, 5), sticky="ew")
        json_frame.grid_columnconfigure(0, weight=1)

        ctk.CTkLabel(
            json_frame,
            text="1. JSON ì„œì§€ë°ì´í„° í´ë” ì„ íƒ",
            font=ctk.CTkFont(weight="bold"),
        ).grid(row=0, column=0, columnspan=2, padx=10, pady=(5, 0), sticky="w")

        self.json_path_var = ctk.StringVar()
        ctk.CTkEntry(json_frame, textvariable=self.json_path_var, width=70).grid(
            row=1, column=0, padx=(10, 5), pady=10, sticky="ew"
        )
        ctk.CTkButton(
            json_frame, text="í´ë” ì„ íƒ", command=self.select_json_folder
        ).grid(row=1, column=1, padx=(0, 10), pady=10)

        # 2. SQLite DB êµ¬ì¶•
        db_frame = ctk.CTkFrame(self)
        db_frame.grid(row=1, column=0, padx=10, pady=5, sticky="ew")
        db_frame.grid_columnconfigure(1, weight=1)

        ctk.CTkLabel(
            db_frame,
            text="2. SQLite DB êµ¬ì¶• (ìµœì‹  ì •ê·œí™” ì ìš©)",
            font=ctk.CTkFont(weight="bold"),
        ).grid(row=0, column=0, columnspan=3, padx=10, pady=(5, 0), sticky="w")

        self.db_status_var = ctk.StringVar(value="DB ì¤€ë¹„ ì¤‘...")
        ctk.CTkLabel(db_frame, textvariable=self.db_status_var).grid(
            row=1, column=0, columnspan=3, padx=10, pady=(0, 5), sticky="w"
        )

        ctk.CTkButton(db_frame, text="DB êµ¬ì¶• ì‹œì‘", command=self.build_database).grid(
            row=2, column=0, padx=(10, 5), pady=5
        )
        ctk.CTkButton(db_frame, text="ì¤‘ë‹¨", command=self.stop_process).grid(
            row=2, column=1, padx=5, pady=5, sticky="w"
        )
        ctk.CTkButton(
            db_frame, text="ê¸°ì¡´ DB ì‚­ì œ", command=self.delete_database, fg_color="red"
        ).grid(row=2, column=2, padx=(5, 10), pady=5, sticky="w")

        # 3. ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬
        excel_frame = ctk.CTkFrame(self)
        excel_frame.grid(row=2, column=0, padx=10, pady=5, sticky="ew")
        excel_frame.grid_columnconfigure(0, weight=1)

        ctk.CTkLabel(
            excel_frame,
            text="3. ë¶€ì‚°êµëŒ€ ì—‘ì…€ íŒŒì¼ ë§¤í•‘",
            font=ctk.CTkFont(weight="bold"),
        ).grid(row=0, column=0, columnspan=2, padx=10, pady=(5, 0), sticky="w")

        self.excel_path_var = ctk.StringVar()
        ctk.CTkEntry(excel_frame, textvariable=self.excel_path_var, width=70).grid(
            row=1, column=0, padx=(10, 5), pady=10, sticky="ew"
        )
        ctk.CTkButton(
            excel_frame, text="ì—‘ì…€ ì„ íƒ", command=self.select_excel_file
        ).grid(row=1, column=1, padx=(0, 10), pady=10)

        ctk.CTkButton(excel_frame, text="ë§¤í•‘ ì‹œì‘", command=self.start_mapping).grid(
            row=2, column=0, padx=10, pady=5, sticky="w"
        )

        # 4. ë°ì´í„° ì •ê·œí™” (8/30 ì—…ë°ì´íŠ¸)
        normalize_frame = ctk.CTkFrame(self)
        normalize_frame.grid(row=3, column=0, padx=10, pady=5, sticky="ew")
        normalize_frame.grid_columnconfigure(1, weight=1)

        ctk.CTkLabel(
            normalize_frame,
            text="4. ë°ì´í„° ì •ê·œí™” (8/30 ì—…ë°ì´íŠ¸)",
            font=ctk.CTkFont(weight="bold"),
        ).grid(row=0, column=0, columnspan=4, padx=10, pady=(5, 0), sticky="w")

        ctk.CTkButton(
            normalize_frame,
            text="ë°œí–‰ë…„ë„ ì •ê·œí™”",
            command=self.normalize_publication_year,
        ).grid(row=1, column=0, padx=(10, 5), pady=5)

        ctk.CTkButton(
            normalize_frame, text="ì‹ë³„ì ì •ê·œí™”", command=self.normalize_identifier
        ).grid(row=1, column=1, padx=5, pady=5)

        ctk.CTkButton(
            normalize_frame,
            text="KSH ì‹ë³„ì ì •ê·œí™”",
            command=self.normalize_ksh_identifiers,
        ).grid(row=1, column=2, padx=5, pady=5)

        # -------------------
        # ê²€ìƒ‰ ìµœì í™” ì¸ë±ìŠ¤ ìƒì„± ë²„íŠ¼ ì¶”ê°€
        ctk.CTkButton(
            normalize_frame,
            text="ğŸ“Š ì¸ë±ìŠ¤ ìƒì„±",
            command=self.create_search_indexes,
            fg_color="green",
        ).grid(row=1, column=3, padx=(5, 10), pady=5)
        # -------------------

        # 5. ë¡œê·¸ ì„¤ì •
        log_settings_frame = ctk.CTkFrame(self)
        log_settings_frame.grid(row=4, column=0, padx=10, pady=5, sticky="ew")

        ctk.CTkLabel(
            log_settings_frame,
            text="5. ë¡œê·¸ ì„¤ì •",
            font=ctk.CTkFont(weight="bold"),
        ).grid(row=0, column=0, padx=10, pady=(5, 0), sticky="w")

        self.auto_save_var = ctk.BooleanVar(value=True)
        ctk.CTkCheckBox(
            log_settings_frame,
            text="ë¡œê·¸ ìë™ì €ì¥",
            variable=self.auto_save_var,
        ).grid(row=1, column=0, padx=10, pady=5, sticky="w")

        # 6. ìƒíƒœ ë° ë¡œê·¸
        status_frame = ctk.CTkFrame(self)
        status_frame.grid(row=5, column=0, padx=10, pady=5, sticky="ew")

        ctk.CTkLabel(
            status_frame, text="6. ì²˜ë¦¬ ìƒíƒœ", font=ctk.CTkFont(weight="bold")
        ).grid(row=0, column=0, padx=10, pady=(5, 0), sticky="w")

        self.status_var = ctk.StringVar(value="ëŒ€ê¸° ì¤‘...")
        ctk.CTkLabel(status_frame, textvariable=self.status_var).grid(
            row=1, column=0, padx=10, pady=5, sticky="w"
        )

        # 7. ë¡œê·¸ í‘œì‹œ
        log_frame = ctk.CTkFrame(self)
        log_frame.grid(row=6, column=0, padx=10, pady=(5, 10), sticky="ew")
        log_frame.grid_columnconfigure(0, weight=1)
        log_frame.grid_rowconfigure(1, weight=1)

        ctk.CTkLabel(
            log_frame, text="7. ì²˜ë¦¬ ë¡œê·¸", font=ctk.CTkFont(weight="bold")
        ).grid(row=0, column=0, padx=10, pady=(5, 0), sticky="w")

        self.log_text = tk.Text(log_frame, height=15, wrap=tk.WORD)
        log_scrollbar = ctk.CTkScrollbar(log_frame, command=self.log_text.yview)
        self.log_text.configure(yscrollcommand=log_scrollbar.set)

        self.log_text.grid(row=1, column=0, padx=(10, 0), pady=10, sticky="ew")
        log_scrollbar.grid(row=1, column=1, padx=(0, 10), pady=10, sticky="ns")

    def log(self, message):
        """í–¥ìƒëœ ë¡œê·¸ ë©”ì‹œì§€ ì¶”ê°€ (GUI + íŒŒì¼)"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        formatted_message = f"[{timestamp}] {message}"

        # GUIì— í‘œì‹œ
        self.log_text.insert(tk.END, f"{formatted_message}\n")
        self.log_text.see(tk.END)
        # -------------------
        # CustomTkinterì—ì„œëŠ” self ìì²´ê°€ ë£¨íŠ¸ ìœˆë„ìš°ì…ë‹ˆë‹¤
        self.update_idletasks()
        # -------------------

        # íŒŒì¼ì— ì €ì¥ (ìë™ì €ì¥ì´ í™œì„±í™”ëœ ê²½ìš°)
        if self.auto_save_logs and hasattr(self, "logger"):
            clean_message = message
            self.logger.info(clean_message)

    def normalize_publication_year(self):
        """ë°œí–‰ë…„ë„ ë°ì´í„° ì •ê·œí™” (8/30 ì‘ì—… ë‚´ìš©)"""
        self.log("ğŸ”„ ë°œí–‰ë…„ë„ ì •ê·œí™” ì‹œì‘...")

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # 1ë‹¨ê³„: ë¶ˆëª… íŒ¨í„´ í†µì¼
                cursor.execute(
                    """
                    UPDATE mapping_data
                    SET publication_year = '[ë°œí–‰ë…„ë¶ˆëª…]'
                    WHERE publication_year IN (
                        'uuuu', '99999999', '[n.d.]',
                        '[Date of publication not identified]',
                        '[ë°œí–‰ë…„ë¯¸ìƒ]', '[ç™¼è¡Œå¹´ä¸æ˜]',
                        '', 'NULL'
                    ) OR publication_year IS NULL
                """
                )
                updated1 = cursor.rowcount

                # 2ë‹¨ê³„: ë°°ì—´ì—ì„œ ì—°ë„ ì¶”ì¶œ
                cursor.execute(
                    """
                    UPDATE mapping_data
                    SET publication_year = (
                        CASE
                            WHEN publication_year LIKE "%'20[0-9][0-9]'%" THEN
                                SUBSTR(publication_year, INSTR(publication_year, "'20") + 1, 4)
                            WHEN publication_year LIKE "%'19[0-9][0-9]'%" THEN
                                SUBSTR(publication_year, INSTR(publication_year, "'19") + 1, 4)
                            WHEN publication_year LIKE '%"20[0-9][0-9]"%' THEN
                                SUBSTR(publication_year, INSTR(publication_year, '"20') + 1, 4)
                            WHEN publication_year LIKE '%"19[0-9][0-9]"%' THEN
                                SUBSTR(publication_year, INSTR(publication_year, '"19') + 1, 4)
                            ELSE '[ë°œí–‰ë…„ë¶ˆëª…]'
                        END
                    )
                    WHERE publication_year LIKE '[%]'
                      AND publication_year != '[ë°œí–‰ë…„ë¶ˆëª…]'
                """
                )
                updated2 = cursor.rowcount

                # 3ë‹¨ê³„: ë‚ ì§œ/ë…„ì›”ì—ì„œ ì—°ë„ ì¶”ì¶œ
                cursor.execute(
                    """
                    UPDATE mapping_data
                    SET publication_year = SUBSTR(publication_year, 1, 4)
                    WHERE LENGTH(publication_year) >= 6
                      AND publication_year GLOB '[12][0-9][0-9][0-9][0-1][0-9]*'
                """
                )
                updated3 = cursor.rowcount

                # 4ë‹¨ê³„: ë¶ˆì™„ì „ íŒ¨í„´ ì •ë¦¬ (ìµœê³  ì„±ëŠ¥)
                cursor.execute(
                    """
                    UPDATE mapping_data
                    SET publication_year = '[ë°œí–‰ë…„ë¶ˆëª…]'
                    WHERE LENGTH(publication_year) < 4
                    OR publication_year NOT GLOB '[12][0-9][0-9][0-9]'  -- ì •ìƒ ì—°ë„ê°€ ì•„ë‹Œ ê²ƒ
                    OR INSTR(publication_year, 'uu') > 0    -- 'uu' í¬í•¨
                    OR INSTR(publication_year, '--') > 0    -- '--' í¬í•¨
                """
                )
                updated4 = cursor.rowcount

                conn.commit()

                total_updated = updated1 + updated2 + updated3 + updated4
                self.log(f"âœ… ë°œí–‰ë…„ë„ ì •ê·œí™” ì™„ë£Œ: {total_updated:,}ê±´ ì²˜ë¦¬")

        except Exception as e:
            self.log(f"âŒ ë°œí–‰ë…„ë„ ì •ê·œí™” ì˜¤ë¥˜: {e}")

    def normalize_identifier(self):
        """ì‹ë³„ì ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì œê±° (8/30 ì‘ì—… ë‚´ìš©)"""
        self.log("ğŸ”„ ì‹ë³„ì ì •ê·œí™” ì‹œì‘...")

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # nlk: ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì œê±°
                cursor.execute(
                    """
                    UPDATE mapping_data
                    SET identifier = SUBSTR(identifier, 5)
                    WHERE identifier LIKE 'nlk:%'
                """
                )

                updated = cursor.rowcount
                conn.commit()

                self.log(f"âœ… ì‹ë³„ì ì •ê·œí™” ì™„ë£Œ: {updated:,}ê±´ ì²˜ë¦¬")

        except Exception as e:
            self.log(f"âŒ ì‹ë³„ì ì •ê·œí™” ì˜¤ë¥˜: {e}")

    def normalize_ksh_identifiers(self):
        """KSH ì‹ë³„ì ì •ê·œí™” - nlk: ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì œê±° ë° í˜•ì‹ í†µì¼"""
        self.log("ğŸ”„ KSH ì‹ë³„ì ì •ê·œí™” ì‹œì‘...")

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # 1. nlk: ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì œê±°
                cursor.execute(
                    """
                    UPDATE mapping_data
                    SET ksh = REPLACE(ksh, 'nlk:', '')
                    WHERE ksh IS NOT NULL AND ksh != '' AND ksh LIKE '%nlk:%'
                    """
                )
                updated1 = cursor.rowcount

                # 2. ì„¸ë¯¸ì½œë¡ ì„ ì‰¼í‘œë¡œ í†µì¼ (ì„ íƒì )
                cursor.execute(
                    """
                    UPDATE mapping_data
                    SET ksh = REPLACE(ksh, ';', ',')
                    WHERE ksh IS NOT NULL AND ksh != '' AND ksh LIKE '%;%'
                    """
                )
                updated2 = cursor.rowcount

                # 3. ì•ë’¤ ê³µë°± ë° ì¤‘ë³µ êµ¬ë¶„ì ì •ë¦¬
                cursor.execute(
                    """
                    UPDATE mapping_data
                    SET ksh = TRIM(REPLACE(REPLACE(ksh, ', ,', ','), ',,', ','))
                    WHERE ksh IS NOT NULL AND ksh != ''
                    """
                )
                updated3 = cursor.rowcount

                conn.commit()
                total_updated = updated1 + updated2 + updated3

                self.log(f"âœ… KSH ì‹ë³„ì ì •ê·œí™” ì™„ë£Œ:")
                self.log(f"   - nlk: ì œê±°: {updated1:,}ê±´")
                self.log(f"   - êµ¬ë¶„ì í†µì¼: {updated2:,}ê±´")
                self.log(f"   - ê³µë°± ì •ë¦¬: {updated3:,}ê±´")
                self.log(f"   - ì´ ì²˜ë¦¬: {total_updated:,}ê±´")

        except Exception as e:
            self.log(f"âŒ KSH ì‹ë³„ì ì •ê·œí™” ì˜¤ë¥˜: {e}")

        except Exception as e:
            self.log(f"âŒ KSH í•œêµ­ì–´ ì»¬ëŸ¼ ìƒì„± ì˜¤ë¥˜: {e}")

    def _extract_ksh_from_item(self, item):
        """
        JSON ì•„ì´í…œì—ì„œ KSH (í•œêµ­ì‹­ì§„ë¶„ë¥˜ë²• ì£¼ì œëª…í‘œ) ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            item (dict): JSON-LD í˜•íƒœì˜ ì„œì§€ ë°ì´í„°

        Returns:
            str: KSH IDë“¤ì„ ì½¤ë§ˆë¡œ êµ¬ë¶„í•œ ë¬¸ìì—´ ë˜ëŠ” ë¹ˆ ë¬¸ìì—´
        """
        ksh_list = []

        try:
            # 1. 'subject' í•„ë“œì—ì„œ KSH ì¶”ì¶œ
            subject_data = item.get("subject")
            if subject_data:
                if isinstance(subject_data, list):
                    for subj in subject_data:
                        if isinstance(subj, str) and "KSH" in subj:
                            ksh_list.append(subj)
                elif isinstance(subject_data, str) and "KSH" in subject_data:
                    ksh_list.append(subject_data)

            # 2. ì§ì ‘ KSH ê´€ë ¨ í•„ë“œ í™•ì¸
            for key in ["ksh", "KSH", "subjectHeading"]:
                if key in item and item[key]:
                    ksh_value = item[key]
                    if isinstance(ksh_value, list):
                        ksh_list.extend([str(k) for k in ksh_value if "KSH" in str(k)])
                    elif isinstance(ksh_value, str) and "KSH" in ksh_value:
                        ksh_list.append(ksh_value)

            # 3. @typeì— ë”°ë¥¸ ì¶”ê°€ ì²˜ë¦¬
            if not ksh_list:
                # ì „ì²´ ì•„ì´í…œì„ ë¬¸ìì—´í™”í•˜ì—¬ KSH íŒ¨í„´ ê²€ìƒ‰
                item_str = str(item)
                ksh_matches = re.findall(r"KSH\d{10}", item_str)
                if ksh_matches:
                    ksh_list.extend(ksh_matches)

        except Exception as e:
            self.log(f"KSH ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

        # ì¤‘ë³µ ì œê±° ë° ê²°í•©
        unique_ksh = list(dict.fromkeys(ksh_list))  # ìˆœì„œ ë³´ì¡´í•˜ë©° ì¤‘ë³µ ì œê±°
        return "; ".join(unique_ksh) if unique_ksh else ""

    def _add_item_to_batch(self, item, batch_data, source_file):
        """JSON ì•„ì´í…œì„ ë°°ì¹˜ ë°ì´í„°ì— ì¶”ê°€í•©ë‹ˆë‹¤ (ìµœì‹  ìŠ¤í‚¤ë§ˆ ì ìš©)"""
        if not isinstance(item, dict):
            return 0

        # ê¸°ë³¸ í•„ë“œ ì¶”ì¶œ
        kdc_raw, ddc_raw = item.get("kdc", ""), item.get("ddc", "")
        kdc = (
            ", ".join(str(k).strip() for k in kdc_raw)
            if isinstance(kdc_raw, list)
            else str(kdc_raw).strip()
        )
        ddc = (
            ", ".join(str(d).strip() for d in ddc_raw)
            if isinstance(ddc_raw, list)
            else str(ddc_raw).strip()
        )

        if (not ddc or ddc.lower() == "nan") or (not kdc or kdc.lower() == "nan"):
            return 0

        # KSH ì¶”ì¶œ
        ksh = self._extract_ksh_from_item(item)
        ksh_labeled = ksh  # ì›ë³¸ í˜•íƒœ
        ksh_korean = re.sub(r"\[.*?\]", "", ksh).strip()  # í•œì ì œê±°

        # ê¸°íƒ€ í•„ë“œ ì¶”ì¶œ
        kdc_edition = str(item.get("editionOfKDC", "")).strip()
        ddc_edition = str(item.get("editionOfDDC", "")).strip()

        # ë°œí–‰ë…„ë„ ì •ê·œí™”
        publication_year = str(
            item.get("issuedYear", "") or item.get("issued", "")
        ).strip()
        publication_year = self._normalize_publication_year_value(publication_year)

        # ì œëª© ì¶”ì¶œ
        title_raw = item.get("title", "") or item.get("label", "")
        title = (
            title_raw[0]
            if isinstance(title_raw, list) and title_raw
            else str(title_raw).strip()
        )

        # ë°ì´í„° íƒ€ì… ê²°ì •
        item_types = item.get("@type", [])
        item_types = [item_types] if isinstance(item_types, str) else item_types
        data_type = "ê¸°íƒ€"
        if any("Thesis" in t for t in item_types):
            data_type = "í•™ìœ„ë…¼ë¬¸"
        elif any("ElectronicBook" in t for t in item_types):
            data_type = "ì „ìì±…"
        elif any("Book" in t for t in item_types):
            data_type = "ë‹¨í–‰ë³¸"

        # ì‹ë³„ì ì •ê·œí™” (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì œê±°)
        identifier = str(item.get("@id", "")).strip()
        if identifier.startswith("nlk:"):
            identifier = identifier[4:]  # nlk: ì œê±°

        # ë°°ì¹˜ ë°ì´í„°ì— ì¶”ê°€ (ìµœì‹  ìŠ¤í‚¤ë§ˆ)
        batch_data.append(
            (
                identifier,
                kdc,
                ddc,
                ksh,
                kdc_edition,
                ddc_edition,
                publication_year,
                title,
                data_type,
                source_file,
                ksh_labeled,
                ksh_korean,
            )
        )

        return 1

    def _normalize_publication_year_value(self, year_value):
        """ë°œí–‰ë…„ë„ ê°’ì„ ì •ê·œí™”í•©ë‹ˆë‹¤"""
        if not year_value or year_value.lower() in ["nan", "null", ""]:
            return "[ë°œí–‰ë…„ë¶ˆëª…]"

        year_str = str(year_value).strip()

        # ë¶ˆëª… íŒ¨í„´ ì²´í¬
        if year_str in [
            "uuuu",
            "99999999",
            "[n.d.]",
            "[Date of publication not identified]",
            "[ë°œí–‰ë…„ë¯¸ìƒ]",
            "[ç™¼è¡Œå¹´ä¸æ˜]",
        ]:
            return "[ë°œí–‰ë…„ë¶ˆëª…]"

        # 4ìë¦¬ ì—°ë„ ì¶”ì¶œ
        year_match = re.search(r"(19|20)\d{2}", year_str)
        if year_match:
            return year_match.group(0)

        return "[ë°œí–‰ë…„ë¶ˆëª…]"

    def _stream_json_to_db(self, json_file, cursor, conn, batch_data, batch_size):
        """JSON íŒŒì¼ì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ DBì— ì €ì¥í•©ë‹ˆë‹¤ (ìµœì‹  ìŠ¤í‚¤ë§ˆ ì ìš©)"""
        processed_count = 0
        file_size = json_file.stat().st_size

        with open(json_file, "rb") as f, tqdm(
            total=file_size, unit="B", unit_scale=True, desc=json_file.name, leave=False
        ) as pbar:
            parser = ijson.items(f, "@graph.item", multiple_values=True)
            try:
                for record in parser:
                    if self.stop_flag:
                        break
                    rows_added = self._add_item_to_batch(
                        record, batch_data, json_file.name
                    )
                    if rows_added > 0:
                        processed_count += rows_added

                    if len(batch_data) >= batch_size:
                        # ìµœì‹  ìŠ¤í‚¤ë§ˆì— ë§ëŠ” INSERT êµ¬ë¬¸ (12ê°œ ì»¬ëŸ¼)
                        cursor.executemany(
                            """
                            INSERT INTO mapping_data
                            (identifier, kdc, ddc, ksh, kdc_edition, ddc_edition,
                             publication_year, title, data_type, source_file,
                             ksh_labeled, ksh_korean)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """,
                            batch_data,
                        )
                        conn.commit()
                        batch_data.clear()

                    if processed_count % 100 == 0:
                        pbar.update(f.tell() - pbar.n)

            except ijson.JSONError as e:
                self.log(f"ijson íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ ({json_file.name}): {e}")

        return processed_count

    def build_database(self):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤"""

        def db_worker():
            self.status_var.set("DB êµ¬ì¶• ì¤‘...")
            json_folder = Path(self.json_path_var.get().strip())

            if not json_folder.exists():
                self.log("JSON í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                self.status_var.set("ëŒ€ê¸° ì¤‘...")
                return

            json_files = list(json_folder.glob("*.json"))
            if not json_files:
                self.log("JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                self.status_var.set("ëŒ€ê¸° ì¤‘...")
                return

            # -------------------
            # âœ… ì¶”ê°€: ì „ì²´ íŒŒì¼ ìˆ˜ í‘œì‹œ
            total_files = len(json_files)
            self.log(f"{total_files}ê°œì˜ JSON íŒŒì¼ ë°œê²¬")
            # -------------------

            try:
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.cursor()
                    batch_data = []
                    batch_size = 1000
                    total_processed = 0

                    # -------------------
                    # âœ… ìˆ˜ì •: íŒŒì¼ ì§„í–‰ë¥  í‘œì‹œ ì¶”ê°€
                    for file_index, json_file in enumerate(json_files, 1):
                        if self.stop_flag:
                            break

                        self.log(
                            f"ì²˜ë¦¬ ì¤‘ ({file_index}/{total_files}): {json_file.name}"
                        )
                        processed = self._stream_json_to_db(
                            json_file, cursor, conn, batch_data, batch_size
                        )
                        total_processed += processed
                        self.log(
                            f"ì™„ë£Œ ({file_index}/{total_files}): {json_file.name} - {processed:,}ê±´ ì²˜ë¦¬"
                        )
                    # -------------------

                    # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
                    if batch_data and not self.stop_flag:
                        cursor.executemany(
                            """
                            INSERT INTO mapping_data
                            (identifier, kdc, ddc, ksh, kdc_edition, ddc_edition,
                             publication_year, title, data_type, source_file,
                             ksh_labeled, ksh_korean)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """,
                            batch_data,
                        )
                        conn.commit()

                self.log(
                    f"SQLite DB êµ¬ì¶• ì™„ë£Œ! ì´ {total_processed:,}ê±´ ì²˜ë¦¬"
                    if not self.stop_flag
                    else "DB êµ¬ì¶•ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."
                )
                self.check_existing_db()

            except Exception as e:
                self.log(f"DB êµ¬ì¶• ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            finally:
                self.status_var.set("ëŒ€ê¸° ì¤‘...")

        threading.Thread(target=db_worker, daemon=True).start()

    def select_json_folder(self):
        """JSON í´ë”ë¥¼ ì„ íƒí•©ë‹ˆë‹¤"""
        folder_path = filedialog.askdirectory(title="JSON ì„œì§€ë°ì´í„° í´ë” ì„ íƒ")
        if folder_path:
            self.json_path_var.set(folder_path)
            self.log(f"JSON í´ë” ì„ íƒë¨: {folder_path}")

    def select_excel_file(self):
        """ì—‘ì…€ íŒŒì¼ì„ ì„ íƒí•©ë‹ˆë‹¤"""
        file_path = filedialog.askopenfilename(
            title="ë¶€ì‚°êµëŒ€ ì—‘ì…€ íŒŒì¼ ì„ íƒ",
            filetypes=[("Excel files", "*.xlsx *.xls"), ("All files", "*.*")],
        )
        if file_path:
            self.excel_path_var.set(file_path)
            self.log(f"ì—‘ì…€ íŒŒì¼ ì„ íƒë¨: {file_path}")

    def stop_process(self):
        """ì²˜ë¦¬ ì¤‘ë‹¨"""
        self.stop_flag = True
        self.log("ì²˜ë¦¬ ì¤‘ë‹¨ ìš”ì²­ë¨...")

    def delete_database(self):
        """ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤"""
        if os.path.exists(self.db_path):
            if messagebox.askyesno("í™•ì¸", "ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
                os.remove(self.db_path)
                self.log("ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì‚­ì œë¨")
                self.check_existing_db()
        else:
            self.log("ì‚­ì œí•  ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")

    def start_mapping(self):
        """ë§¤í•‘ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤"""
        excel_path = self.excel_path_var.get().strip()
        if not excel_path or not Path(excel_path).exists():
            messagebox.showerror("ì˜¤ë¥˜", "ìœ íš¨í•œ ì—‘ì…€ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return

        def mapping_worker():
            self.status_var.set("ë§¤í•‘ ì¤‘...")
            try:
                # ì—‘ì…€ íŒŒì¼ ì½ê¸°
                df = pd.read_excel(excel_path)
                self.log(f"ì—‘ì…€ íŒŒì¼ ì½ê¸° ì™„ë£Œ: {len(df)}í–‰")

                # ë§¤í•‘ ë¡œì§ ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                results = self.perform_mapping(df)

                # ê²°ê³¼ ì €ì¥
                output_path = self.save_results_to_excel(results, excel_path)
                self.log(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")

            except Exception as e:
                self.log(f"ë§¤í•‘ ì¤‘ ì˜¤ë¥˜: {e}")
            finally:
                self.status_var.set("ëŒ€ê¸° ì¤‘...")

        threading.Thread(target=mapping_worker, daemon=True).start()

    def perform_mapping(self, df):
        """ì‹¤ì œ ë§¤í•‘ ë¡œì§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤"""
        results = []

        with sqlite3.connect(self.db_path) as conn:
            for idx, row in df.iterrows():
                if self.stop_flag:
                    break

                # ê¸°ì¡´ ë§¤í•‘ ë¡œì§ (ISBN, KDC ë§¤ì¹­ ë“±)
                # ì—¬ê¸°ì— ì›ë˜ ë§¤í•‘ ë¡œì§ì„ êµ¬í˜„
                result = self._map_single_row(row, conn)
                results.append(result)

                if idx % 100 == 0:
                    self.log(f"ë§¤í•‘ ì§„í–‰: {idx}/{len(df)}")

        return results

    def _map_single_row(self, row, conn):
        """ë‹¨ì¼ í–‰ì— ëŒ€í•œ ë§¤í•‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤"""
        # ê¸°ì¡´ ë§¤í•‘ ë¡œì§ êµ¬í˜„
        # ISBN ë§¤ì¹­, KDC ìƒìœ„ ë§¤ì¹­ ë“±
        return {
            "original_data": row.to_dict(),
            "mapped_ddc": "",  # ë§¤í•‘ëœ DDC
            "mapping_method": "",  # ë§¤í•‘ ë°©ë²•
            "confidence": 0.0,  # ì‹ ë¢°ë„
        }

    def save_results_to_excel(self, results, original_path):
        """ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = Path(original_path).parent / f"ë§¤í•‘ê²°ê³¼_{timestamp}.xlsx"

        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df_results = pd.DataFrame(results)

        # ì—‘ì…€ë¡œ ì €ì¥
        df_results.to_excel(output_path, index=False)

        return output_path


if __name__ == "__main__":
    app = KDCDDCMapperSQLite()
    app.mainloop()
