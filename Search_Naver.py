# -*- coding: utf-8 -*-
# Version: v1.0.0
# ìƒì„±ì¼ì‹œ: 2025-08-10 KST (GAS ë„¤ì´ë²„ API ë¡œì§ì„ íŒŒì´ì¬ìœ¼ë¡œ í¬íŒ…)
# ìˆ˜ì •ì¼ì‹œ: 2025-09-17 KST
"""
Search_Naver.py - ë„¤ì´ë²„ ê²€ìƒ‰ì„ ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ ë„ì„œ ì •ë³´ ìˆ˜ì§‘ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë„¤ì´ë²„ ì±… APIì™€ ì›¹ ìŠ¤í¬ë ˆì´í•‘ì„ ê²°í•©í•˜ì—¬ ë„ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ë³´ê°•í•˜ëŠ” ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤.
ì£¼ìš” ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
1.  **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**:
    - ë„¤ì´ë²„ ì±… APIë¥¼ í†µí•´ ISBN, ì œëª©, ì €ì ë“± ë‹¤ì–‘í•œ ì¡°ê±´ìœ¼ë¡œ ê¸°ë³¸ ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ ì¡°íšŒí•©ë‹ˆë‹¤.
    - API ê²°ê³¼ë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•œ ìƒì„¸ ì •ë³´(ì €ì ì†Œê°œ, ëª©ì°¨, ì¶œíŒì‚¬ ì„œí‰, ë‹¤ë¥¸ ì‘í’ˆ ëª©ë¡ ë“±)ë¥¼ ì–»ê¸° ìœ„í•´, `requests`ì™€ `BeautifulSoup`ë¥¼ ì‚¬ìš©í•˜ì—¬ Yes24ì™€ êµë³´ë¬¸ê³ ì˜ ë„ì„œ ìƒì„¸ í˜ì´ì§€ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤í¬ë ˆì´í•‘í•©ë‹ˆë‹¤.

2.  **ë°ì´í„° ë³´ê°• ë° ë³‘í•©**:
    - ISBN ê²€ìƒ‰ ì‹œ, ë³‘ë ¬ ìŠ¤ë ˆë“œ(`threading`)ë¥¼ ì´ìš©í•´ Yes24ì™€ êµë³´ë¬¸ê³  ìŠ¤í¬ë ˆì´í•‘ì„ ë™ì‹œì— ìˆ˜í–‰í•˜ì—¬ ì‘ë‹µ ì‹œê°„ì„ ë‹¨ì¶•í•©ë‹ˆë‹¤.
    - ê° ì†ŒìŠ¤(Naver, Yes24, Kyobo)ì—ì„œ ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'AI-Feed Merge', 'OtherWorks Merge' ë“± ì—¬ëŸ¬ ë²„ì „ì˜ ê°€ê³µëœ ê²°ê³¼ ë ˆì½”ë“œë¥¼ ìƒì„±í•˜ì—¬, ì‚¬ìš© ëª©ì ì— ë§ëŠ” í’ë¶€í•œ ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

3.  **ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜**:
    - ì €ì ì†Œê°œ í…ìŠ¤íŠ¸ì—ì„œ ë‹¤ë¥¸ ì €ì‘ë¬¼ ëª©ë¡ì„ ì¶”ì¶œí•˜ê³  ì •ê·œí™”í•˜ëŠ”(`extract_other_works_grouped`) ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.

GAS(Google Apps Script)ì˜ `fetchNaverBookInfo` í•¨ìˆ˜ë¥¼ íŒŒì´ì¬ìœ¼ë¡œ í¬íŒ…í•œ ê²ƒì„ ì‹œì‘ìœ¼ë¡œ, í˜„ì¬ëŠ” í›¨ì”¬ ë” ê³ ë„í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ë„ë¡ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.
"""
import requests
import xml.etree.ElementTree as ET
import re
import time
import urllib.parse
import threading  # âœ… [ì¶”ê°€] ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ threading ëª¨ë“ˆì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from bs4 import BeautifulSoup  # âœ… ìƒˆë¡œ ì¶”ê°€
from qt_api_clients import clean_text
from database_manager import DatabaseManager

import re as _re
import unicodedata as _ud

# âœ… [ì¶”ê°€] PyInstaller í™˜ê²½ì—ì„œ SSL ì¸ì¦ì„œ ê²½ë¡œ ì„¤ì •
from ssl_cert_utils import configure_ssl_certificates

configure_ssl_certificates()

_MEDIA_STOPWORDS = (
    "ë‰´ìš• íƒ€ì„ìŠ¤",
    "ë‰´ìš•íƒ€ì„ìŠ¤",
    "ì›Œì‹±í„´í¬ìŠ¤íŠ¸",
    "ë¡œìŠ¤ì•¤ì ¤ë ˆìŠ¤ íƒ€ì„ìŠ¤",
    "í—ˆí•‘í„´ í¬ìŠ¤íŠ¸",
    "ì»¤ì»¤ìŠ¤ ë¦¬ë·°",
    "í”¼í”Œ",
    "ìœ ì—ìŠ¤ ìœ„í´ë¦¬",
    "íƒ€ì„",
    "LA íƒ€ì„ìŠ¤",
    "Washington Post",
    "New York Times",
    "Huffington Post",
    "Kirkus Reviews",
    "People",
    "Us Weekly",
    "TIME",
    "ì¡°ì„ ì¼ë³´",
    "ì¤‘ì•™ì¼ë³´",
    "í•œê²¨ë ˆ",
    "ê²½í–¥ì‹ ë¬¸",
    "ë™ì•„ì¼ë³´",
)

_EDGE_QUOTES_RE = _re.compile(r'^[ã€Šã€ˆ<Â«â‰ªã€ã€Œâ€œ"\']+|[ã€‹ã€‰>Â»â‰«ã€ã€â€"\']+$')
_TAIL_PAREN_RE = _re.compile(
    r"[\(\[\{ï¼ˆï¼»ï½›ã€”ã€][^)\]\}ï¼‰ï¼½ï½ã€•ã€‘]*[\)\]\}ï¼‰ï¼½ï½ã€•ã€‘]\s*$"
)


def normalize_title_for_match(s: str) -> str:
    t = _ud.normalize("NFKC", (s or "").strip())
    # ì–‘ë ì¸ìš©/êº¾ì‡  ì œê±° ë°˜ë³µ
    prev = None
    while prev != t:
        prev = t
        t = _EDGE_QUOTES_RE.sub("", t).strip()
    # ë§ë¯¸ ê´„í˜¸ ê¼¬ë¦¬ ë°˜ë³µ ì œê±°
    while _TAIL_PAREN_RE.search(t):
        t = _TAIL_PAREN_RE.sub("", t).strip()
    # ê³µë°± ì¶•ì†Œ
    t = _re.sub(r"\s{2,}", " ", t)
    return t


def combine_author_bios(*bios: str) -> str:
    parts = [b.strip() for b in bios if b and b.strip()]
    return "\n\n".join(parts)


def extract_other_works_from_author_bio(
    author_bio: str, current_title: str
) -> list[str]:
    """
    - ì˜ˆìŠ¤24/êµë³´ ì €ìì†Œê°œë¥¼ í•©ì¹œ ë³¸ë¬¸ì—ì„œ ì¸ìš©ë¶€í˜¸ ë‚´ ì„œëª…ë§Œ ì¶”ì¶œ
    - ë§¤ì²´/ì–¸ë¡ ì‚¬ëª… í•„í„°ë§
    - ì‰¼í‘œ/ì¤‘ì  ë¶„í•´
    """
    if not author_bio:
        return []
    cur = normalize_title_for_match(current_title)

    # ì¸ìš©ë¶€í˜¸ ë‚´ë¶€ ìº¡ì²˜
    pat = _re.compile(r'[ã€Šã€ˆ<Â«â‰ªã€ã€Œâ€œ"\']\s*(.+?)\s*[ã€‹ã€‰>Â»â‰«ã€ã€â€"\']')
    cand = [m.group(1).strip() for m in pat.finditer(author_bio)]

    # ë‚˜ì—´ ë¶„í•´
    spread = []
    for c in cand:
        if ("Â·" in c) or ("," in c) or ("ï¼Œ" in c):
            spread.extend([x.strip() for x in _re.split(r"[ï¼Œ,Â·]", c) if x.strip()])
        else:
            spread.append(c)

    # ì •ê·œí™” + í•„í„°
    seen, out = set(), []
    for t in spread:
        # ê´„í˜¸ ì† ì˜ë¬¸ ì›ì œ ë³´ì¡´ (ì˜ˆ: (The Seven Husbands of Evelyn Hugo))
        m = _re.search(r"[\(ï¼ˆ]([^)ï¼‰]+)[\)ï¼‰]", t)
        original_eng = f"({m.group(1).strip()})" if m else ""

        n = normalize_title_for_match(t)
        if not n or len(n) < 2 or len(n) > 120:  # â† 80 â†’ 120
            continue
        if n == cur:
            continue
        # ë§¤ì²´ í•„í„°ëŠ” 'ì œëª©'ì—ë§Œ ì ìš© (ì›ì„œëª… ì¡°ê°ì—” ì ìš© X)
        if any(sw in n for sw in _MEDIA_STOPWORDS):
            continue
        if _re.search(r"(ë¦¬ë·°|íƒ€ì„ìŠ¤|í¬ìŠ¤íŠ¸|ìœ„í´ë¦¬)\s*$", n):
            continue

        # í‘œì‹œìš© ì œëª©: ì •ê·œí™”ëœ í•œê¸€í‘œì œ + (ì›ì„œëª…) ë³‘ê¸°
        display_title = (
            f"{n}{original_eng}" if original_eng and original_eng not in n else n
        )

        if display_title not in seen:
            seen.add(display_title)
            out.append(display_title)

    return out  # âœ… ëˆ„ë½ëœ ë°˜í™˜ ì¶”ê°€


def extract_other_works_grouped(
    author_bio_blocks: list[str], current_title: str
) -> list[dict]:
    """
    author_bio_blocks: ì €ìë³„ ì†Œê°œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (ì˜ˆìŠ¤24/êµë³´ í•©ë³¸)
    return: [{"author_label": "ì €ì1", "works": [{"title": "ë²ˆì—­ì„œëª…", "orig": "ì›ì„œëª… or ''"}, ...]}, ...]
    """
    groups = []
    for idx, bio in enumerate(author_bio_blocks or []):
        works = []
        # ê¸°ì¡´ ë‹¨ì¼ ì¶”ì¶œ ë¡œì§ ì¬ì‚¬ìš©(ì›ì„œëª… ë³‘ê¸° ìœ ì§€)
        # cand/spread ê³„ì‚°
        pat = _re.compile(r'[ã€Šã€ˆ<Â«â‰ªã€ã€Œâ€œ"\']\s*(.+?)\s*[ã€‹ã€‰>Â»â‰«ã€ã€â€"\']')
        cand = [m.group(1).strip() for m in pat.finditer(bio)]
        spread = []
        for c in cand:
            if ("Â·" in c) or ("," in c) or ("ï¼Œ" in c):
                spread.extend([x.strip() for x in _re.split(r"[ï¼Œ,Â·]", c) if x.strip()])
            else:
                spread.append(c)

        seen = set()
        for t in spread:
            # ê´„í˜¸ ì† ì›ì œ(ì˜ë¬¸/ê¸°íƒ€) ì¶”ì¶œ â†’ ê´„í˜¸ ì œê±°ë³¸ì„ origë¡œ ì‚¬ìš©
            m = _re.search(r"[\(ï¼ˆ]([^)ï¼‰]+)[\)ï¼‰]", t)
            orig = m.group(1).strip() if m else ""
            normalized = normalize_title_for_match(t)
            if not normalized or len(normalized) < 2 or len(normalized) > 120:
                continue
            if normalized == normalize_title_for_match(current_title):
                continue
            if any(sw in normalized for sw in _MEDIA_STOPWORDS):
                continue
            if _re.search(r"(ë¦¬ë·°|íƒ€ì„ìŠ¤|í¬ìŠ¤íŠ¸|ìœ„í´ë¦¬)\s*$", normalized):
                continue

            # í‘œì‹œëŠ” ë²ˆì—­ì„œëª…(ì •ê·œí™” ê²°ê³¼)
            display_title = normalized
            key = (display_title, orig)
            if key in seen:
                continue
            seen.add(key)
            works.append({"title": display_title, "orig": orig})

        groups.append({"author_label": f"ì €ì{idx+1}", "works": works})
    return groups


def render_other_works_grouped(
    groups: list[dict], author_names: list[str] | None = None
) -> str:
    """
    groups: extract_other_works_grouped() ê²°ê³¼
    author_names: ì €ìëª… ë¦¬ìŠ¤íŠ¸(ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©), ì—†ìœ¼ë©´ groupsì˜ author_label ì‚¬ìš©
    """
    lines = ["4. ë‹¤ë¥¸ ì‘í’ˆ", ""]
    for i, g in enumerate(groups):
        header = (
            author_names[i].strip()
            if author_names and i < len(author_names) and author_names[i].strip()
            else g.get("author_label", f"ì €ì{i+1}")
        )
        if not g.get("works"):
            continue

        # âœ… ì €ì í—¤ë”ë¥¼ ëª…í™•íˆ ê°•ì¡° (CTk TextBrowserìš©)
        lines.append(f"â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•")
        lines.append(header)
        lines.append("")

        for w in g["works"]:
            lines.append(w["title"])
            if w.get("orig"):  # ì›ì„œ ì œëª©ì€ ê´„í˜¸ ì—†ì´ ë°”ë¡œ ì•„ë˜ í–‰
                lines.append(w["orig"])

        # âœ… ì €ì ë¸”ë¡ ì‚¬ì´ì— êµ¬ë¶„ìš© ë¹ˆ ì¤„ 2ì¤„
        lines.extend(["", ""])

    # ë§ˆì§€ë§‰ ê³µë°± ì •ë¦¬
    while lines and not lines[-1].strip():
        lines.pop()

    return "\n".join(lines)


def get_naver_api_credentials(db_manager):
    """
    ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸ IDì™€ ì‹œí¬ë¦¿ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    GAS getNaverApiCredentialsInternal() í•¨ìˆ˜ í¬íŒ…

    Args:
        db_manager (DatabaseManager): ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤

    Returns:
        tuple: (client_id, client_secret) ë˜ëŠ” (None, None) if not found
    """
    try:
        return db_manager.get_naver_api_credentials()
    except Exception as e:
        print(f"ì˜¤ë¥˜: ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None, None


def search_naver_catalog(
    title_query, author_query, isbn_query, app_instance=None, db_manager=None
):
    """
    ë„¤ì´ë²„ ì±… APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì œëª©, ì €ì, ISBNìœ¼ë¡œ ë„ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    GAS fetchNaverBookInfo() í•¨ìˆ˜ë¥¼ íŒŒì´ì¬ìœ¼ë¡œ í¬íŒ…í•˜ì—¬ ë‹¤ì¤‘ ê²€ìƒ‰ ì¡°ê±´ ì§€ì›

    Args:
        title_query (str): ê²€ìƒ‰í•  ì±… ì œëª©
        author_query (str): ê²€ìƒ‰í•  ì €ìëª…
        isbn_query (str): ê²€ìƒ‰í•  ISBN
        app_instance (object, optional): GUI ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ (ë¡œê·¸ìš©)
        db_manager (DatabaseManager, optional): ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤

    Returns:
        list: ê²€ìƒ‰ ê²°ê³¼ ë ˆì½”ë“œ ëª©ë¡. ê° ë ˆì½”ë“œëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœ
    """
    # ê²€ìƒ‰ì–´ ìœ íš¨ì„± ê²€ì‚¬
    if not any([title_query, author_query, isbn_query]):
        if app_instance:
            app_instance.log_message(
                "ê²½ê³ : ì œëª©, ì €ì, ISBN ì¤‘ í•˜ë‚˜ ì´ìƒì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", level="WARNING"
            )
        return []

    if not db_manager:
        if app_instance:
            app_instance.log_message(
                "ì˜¤ë¥˜: DatabaseManager ì¸ìŠ¤í„´ìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.", level="ERROR"
            )
        return []

    if app_instance:
        search_info = (
            f"ì œëª©='{title_query}', ì €ì='{author_query}', ISBN='{isbn_query}'"
        )
        app_instance.log_message(f"ì •ë³´: ë„¤ì´ë²„ ì±… API ê²€ìƒ‰ ì‹œì‘ ({search_info})")
        app_instance.update_progress(10)

    # ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    client_id, client_secret = get_naver_api_credentials(db_manager)
    if not client_id or not client_secret:
        if app_instance:
            app_instance.log_message(
                "ì˜¤ë¥˜: ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸ ID ë˜ëŠ” ì‹œí¬ë¦¿ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì • íƒ­ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.",
                level="ERROR",
            )
        return []

    # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ìš°ì„ ìˆœìœ„: ISBN > ì œëª©+ì €ì > ì œëª© > ì €ì)
    if isbn_query:
        # ISBN ê²€ìƒ‰ (ê°€ì¥ ì •í™•í•¨)
        api_url = (
            f"https://openapi.naver.com/v1/search/book_adv.xml?d_isbn={isbn_query}"
        )
        search_type = "ISBN ê²€ìƒ‰"
        primary_query = isbn_query
    elif title_query and author_query:
        # ì œëª© + ì €ì ì¡°í•© ê²€ìƒ‰
        query = f"{title_query} {author_query}"
        api_url = f"https://openapi.naver.com/v1/search/book.xml?query={requests.utils.quote(query)}&display=100"
        search_type = "ì œëª©+ì €ì ê²€ìƒ‰"
        primary_query = query
    elif title_query:
        # ì œëª©ë§Œ ê²€ìƒ‰
        api_url = f"https://openapi.naver.com/v1/search/book.xml?query={requests.utils.quote(title_query)}&display=100"
        search_type = "ì œëª© ê²€ìƒ‰"
        primary_query = title_query
    elif author_query:
        # ì €ìë§Œ ê²€ìƒ‰
        api_url = f"https://openapi.naver.com/v1/search/book.xml?query={requests.utils.quote(author_query)}&display=100"
        search_type = "ì €ì ê²€ìƒ‰"
        primary_query = author_query
    else:
        return []

    if app_instance:
        app_instance.log_message(f"ì •ë³´: ë„¤ì´ë²„ API ìš”ì²­ URL: {api_url}")
        app_instance.update_progress(30)

    # API ìš”ì²­ í—¤ë” ì„¤ì •
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    }

    # ê¸°ë³¸ê°’ ì„¤ì •
    results = []

    try:
        # API í˜¸ì¶œ
        response = requests.get(api_url, headers=headers, timeout=10)
        response_code = response.status_code
        response_text = response.text

        if app_instance:
            app_instance.log_message(f"ì •ë³´: ë„¤ì´ë²„ API ì‘ë‹µ ì½”ë“œ: {response_code}")
            app_instance.update_progress(60)

        if response_code == 200:
            try:
                # XML íŒŒì‹± (GAS ì½”ë“œì™€ ë™ì¼í•œ êµ¬ì¡°)
                root = ET.fromstring(response_text)
                channel = root.find("channel")

                if channel is not None:
                    items = channel.findall("item")

                    if items:
                        for item in items:
                            # ê° í•­ëª©ì—ì„œ ì •ë³´ ì¶”ì¶œ (HTML íƒœê·¸ ì œê±°)
                            title = clean_text(item.findtext("title", "ì •ë³´ ì—†ìŒ"))
                            author = clean_text(item.findtext("author", "ì •ë³´ ì—†ìŒ"))
                            publisher = clean_text(
                                item.findtext("publisher", "ì •ë³´ ì—†ìŒ")
                            )
                            pubdate = clean_text(item.findtext("pubdate", "ì •ë³´ ì—†ìŒ"))
                            isbn = clean_text(item.findtext("isbn", "ì •ë³´ ì—†ìŒ"))
                            price = clean_text(item.findtext("price", "ì •ë³´ ì—†ìŒ"))
                            description = clean_text(
                                item.findtext("description", "ì •ë³´ ì—†ìŒ")
                            )
                            link = item.findtext("link", "")

                            # ê°€ê²© í¬ë§·íŒ…
                            if price and price != "ì •ë³´ ì—†ìŒ" and price.isdigit():
                                price = f"{int(price):,}ì›"

                            # ì¶œê°„ì¼ í¬ë§·íŒ… (YYYYMMDD â†’ YYYY-MM-DD)
                            if (
                                pubdate
                                and pubdate != "ì •ë³´ ì—†ìŒ"
                                and len(pubdate) == 8
                                and pubdate.isdigit()
                            ):
                                pubdate = f"{pubdate[:4]}-{pubdate[4:6]}-{pubdate[6:8]}"

                            # âœ… 1. ë„¤ì´ë²„ API ê¸°ë³¸ ê²°ê³¼
                            naver_record = {
                                "ê²€ìƒ‰ì†ŒìŠ¤": "Naver",
                                "ì„œëª…": title,
                                "ì €ì": author,
                                "ë¶„ë¥˜ ì •ë³´ ì·¨í•©": description,
                                "ì €ìì†Œê°œ": "",  # ë„¤ì´ë²„ APIëŠ” ì œê³µ ì•ˆí•¨
                                "ëª©ì°¨": "",  # ë„¤ì´ë²„ APIëŠ” ì œê³µ ì•ˆí•¨
                                "ì„œí‰": description,
                                "ISBN": isbn,
                                "ì¶œíŒì‚¬": publisher,
                                "ì¶œê°„ì¼": pubdate,
                                "ê°€ê²©": price,
                                "ë§í¬": link,
                            }
                            results.append(naver_record)
                            # âœ… 2. ì˜ˆìŠ¤24 ë° êµë³´ë¬¸ê³  ì¶”ê°€ ì •ë³´
                            # -------------------
                            # âœ… ìˆ˜ì •: ISBNìœ¼ë¡œ ê²€ìƒ‰í–ˆì„ ë•Œë§Œ ì›¹ìŠ¤í¬ë ˆì´í•‘ ë¡œì§ ì‹¤í–‰
                            if (
                                search_type == "ISBN ê²€ìƒ‰"
                                and isbn
                                and isbn != "ì •ë³´ ì—†ìŒ"
                            ):
                                clean_isbn = isbn.strip()
                                if clean_isbn:
                                    # ê° ìŠ¤í¬ë ˆì´í¼ì˜ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
                                    scraping_results = {"yes24": {}, "kyobo": {}}

                                    # ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰í•  í•¨ìˆ˜ ì •ì˜
                                    def run_scraper(site, isbn_code):
                                        if site == "yes24":
                                            scraping_results["yes24"] = (
                                                scrape_yes24_book_info(
                                                    isbn_code, app_instance
                                                )
                                            )
                                        elif site == "kyobo":
                                            scraping_results["kyobo"] = (
                                                scrape_kyobo_book_info(
                                                    isbn_code, app_instance
                                                )
                                            )

                                    # ìŠ¤ë ˆë“œ ìƒì„± ë° ì‹œì‘
                                    yes24_thread = threading.Thread(
                                        target=run_scraper, args=("yes24", clean_isbn)
                                    )
                                    kyobo_thread = threading.Thread(
                                        target=run_scraper, args=("kyobo", clean_isbn)
                                    )

                                    yes24_thread.start()
                                    kyobo_thread.start()

                                    # ë‘ ìŠ¤ë ˆë“œê°€ ëª¨ë‘ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
                                    yes24_thread.join(timeout=15)
                                    kyobo_thread.join(timeout=15)

                                    yes24_info = scraping_results["yes24"]
                                    kyobo_info = scraping_results["kyobo"]

                                    # -------------------
                                    # -------------------
                                    # âœ… [ìˆ˜ì •] ì˜ˆìŠ¤24: ì €ìì†Œê°œ, ëª©ì°¨, ì„œí‰ ìˆ˜ì§‘
                                    author_intro_y24 = yes24_info.get("ì €ìì†Œê°œ", "")
                                    toc_y24 = yes24_info.get("ëª©ì°¨", "")
                                    review_y24 = yes24_info.get("ì¶œíŒì‚¬ì„œí‰", "")

                                    review_parts_y24 = []
                                    if author_intro_y24:
                                        review_parts_y24.append(
                                            f"1. ì €ì ì†Œê°œ\n{author_intro_y24}"
                                        )
                                    if toc_y24:
                                        review_parts_y24.append(f"2. ëª©ì°¨\n{toc_y24}")
                                    if review_y24:
                                        review_parts_y24.append(
                                            f"3. ì„œí‰\n{review_y24}"
                                        )

                                    # âœ… [í•µì‹¬ ì¶”ê°€] ì˜ˆìŠ¤24 í–‰ ìƒì„± ë° append (ë¹ ì ¸ìˆë˜ ë¶€ë¶„)
                                    yes24_record = {
                                        "ê²€ìƒ‰ì†ŒìŠ¤": "Yes24",
                                        "ì„œëª…": title,
                                        "ì €ì": author,
                                        "ë¶„ë¥˜ ì •ë³´ ì·¨í•©": "\n\n".join(review_parts_y24),
                                        "ì €ìì†Œê°œ": author_intro_y24,
                                        "ëª©ì°¨": toc_y24,
                                        "ì„œí‰": review_y24,
                                        "ISBN": isbn,
                                        "ì¶œíŒì‚¬": publisher,
                                        "ì¶œê°„ì¼": pubdate,
                                        "ê°€ê²©": price,
                                        "ë§í¬": yes24_info.get("ìƒí’ˆë§í¬", link),
                                    }
                                    results.append(yes24_record)

                                    # âœ… ì˜ˆìŠ¤24 / êµë³´ ì €ìì†Œê°œ ì·¨í•©
                                    author_intro_kb = kyobo_info.get("ì €ìì†Œê°œ", "")
                                    toc_kb = kyobo_info.get("ëª©ì°¨", "")
                                    review_kb = kyobo_info.get("ì¶œíŒì‚¬ì„œí‰", "")

                                    review_parts_kb = []
                                    if author_intro_kb:
                                        review_parts_kb.append(
                                            f"1. ì €ì ì†Œê°œ\n{author_intro_kb}"
                                        )
                                    if toc_kb:
                                        review_parts_kb.append(f"2. ëª©ì°¨\n{toc_kb}")
                                    if review_kb:
                                        review_parts_kb.append(f"3. ì„œí‰\n{review_kb}")

                                    kyobo_record = {
                                        "ê²€ìƒ‰ì†ŒìŠ¤": "Kyobo Book",
                                        "ì„œëª…": title,
                                        "ì €ì": author,
                                        "ë¶„ë¥˜ ì •ë³´ ì·¨í•©": "\n\n".join(review_parts_kb),
                                        "ì €ìì†Œê°œ": author_intro_kb,
                                        "ëª©ì°¨": toc_kb,
                                        "ì„œí‰": review_kb,
                                        "ISBN": isbn,
                                        "ì¶œíŒì‚¬": publisher,
                                        "ì¶œê°„ì¼": pubdate,
                                        "ê°€ê²©": price,
                                        "ë§í¬": kyobo_info.get("ìƒí’ˆë§í¬", link),
                                    }
                                    results.append(kyobo_record)

                                    # âœ… 3-a) ê¸¸ì´ ìš°ì„  ë³‘í•©(ì €ì/ëª©ì°¨/ì„œí‰) ê³„ì‚° ì¶”ê°€
                                    def _longer(a: str, b: str) -> str:
                                        return (
                                            (a or "")
                                            if len(a or "") >= len(b or "")
                                            else (b or "")
                                        )

                                    merged_author = _longer(
                                        author_intro_y24, author_intro_kb
                                    )
                                    merged_toc = _longer(toc_y24, toc_kb)
                                    merged_review = _longer(review_y24, review_kb)

                                    merged_parts = []
                                    if merged_author:
                                        merged_parts.append(
                                            f"1. ì €ì ì†Œê°œ\n{merged_author}"
                                        )
                                    if merged_toc:
                                        merged_parts.append(f"2. ëª©ì°¨\n{merged_toc}")
                                    if merged_review:
                                        merged_parts.append(f"3. ì„œí‰\n{merged_review}")

                                    # âœ… 3-b) ì €ìë³„ ë¸”ë¡ ì¶”ì¶œ/ë Œë” (ìš°ì„ : ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜)
                                    author_blocks = []
                                    y24_blocks = yes24_info.get("ì €ìì†Œê°œ_ë¦¬ìŠ¤íŠ¸") or []
                                    kb_blocks = kyobo_info.get("ì €ìì†Œê°œ_ë¦¬ìŠ¤íŠ¸") or []
                                    author_blocks.extend(y24_blocks)
                                    author_blocks.extend(kb_blocks)

                                    # âœ… 3-c) ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ë©´ í•©ë³¸ í…ìŠ¤íŠ¸ë¡œë¼ë„ ê·¸ë£¹ ìƒì„± (fallback)
                                    if not author_blocks:
                                        combined_bio = combine_author_bios(
                                            author_intro_y24, author_intro_kb
                                        )
                                        author_blocks = (
                                            [combined_bio] if combined_bio else []
                                        )

                                    # ì €ìëª… íŒŒì‹±
                                    author_names = []
                                    if author and isinstance(author, str):
                                        parts = [
                                            p.strip()
                                            for p in re.split(r"[|,ï¼/]", author)
                                            if p.strip()
                                        ]
                                        if parts:
                                            author_names = parts

                                    groups = extract_other_works_grouped(
                                        author_blocks, title
                                    )
                                    pattern_text = render_other_works_grouped(
                                        groups, author_names or None
                                    )

                                    # í‰íƒ„í™” ë¦¬ìŠ¤íŠ¸ (AI-Feed Mergeìš©)
                                    other_works_flat = []
                                    for g in groups:
                                        for w in g["works"]:
                                            other_works_flat.append(
                                                f"{w['title']}({w['orig']})"
                                                if w["orig"]
                                                else w["title"]
                                            )

                                    # 4ë²ˆì§¸ í–‰ â€” AI-Feed Merge
                                    merged_record = {
                                        "ê²€ìƒ‰ì†ŒìŠ¤": "AI-Feed Merge",
                                        "ì„œëª…": title,
                                        "ì €ì": author,
                                        "ë¶„ë¥˜ ì •ë³´ ì·¨í•©": "\n\n".join(
                                            merged_parts
                                        ).strip(),
                                        "ì €ìì†Œê°œ": merged_author,
                                        "ëª©ì°¨": merged_toc,
                                        "ì„œí‰": merged_review,
                                        "ISBN": isbn,
                                        "ì¶œíŒì‚¬": publisher,
                                        "ì¶œê°„ì¼": pubdate,
                                        "ê°€ê²©": price,
                                        "ë§í¬": yes24_info.get("ìƒí’ˆë§í¬")
                                        or kyobo_info.get("ìƒí’ˆë§í¬")
                                        or link,
                                    }
                                    results.append(merged_record)

                                    # 5ë²ˆì§¸ í–‰ â€” OtherWorks Merge (ì €ìë³„ íŒ¨í„´)
                                    if groups:
                                        otherworks_record = {
                                            "ê²€ìƒ‰ì†ŒìŠ¤": "OtherWorks Merge",
                                            "ì„œëª…": title,
                                            "ì €ì": author,
                                            "ë¶„ë¥˜ ì •ë³´ ì·¨í•©": pattern_text,  # ì €ìA/ì‘í’ˆ/ì›ì„œëª… í˜•ì‹
                                            "ì €ìì†Œê°œ": "",
                                            "ëª©ì°¨": "",
                                            "ì„œí‰": "",
                                            "ISBN": isbn,
                                            "ì¶œíŒì‚¬": publisher,
                                            "ì¶œê°„ì¼": pubdate,
                                            "ê°€ê²©": price,
                                            "ë§í¬": yes24_info.get("ìƒí’ˆë§í¬")
                                            or kyobo_info.get("ìƒí’ˆë§í¬")
                                            or link,
                                        }
                                        results.append(otherworks_record)

                            if app_instance:
                                app_instance.log_message(
                                    f"ì •ë³´: ë„¤ì´ë²„ API ê²°ê³¼ - ì„œëª…: {title[:50]}{'...' if len(title) > 50 else ''}"
                                )

                    else:
                        # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ
                        if app_instance:
                            app_instance.log_message(
                                "ì •ë³´: ë„¤ì´ë²„ API ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
                            )

                        results = [
                            {
                                "ì„œëª…": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ",
                                "ì €ì": "",
                                "ì¶œíŒì‚¬": "",
                                "ì¶œê°„ì¼": "",
                                "ISBN": primary_query if isbn_query else "",
                                "ê°€ê²©": "",
                                "ì„œí‰": f"'{primary_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                                "ë¶„ë¥˜ ì •ë³´ ì·¨í•©": "",
                                "ì €ìì†Œê°œ": "",
                                "ëª©ì°¨": "",
                                "ê²€ìƒ‰ì†ŒìŠ¤": search_type,
                                "ë§í¬": "",
                            }
                        ]
                else:
                    # XML êµ¬ì¡° ì˜¤ë¥˜
                    error_msg = "ë„¤ì´ë²„ API ì‘ë‹µì—ì„œ channel íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    if app_instance:
                        app_instance.log_message(f"ì˜¤ë¥˜: {error_msg}", level="ERROR")

                    results = [
                        {
                            "ì„œëª…": "API ì‘ë‹µ ì˜¤ë¥˜",
                            "ì €ì": "",
                            "ì¶œíŒì‚¬": "",
                            "ì¶œê°„ì¼": "",
                            "ISBN": primary_query if isbn_query else "",
                            "ê°€ê²©": "",
                            "ì„œí‰": error_msg,
                            "ë¶„ë¥˜ ì •ë³´ ì·¨í•©": "",
                            "ì €ìì†Œê°œ": "",
                            "ëª©ì°¨": "",
                            "ê²€ìƒ‰ì†ŒìŠ¤": search_type,
                            "ë§í¬": "",
                        }
                    ]

            except ET.ParseError as e:
                # XML íŒŒì‹± ì˜¤ë¥˜
                error_msg = f"XML íŒŒì‹± ì‹¤íŒ¨: {str(e)}"
                if app_instance:
                    app_instance.log_message(
                        f"ì˜¤ë¥˜: ë„¤ì´ë²„ API ì‘ë‹µ XML íŒŒì‹± ì‹¤íŒ¨: {e}", level="ERROR"
                    )

                results = [
                    {
                        "ì„œëª…": "XML íŒŒì‹± ì˜¤ë¥˜",
                        "ì €ì": "",
                        "ì¶œíŒì‚¬": "",
                        "ì¶œê°„ì¼": "",
                        "ISBN": primary_query if isbn_query else "",
                        "ê°€ê²©": "",
                        "ì„œí‰": error_msg,
                        "ë¶„ë¥˜ ì •ë³´ ì·¨í•©": "",
                        "ì €ìì†Œê°œ": "",
                        "ëª©ì°¨": "",
                        "ê²€ìƒ‰ì†ŒìŠ¤": search_type,
                        "ë§í¬": "",
                    }
                ]

        else:
            # API ì˜¤ë¥˜ ì²˜ë¦¬ (GAS ì½”ë“œì™€ ë™ì¼)
            error_msg = f"API ì˜¤ë¥˜ ({response_code}): {response_text[:100]}..."
            if app_instance:
                app_instance.log_message(
                    f"ì˜¤ë¥˜: ë„¤ì´ë²„ API í˜¸ì¶œ ì˜¤ë¥˜: ì‘ë‹µ ì½”ë“œ {response_code}, ì‘ë‹µ í…ìŠ¤íŠ¸: {response_text[:100]}...",
                    level="ERROR",
                )

            results = [
                {
                    "ì„œëª…": "API í˜¸ì¶œ ì˜¤ë¥˜",
                    "ì €ì": "",
                    "ì¶œíŒì‚¬": "",
                    "ì¶œê°„ì¼": "",
                    "ISBN": primary_query if isbn_query else "",
                    "ê°€ê²©": "",
                    "ì„œí‰": error_msg,
                    "ë¶„ë¥˜ ì •ë³´ ì·¨í•©": "",
                    "ì €ìì†Œê°œ": "",
                    "ëª©ì°¨": "",
                    "ê²€ìƒ‰ì†ŒìŠ¤": search_type,
                    "ë§í¬": "",
                }
            ]

    except requests.exceptions.RequestException as e:
        # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì²˜ë¦¬
        error_msg = f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)}"
        if app_instance:
            app_instance.log_message(
                f"ì˜¤ë¥˜: ë„¤ì´ë²„ API í˜¸ì¶œ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}", level="ERROR"
            )

        results = [
            {
                "ì„œëª…": "ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜",
                "ì €ì": "",
                "ì¶œíŒì‚¬": "",
                "ì¶œê°„ì¼": "",
                "ISBN": primary_query if isbn_query else "",
                "ê°€ê²©": "",
                "ì„œí‰": error_msg,
                "ë¶„ë¥˜ ì •ë³´ ì·¨í•©": "",
                "ì €ìì†Œê°œ": "",
                "ëª©ì°¨": "",
                "ê²€ìƒ‰ì†ŒìŠ¤": search_type,
                "ë§í¬": "",
            }
        ]

    except Exception as e:
        # ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬ (GAS ì½”ë“œì™€ ë™ì¼)
        error_msg = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        if app_instance:
            app_instance.log_message(
                f"ì˜¤ë¥˜: ë„¤ì´ë²„ API í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}", level="ERROR"
            )

        results = [
            {
                "ì„œëª…": "ì˜ˆì™¸ ì˜¤ë¥˜",
                "ì €ì": "",
                "ì¶œíŒì‚¬": "",
                "ì¶œê°„ì¼": "",
                "ISBN": primary_query if isbn_query else "",
                "ê°€ê²©": "",
                "ì„œí‰": error_msg,
                "ë¶„ë¥˜ ì •ë³´ ì·¨í•©": "",
                "ì €ìì†Œê°œ": "",
                "ëª©ì°¨": "",
                "ê²€ìƒ‰ì†ŒìŠ¤": search_type,
                "ë§í¬": "",
            }
        ]

    if app_instance:
        app_instance.update_progress(100)
        app_instance.log_message(
            f"ì •ë³´: ë„¤ì´ë²„ ì±… API ê²€ìƒ‰ ì™„ë£Œ. ({len(results)}ê°œ ê²°ê³¼)"
        )

    return results


def set_naver_api_credentials(client_id, client_secret, db_manager):
    """
    ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸ IDì™€ ì‹œí¬ë¦¿ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    GAS setNaverApiCredentials() í•¨ìˆ˜ í¬íŒ…

    Args:
        client_id (str): ë„¤ì´ë²„ ê°œë°œì ì„¼í„°ì—ì„œ ë°œê¸‰ë°›ì€ í´ë¼ì´ì–¸íŠ¸ ID
        client_secret (str): ë„¤ì´ë²„ ê°œë°œì ì„¼í„°ì—ì„œ ë°œê¸‰ë°›ì€ í´ë¼ì´ì–¸íŠ¸ ì‹œí¬ë¦¿
        db_manager (DatabaseManager): ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤

    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    try:
        return db_manager.set_naver_api_credentials(client_id, client_secret)
    except Exception as e:
        print(f"ì˜¤ë¥˜: ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


def scrape_yes24_book_info(isbn, app_instance=None):
    """
    ì˜ˆìŠ¤24ì—ì„œ ISBNìœ¼ë¡œ ë„ì„œì˜ ì €ìì†Œê°œ, ëª©ì°¨, ì¶œíŒì‚¬ì„œí‰ì„ ìŠ¤í¬ë ˆì´í•‘í•©ë‹ˆë‹¤.
    ì‹¤ì œ HTML êµ¬ì¡° ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë¨.

    Args:
        isbn (str): ê²€ìƒ‰í•  ISBN
        app_instance: ë¡œê·¸ìš© ì•± ì¸í„´ìŠ¤

    Returns:
        dict: {"ì €ìì†Œê°œ": str, "ëª©ì°¨": str, "ì¶œíŒì‚¬ì„œí‰": str}
    """
    result = {"ì €ìì†Œê°œ": "", "ëª©ì°¨": "", "ì¶œíŒì‚¬ì„œí‰": "", "ìƒí’ˆë§í¬": ""}

    if not isbn:
        return result

    try:
        if app_instance:
            app_instance.log_message(
                f"ì •ë³´: ì˜ˆìŠ¤24ì—ì„œ ISBN {isbn} ì •ë³´ ìŠ¤í¬ë ˆì´í•‘ ì‹œì‘"
            )

        search_url = f"https://www.yes24.com/product/search?domain=BOOK&query={isbn}"

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        }

        session = requests.Session()
        session.headers.update(headers)

        try:
            session.get("https://www.yes24.com/", timeout=10)
            time.sleep(0.5)
            session.headers.update({"Referer": "https://www.yes24.com/"})
        except requests.exceptions.RequestException:
            if app_instance:
                app_instance.log_message(
                    "ê²½ê³ : ì˜ˆìŠ¤24 í™ˆí˜ì´ì§€ ë°©ë¬¸ ì‹¤íŒ¨ (ì¿ í‚¤ íšë“ ì‹¤íŒ¨)", level="WARNING"
                )

        search_response = session.get(search_url, timeout=15)
        search_response.raise_for_status()

        # -------------------
        # âœ… [ìˆ˜ì • 1] ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì˜ ì¸ì½”ë”©ì„ 'euc-kr'ë¡œ ëª…ì‹œí•˜ì—¬ íŒŒì‹± ì˜¤ë¥˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
        search_response.encoding = "utf-8"
        search_soup = BeautifulSoup(search_response.text, "html.parser")
        # -------------------

        product_link = None

        # -------------------
        # âœ… [ìˆ˜ì • 2] ë¶ˆí•„ìš”í•œ ë””ë²„ê¹… ì½”ë“œë¥¼ ì •ë¦¬í•˜ê³ , ê°€ì¥ ì •í™•í•œ 'data-goods-no' ì†ì„±ì„ ì§ì ‘ ì°¾ì•„ ë§í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        goods_item = search_soup.find(attrs={"data-goods-no": True})
        if goods_item and goods_item.get("data-goods-no"):
            goods_no = goods_item.get("data-goods-no").strip()
            if goods_no:
                product_link = f"https://www.yes24.com/product/goods/{goods_no}"
                # -------------------
                # âœ… [ìˆ˜ì •] ì°¾ì€ ìƒí’ˆ ë§í¬ë¥¼ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
                result["ìƒí’ˆë§í¬"] = product_link
                # -------------------
                if app_instance:
                    app_instance.log_message(
                        f"ì •ë³´: ì˜ˆìŠ¤24 ìƒí’ˆ ë§í¬ ë°œê²¬ (data-goods-no): {product_link}"
                    )
        # -------------------

        if not product_link:
            if app_instance:
                app_instance.log_message(
                    "ê²½ê³ : ì˜ˆìŠ¤24ì—ì„œ ìƒí’ˆ ë§í¬(data-goods-no)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            return result

        time.sleep(1)

        detail_response = session.get(product_link, timeout=15)
        detail_response.raise_for_status()

        # -------------------
        # âœ… [ìˆ˜ì • 3] ìƒì„¸ í˜ì´ì§€ ì—­ì‹œ 'euc-kr' ì¸ì½”ë”©ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•´ì•¼ ë¬¸ìê°€ ê¹¨ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤. (ê°€ì¥ í•µì‹¬ì ì¸ ìˆ˜ì •)
        detail_response.encoding = "utf-8"
        detail_soup = BeautifulSoup(detail_response.text, "html.parser")
        # -------------------

        # ì—¬ëŸ¬ ê¸°ì—¬ì(ì§€ì€ì´/ì˜®ê¸´ì´)ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª¨ë‘ ìˆ˜ì§‘
        author_spans = detail_soup.find_all("span", class_="author_info info_origin")
        author_chunks = []
        for sp in author_spans:
            txt = sp.get_text(separator="\n", strip=True)
            if txt and len(txt) > 30:
                author_chunks.append(txt)
        if author_chunks:
            result["ì €ìì†Œê°œ"] = "\n\n---\n\n".join(author_chunks)  # êµ¬ë¶„ì„ ìœ¼ë¡œ ì—°ê²°
            result["ì €ìì†Œê°œ_ë¦¬ìŠ¤íŠ¸"] = author_chunks  # âœ… ì¶”ê°€: ì €ìë³„ ë¸”ë¡ ê·¸ëŒ€ë¡œ

        toc_section = detail_soup.find("div", id="infoset_toc")
        if toc_section:
            toc_textarea = toc_section.find("textarea", class_="txtContentText")
            if toc_textarea:
                toc_html = toc_textarea.get_text()
                toc_soup = BeautifulSoup(toc_html, "html.parser")
                toc_text = toc_soup.get_text(separator="\n", strip=True)
                if len(toc_text) > 20:
                    result["ëª©ì°¨"] = toc_text

        # -------------------
        # âœ… [ìˆ˜ì • 4] 'ì¶œíŒì‚¬ ë¦¬ë·°' ë˜ëŠ” 'ì¶œíŒì‚¬ ì„œí‰' ë“± ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ì— ëŒ€ì‘í•˜ë„ë¡ ì •ê·œì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        publisher_review_heading = detail_soup.find(
            "h4", class_="tit_txt", string=re.compile(r"ì¶œíŒì‚¬\s*(ë¦¬ë·°|ì„œí‰)")
        )
        # -------------------
        if publisher_review_heading:
            parent_section = publisher_review_heading.find_parent("div")
            if parent_section:
                next_div = parent_section.find_next_sibling(
                    "div", class_="infoSetCont_wrap"
                )
                if next_div:
                    review_textarea = next_div.find("textarea", class_="txtContentText")
                    if review_textarea:
                        review_html = review_textarea.get_text()
                        review_soup = BeautifulSoup(review_html, "html.parser")
                        review_text = review_soup.get_text(separator="\n", strip=True)
                        cleaned_review = (
                            review_text.replace("ì¶œíŒì‚¬ ë¦¬ë·°", "")
                            .replace("ì¶œíŒì‚¬ ì„œí‰", "")
                            .strip()
                        )
                        if len(cleaned_review) > 20:
                            result["ì¶œíŒì‚¬ì„œí‰"] = cleaned_review

        if app_instance:
            found_info = []
            if result["ì €ìì†Œê°œ"]:
                found_info.append("ì €ìì†Œê°œ")
            if result["ëª©ì°¨"]:
                found_info.append("ëª©ì°¨")
            if result["ì¶œíŒì‚¬ì„œí‰"]:
                found_info.append("ì¶œíŒì‚¬ì„œí‰")
            if found_info:
                app_instance.log_message(
                    f"ì •ë³´: ì˜ˆìŠ¤24ì—ì„œ ì¶”ì¶œ ì™„ë£Œ: {', '.join(found_info)}"
                )
            else:
                app_instance.log_message(
                    f"ì •ë³´: ì˜ˆìŠ¤24ì—ì„œ ì¶”ê°€ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤"
                )

    except requests.exceptions.RequestException as e:
        if app_instance:
            app_instance.log_message(
                f"ì˜¤ë¥˜: ì˜ˆìŠ¤24 ìŠ¤í¬ë ˆì´í•‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}", level="ERROR"
            )
    except Exception as e:
        if app_instance:
            app_instance.log_message(
                f"ì˜¤ë¥˜: ì˜ˆìŠ¤24 ìŠ¤í¬ë ˆì´í•‘ ì‹¤íŒ¨: {e}", level="ERROR"
            )

    return result


def scrape_kyobo_book_info(isbn, app_instance=None):
    """
    êµë³´ë¬¸ê³ ì—ì„œ ISBNìœ¼ë¡œ ë„ì„œì˜ ì €ìì†Œê°œ, ëª©ì°¨, ì¶œíŒì‚¬ì„œí‰ì„ ìŠ¤í¬ë ˆì´í•‘í•©ë‹ˆë‹¤.
    ì‹¤ì œ HTML êµ¬ì¡° ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë¨.

    Args:
        isbn (str): ê²€ìƒ‰í•  ISBN
        app_instance: ë¡œê·¸ìš© ì•± ì¸ìŠ¤í„´ìŠ¤

    Returns:
        dict: {"ì €ìì†Œê°œ": str, "ëª©ì°¨": str, "ì¶œíŒì‚¬ì„œí‰": str}
    """
    result = {"ì €ìì†Œê°œ": "", "ëª©ì°¨": "", "ì¶œíŒì‚¬ì„œí‰": "", "ìƒí’ˆë§í¬": ""}

    if not isbn:
        return result

    try:
        if app_instance:
            app_instance.log_message(
                f"ì •ë³´: êµë³´ë¬¸ê³ ì—ì„œ ISBN {isbn} ì •ë³´ ìŠ¤í¬ë ˆì´í•‘ ì‹œì‘"
            )

        # êµë³´ë¬¸ê³  ê²€ìƒ‰ URL (ISBNìœ¼ë¡œ ê²€ìƒ‰)
        search_url = f"https://search.kyobobook.co.kr/search?keyword={isbn}&gbCode=TOT&target=total"

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }

        # ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ìƒí’ˆ ë§í¬ ì°¾ê¸°
        search_response = requests.get(search_url, headers=headers, timeout=10)
        search_response.raise_for_status()

        search_soup = BeautifulSoup(search_response.content, "html.parser")

        # âœ… êµë³´ë¬¸ê³  ì‹¤ì œ êµ¬ì¡°: data-pid="S000217279197" data-bid="9788901296883"
        product_link = None

        # 1ì°¨ ì‹œë„: ISBNìœ¼ë¡œ ì •í™•í•œ ë§¤ì¹­
        checkbox_input = search_soup.find(
            "input", {"class": "result_checkbox", "data-pid": True, "data-bid": isbn}
        )

        if checkbox_input:
            product_id = checkbox_input.get("data-pid")
            if product_id:
                product_link = f"https://product.kyobobook.co.kr/detail/{product_id}"
                # -------------------
                # âœ… [ìˆ˜ì •] ì°¾ì€ ìƒí’ˆ ë§í¬ë¥¼ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
                result["ìƒí’ˆë§í¬"] = product_link
                # -------------------
                if app_instance:
                    app_instance.log_message(
                        f"ì •ë³´: êµë³´ë¬¸ê³  ìƒí’ˆ í˜ì´ì§€ ë°œê²¬: {product_link}"
                    )

        # 2ì°¨ ì‹œë„: data-pidë§Œìœ¼ë¡œ ì°¾ê¸° (ë°±ì—…)
        if not product_link:
            checkbox_input = search_soup.find("input", {"data-pid": True})
            if checkbox_input:
                product_id = checkbox_input.get("data-pid")
                if product_id:
                    product_link = (
                        f"https://product.kyobobook.co.kr/detail/{product_id}"
                    )

        if not product_link:
            if app_instance:
                app_instance.log_message(
                    "ê²½ê³ : êµë³´ë¬¸ê³ ì—ì„œ ìƒí’ˆ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            return result

        # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
        time.sleep(1)

        # ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼
        detail_response = requests.get(product_link, headers=headers, timeout=10)
        detail_response.raise_for_status()

        detail_soup = BeautifulSoup(detail_response.content, "html.parser")

        # âœ… ì €ìì†Œê°œ ì¶”ì¶œ: `writer_info_box`ë¥¼ ë¨¼ì € ì°¾ê³ , ê·¸ ì•ˆì—ì„œ `info_text` í´ë˜ìŠ¤ì˜ `p` íƒœê·¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        # -------------------
        # [ìˆ˜ì •] .find()ëŠ” ì²« ë²ˆì§¸ ì €ì(ì•¨ë¦¬ìŠ¨ ìš°ë“œ ë¸Œë£©ìŠ¤)ë§Œ ì°¾ìŠµë‹ˆë‹¤.
        #       .find_all()ë¡œ ë³€ê²½í•˜ì—¬ ì €ì, ë²ˆì—­ì ë“± ëª¨ë“  'writer_info_box'ë¥¼ ìˆœíšŒí•´ì•¼ í•©ë‹ˆë‹¤.
        author_boxes = detail_soup.find_all("div", class_="writer_info_box")
        author_chunks = []  # ëª¨ë“  ì €ì/ë²ˆì—­ìì˜ ì†Œê°œ ë¸”ë¡ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

        if author_boxes:
            for box in author_boxes:  # ğŸ‘ˆ [ìˆ˜ì •] ë°œê²¬ëœ ëª¨ë“  ë°•ìŠ¤ë¥¼ ìˆœíšŒ
                # writer_info_box ì•ˆì— ì—¬ëŸ¬ ëª…ì˜ info_text ë‹¨ë½ì´ ì¡´ì¬ ê°€ëŠ¥
                author_ps = box.find_all("p", class_="info_text")
                for p in author_ps:  # ğŸ‘ˆ [ìˆ˜ì •] ê° ë°•ìŠ¤ ì•ˆì˜ p íƒœê·¸ ìˆœíšŒ
                    t = p.get_text(separator="\n", strip=True)
                    if t and len(t) > 30:
                        author_chunks.append(t)  # ğŸ‘ˆ [ìˆ˜ì •] ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ì— ëª¨ë‘ ì¶”ê°€

            if author_chunks:
                result["ì €ìì†Œê°œ"] = "\n\n---\n\n".join(author_chunks)
                result["ì €ìì†Œê°œ_ë¦¬ìŠ¤íŠ¸"] = author_chunks  # âœ… ìˆ˜ì •: ì €ì/ë²ˆì—­ì ë¸”ë¡ì´ ëª¨ë‘ í¬í•¨ë¨
        # -------------------

        # âœ… ëª©ì°¨ ì¶”ì¶œ: <h2 class="title_heading">ëª©ì°¨</h2> â†’ <ul class="book_contents_list">
        toc_heading = detail_soup.find("h2", class_="title_heading", string="ëª©ì°¨")
        if toc_heading:
            # ëª©ì°¨ í—¤ë”© ë‹¤ìŒì˜ auto_overflow_wrap ì°¾ê¸°
            toc_container = toc_heading.find_next("div", class_="auto_overflow_wrap")
            if toc_container:
                # book_contents_list ì°¾ê¸°
                contents_list = toc_container.find("ul", class_="book_contents_list")
                if contents_list:
                    # ëª¨ë“  ëª©ì°¨ í•­ëª© ì¶”ì¶œ
                    content_items = contents_list.find_all(
                        "li", class_="book_contents_item"
                    )
                    toc_texts = []
                    for item in content_items:
                        item_text = item.get_text(separator="\n", strip=True)
                        if item_text:
                            toc_texts.append(item_text)

                    if toc_texts:
                        toc_content = "\n".join(toc_texts)
                        if len(toc_content) > 20:
                            result["ëª©ì°¨"] = toc_content

        # âœ… ì¶œíŒì‚¬ì„œí‰ ì¶”ì¶œ: <h2 class="title_heading">ì¶œíŒì‚¬ ì„œí‰</h2> â†’ <p class="info_text">
        publisher_review_heading = detail_soup.find(
            "h2", class_="title_heading", string="ì¶œíŒì‚¬ ì„œí‰"
        )
        if publisher_review_heading:
            # ì¶œíŒì‚¬ ì„œí‰ í—¤ë”© ë‹¤ìŒì˜ auto_overflow_wrap ì°¾ê¸°
            review_container = publisher_review_heading.find_next(
                "div", class_="auto_overflow_wrap"
            )
            if review_container:
                # info_text ë‹¨ë½ë“¤ ì¶”ì¶œ
                review_paragraphs = review_container.find_all("p", class_="info_text")
                review_texts = []
                for p in review_paragraphs:
                    p_text = p.get_text(separator="\n", strip=True)
                    if p_text:
                        review_texts.append(p_text)

                if review_texts:
                    review_content = "\n".join(review_texts)
                    cleaned_review = (
                        review_content.replace("ì¶œíŒì‚¬ ì„œí‰", "")
                        .replace("ì¶œíŒì‚¬ ë¦¬ë·°", "")
                        .strip()
                    )
                    if len(cleaned_review) > 20:
                        result["ì¶œíŒì‚¬ì„œí‰"] = cleaned_review

        if app_instance:
            found_info = []
            if result["ì €ìì†Œê°œ"]:
                found_info.append("ì €ìì†Œê°œ")
            if result["ëª©ì°¨"]:
                found_info.append("ëª©ì°¨")
            if result["ì¶œíŒì‚¬ì„œí‰"]:
                found_info.append("ì¶œíŒì‚¬ì„œí‰")
            if found_info:
                app_instance.log_message(
                    f"ì •ë³´: êµë³´ë¬¸ê³ ì—ì„œ ì¶”ì¶œ ì™„ë£Œ: {', '.join(found_info)}"
                )
            else:
                app_instance.log_message(
                    f"ì •ë³´: êµë³´ë¬¸ê³ ì—ì„œ ì¶”ê°€ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤"
                )

    except requests.exceptions.RequestException as e:
        if app_instance:
            app_instance.log_message(
                f"ì˜¤ë¥˜: êµë³´ë¬¸ê³  ìŠ¤í¬ë ˆì´í•‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}", level="ERROR"
            )
    except Exception as e:
        if app_instance:
            app_instance.log_message(
                f"ì˜¤ë¥˜: êµë³´ë¬¸ê³  ìŠ¤í¬ë ˆì´í•‘ ì‹¤íŒ¨: {e}", level="ERROR"
            )

    return result


def get_additional_book_info(isbn, app_instance=None):
    """
    ì˜ˆìŠ¤24ì™€ êµë³´ë¬¸ê³ ì—ì„œ ì¶”ê°€ ë„ì„œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

    Args:
        isbn (str): ê²€ìƒ‰í•  ISBN
        app_instance: ë¡œê·¸ìš© ì•± ì¸ìŠ¤í„´ìŠ¤

    Returns:
        dict: {"ì €ìì†Œê°œ": str, "ëª©ì°¨": str, "ì¶œíŒì‚¬ì„œí‰": str}
    """
    result = {"ì €ìì†Œê°œ": "", "ëª©ì°¨": "", "ì¶œíŒì‚¬ì„œí‰": ""}

    if not isbn:
        return result

    # ì˜ˆìŠ¤24ì—ì„œ ë¨¼ì € ì‹œë„
    yes24_info = scrape_yes24_book_info(isbn, app_instance)

    # êµë³´ë¬¸ê³ ì—ì„œ ì‹œë„ (ì˜ˆìŠ¤24ì—ì„œ ëª» ì°¾ì€ ì •ë³´ê°€ ìˆì„ ê²½ìš°)
    need_kyobo = (
        not yes24_info["ì €ìì†Œê°œ"]
        or not yes24_info["ëª©ì°¨"]
        or not yes24_info["ì¶œíŒì‚¬ì„œí‰"]
    )

    if need_kyobo:
        kyobo_info = scrape_kyobo_book_info(isbn, app_instance)

        # ì •ë³´ ë³‘í•© (ì˜ˆìŠ¤24 ìš°ì„ , ì—†ìœ¼ë©´ êµë³´ë¬¸ê³ )
        if not yes24_info["ì €ìì†Œê°œ"] and kyobo_info["ì €ìì†Œê°œ"]:
            result["ì €ìì†Œê°œ"] = kyobo_info["ì €ìì†Œê°œ"]
        else:
            result["ì €ìì†Œê°œ"] = yes24_info["ì €ìì†Œê°œ"]

        if not yes24_info["ëª©ì°¨"] and kyobo_info["ëª©ì°¨"]:
            result["ëª©ì°¨"] = kyobo_info["ëª©ì°¨"]
        else:
            result["ëª©ì°¨"] = yes24_info["ëª©ì°¨"]

        if not yes24_info["ì¶œíŒì‚¬ì„œí‰"] and kyobo_info["ì¶œíŒì‚¬ì„œí‰"]:
            result["ì¶œíŒì‚¬ì„œí‰"] = kyobo_info["ì¶œíŒì‚¬ì„œí‰"]
        else:
            result["ì¶œíŒì‚¬ì„œí‰"] = yes24_info["ì¶œíŒì‚¬ì„œí‰"]
    else:
        result = yes24_info

    return result


# í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
    print("Search_Naver.py í…ŒìŠ¤íŠ¸ ì‹¤í–‰")

    # í…ŒìŠ¤íŠ¸ ISBN
    test_isbn = "9788960773417"

    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ API í‚¤ ì—†ì´)
    print(f"í…ŒìŠ¤íŠ¸ ISBN: {test_isbn}")

    # DatabaseManager ì—†ì´ í…ŒìŠ¤íŠ¸
    results = search_naver_catalog(test_isbn, app_instance=None, db_manager=None)
    print("í…ŒìŠ¤íŠ¸ ê²°ê³¼:", results)
