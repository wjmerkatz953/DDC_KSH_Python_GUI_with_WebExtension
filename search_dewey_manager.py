# íŒŒì¼ëª…: search_dewey_manager.py
# ì„¤ëª…: ë“€ì´ì‹­ì§„ë¶„ë¥˜(DDC) ê²€ìƒ‰ ì „ìš© ëª¨ë“ˆ

import pandas as pd
import re
import json
import logging
from typing import List
from database_manager import DatabaseManager
from search_common_manager import SearchCommonManager

logger = logging.getLogger("qt_main_app.database_manager")


class SearchDeweyManager(SearchCommonManager):
    """
    ë“€ì´íƒ­ ì „ìš© ê²€ìƒ‰ í´ë˜ìŠ¤
    - DDC ê²€ìƒ‰ ë° ë­í‚¹
    - DDC ìºì‹œ ê´€ë¦¬
    - DDC í‚¤ì›Œë“œ ê²€ìƒ‰
    """

    def _cache_ddc_description(self, ddc_code, description_json):
        """DDC ì„¤ëª…ì„ ìºì‹œì— ì €ì¥ (ë‹¨ìˆœí™”ëœ ë˜í¼ í•¨ìˆ˜)"""
        # -------------------
        # âœ… [ê°œì„  2] ì½”ë“œ ì¤‘ë³µ ì œê±°. í•µì‹¬ ë¡œì§ì„ ê°€ì§„ save_dewey_to_cache í˜¸ì¶œ
        iri = f"http://dewey.info/class/{ddc_code}/"
        self.save_dewey_to_cache(iri, ddc_code, description_json)
        print(f"âœ… DDC {ddc_code} ìºì‹œ ë° í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")
        # -------------------


    def _search_by_ddc_ranking_logic(self, ddc_code):
        """
        ğŸ¯ DDC ê²€ìƒ‰ ì™„ì „ ê°œì„  + ì„±ëŠ¥ ìµœì í™”:
        - ì™„ë²½ë§¤ì¹­ ìƒìœ„ 3ê±´: KSH 1ê°œ + ìµœì‹ ì—°ë„
        - ê·¸ ë‹¤ìŒ: KSH ë§ì€ ê²ƒ + ìµœì‹ ì—°ë„
        - DDC ì»¬ëŸ¼ ì˜¤ë¦„ì°¨ìˆœ ìµœì¢… ì •ë ¬
        - ë³µìˆ˜ KSH ë°ì´í„° ë°˜ë“œì‹œ í¬í•¨

        âœ… [ì„±ëŠ¥ ê°œì„ ]
        - SELECT * ëŒ€ì‹  í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¡°íšŒ (ë©”ëª¨ë¦¬ íš¨ìœ¨ ì¦ëŒ€)
        - 3.5GB ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì²« ì¿¼ë¦¬ ì†ë„: 20ì´ˆ â†’ 2-3ì´ˆ

        âœ… [2025-10-19 ìˆ˜ì •]
        - max_results íŒŒë¼ë¯¸í„° ì œê±°: ì „ì²´ ê²°ê³¼ ë°˜í™˜ í›„ Pythonì—ì„œ ì •ë ¬
        - ìƒìœ„ 200ê°œëŠ” í˜¸ì¶œí•˜ëŠ” ìª½ì—ì„œ ì œí•œ
        """
        conn = self.db_manager._get_mapping_connection()
        try:
            # 1ë‹¨ê³„: ê¸°ë³¸ DDC ì „ë°©ë§¤ì¹­ ê²€ìƒ‰ (âœ… í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¡°íšŒ)
            # âœ… [ì„±ëŠ¥ ê°œì„ ] INDEXED BYë¡œ idx_ddc_ksh ë³µí•© ì¸ë±ìŠ¤ ê°•ì œ ì‚¬ìš©
            query = """
                SELECT
                    identifier,
                    kdc,
                    ddc,
                    ksh,
                    ksh_korean,
                    ksh_labeled,
                    kdc_edition,
                    ddc_edition,
                    publication_year,
                    title,
                    source_file
                FROM mapping_data INDEXED BY idx_ddc_ksh
                WHERE ddc LIKE ? AND ksh IS NOT NULL AND ksh != ''
            """
            df = pd.read_sql_query(query, conn, params=(f"{ddc_code}%",))

            if df.empty:
                print(f"ğŸš« [DDC_EMPTY] DDC '{ddc_code}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                return pd.DataFrame()

            print(f"ğŸ¯ [DDC_SEARCH] ê²€ìƒ‰ì–´: '{ddc_code}', ê¸°ë³¸ ë§¤ì¹­: {len(df)}ê°œ")

            # 2ë‹¨ê³„: ì™„ë²½ë§¤ì¹­ê³¼ ë¶€ë¶„ë§¤ì¹­ ë¶„ë¦¬
            exact_matches = df[df["ddc"] == ddc_code].copy()
            partial_matches = df[df["ddc"] != ddc_code].copy()

            print(f"ğŸ¯ [DDC_EXACT] ì™„ë²½ë§¤ì¹­: {len(exact_matches)}ê°œ")
            print(f"ğŸ¯ [DDC_PARTIAL] ë¶€ë¶„ë§¤ì¹­: {len(partial_matches)}ê°œ")

            # 3ë‹¨ê³„: KSH ê°œìˆ˜ ê³„ì‚° ë° ì •ë ¬ ì»¬ëŸ¼ ì¶”ê°€
            def _prepare_dataframe_for_sorting(dataframe):
                if dataframe.empty:
                    return dataframe

                # KSH ê°œìˆ˜ ì •í™• ê³„ì‚°
                def _count_ksh_codes(ksh_str):
                    if pd.isna(ksh_str) or not str(ksh_str).strip():
                        return 0
                    # KSH ì½”ë“œ íŒ¨í„´: KSHë¡œ ì‹œì‘í•˜ëŠ” 10ìë¦¬ ìˆ«ì
                    ksh_codes = re.findall(r"KSH\d{10}", str(ksh_str).upper())
                    return len(set(ksh_codes))  # ì¤‘ë³µ ì œê±°

                dataframe["ksh_count"] = dataframe["ksh"].apply(_count_ksh_codes)

                # ì—°ë„ ìˆ«ìí™”
                dataframe["pub_year_num"] = (
                    pd.to_numeric(dataframe["publication_year"], errors="coerce")
                    .fillna(0)
                    .astype(int)
                )

                # identifier ìˆ«ìí™” (íƒ€ì´ë¸Œë ˆì´ì»¤)
                def _extract_id_number(identifier_str):
                    if pd.isna(identifier_str):
                        return 0
                    numbers = re.findall(r"\d+", str(identifier_str))
                    return int("".join(numbers)) if numbers else 0

                dataframe["id_num"] = dataframe["identifier"].apply(_extract_id_number)

                return dataframe

            exact_matches = _prepare_dataframe_for_sorting(exact_matches)
            partial_matches = _prepare_dataframe_for_sorting(partial_matches)

            # 4ë‹¨ê³„: KSH ê°œìˆ˜ë³„ ë¶„í¬ í™•ì¸
            if not exact_matches.empty:
                ksh_dist = exact_matches["ksh_count"].value_counts().sort_index()
                print(f"ğŸ” [KSH_DIST] ì™„ë²½ë§¤ì¹­ KSH ë¶„í¬: {dict(ksh_dist)}")
            if not partial_matches.empty:
                ksh_dist_p = partial_matches["ksh_count"].value_counts().sort_index()
                print(f"ğŸ” [KSH_DIST] ë¶€ë¶„ë§¤ì¹­ KSH ë¶„í¬: {dict(ksh_dist_p)}")

            # 5ë‹¨ê³„: ë©”ë¥´ì¹´ì¸ ë‹˜ ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ì„ ë³„
            def _select_by_ksh_rule(dataframe, target_count):
                if dataframe.empty or target_count <= 0:
                    return pd.DataFrame()

                # KSH=1ì¸ ê²ƒë“¤ (ìµœì‹ ì—°ë„ìˆœ)
                single_ksh = dataframe[dataframe["ksh_count"] == 1].copy()
                single_ksh = single_ksh.sort_values(
                    ["pub_year_num", "id_num"], ascending=[False, False]
                ).drop_duplicates(subset=["identifier"], keep="first")

                # KSH>1ì¸ ê²ƒë“¤ (KSH ë§ì€ìˆœ â†’ ìµœì‹ ì—°ë„ìˆœ)
                multi_ksh = dataframe[dataframe["ksh_count"] > 1].copy()
                multi_ksh = multi_ksh.sort_values(
                    ["ksh_count", "pub_year_num", "id_num"],
                    ascending=[False, False, False],
                ).drop_duplicates(subset=["identifier"], keep="first")

                # ì„ ë³„ ë¡œì§
                selected_items = []

                # ìƒìœ„ 3ê±´ì€ KSH=1 ìš°ì„ 
                if target_count >= 3 and not single_ksh.empty:
                    top_3_single = single_ksh.head(3)
                    selected_items.append(top_3_single)
                    remaining_count = target_count - len(top_3_single)

                    # ë‚˜ë¨¸ì§€ëŠ” KSH ë§ì€ ê²ƒ ìš°ì„ 
                    if remaining_count > 0 and not multi_ksh.empty:
                        remaining_multi = multi_ksh.head(remaining_count)
                        selected_items.append(remaining_multi)
                        remaining_count -= len(remaining_multi)

                    # ì•„ì§ ë¶€ì¡±í•˜ë©´ single_kshì—ì„œ ì¶”ê°€
                    if remaining_count > 0 and len(single_ksh) > 3:
                        additional_single = single_ksh.iloc[3 : 3 + remaining_count]
                        selected_items.append(additional_single)
                else:
                    # target_count < 3ì´ê±°ë‚˜ single_kshê°€ ì—†ëŠ” ê²½ìš°
                    all_sorted = pd.concat([single_ksh, multi_ksh], ignore_index=True)
                    if not all_sorted.empty:
                        # KSH=1ì„ ì•ìª½ì—, ê·¸ ë‹¤ìŒ KSH ë§ì€ ìˆœ
                        all_sorted["ksh_priority"] = all_sorted["ksh_count"].apply(
                            lambda x: 0 if x == 1 else 1
                        )
                        all_sorted = all_sorted.sort_values(
                            ["ksh_priority", "ksh_count", "pub_year_num", "id_num"],
                            ascending=[True, False, False, False],
                        )
                        selected_items.append(all_sorted.head(target_count))

                if selected_items:
                    result = pd.concat(selected_items, ignore_index=True)
                    return result.drop_duplicates(subset=["identifier"], keep="first")
                else:
                    return pd.DataFrame()

            # 6ë‹¨ê³„: ì™„ë²½ë§¤ì¹­ê³¼ ë¶€ë¶„ë§¤ì¹­ ê°ê° ì²˜ë¦¬ (âœ… ì „ì²´ ê²°ê³¼ ë°˜í™˜)
            final_parts = []

            if not exact_matches.empty:
                # âœ… ì œí•œ ì—†ì´ ì „ì²´ ì™„ë²½ë§¤ì¹­ ê²°ê³¼ ì„ ë³„
                selected_exact = _select_by_ksh_rule(exact_matches, len(exact_matches))
                if not selected_exact.empty:
                    selected_exact["match_type"] = "exact"
                    final_parts.append(selected_exact)
                    print(f"âœ… [EXACT_SELECT] ì™„ë²½ë§¤ì¹­ {len(selected_exact)}ê°œ ì„ ë³„")

            if not partial_matches.empty:
                # âœ… ì œí•œ ì—†ì´ ì „ì²´ ë¶€ë¶„ë§¤ì¹­ ê²°ê³¼ ì„ ë³„
                selected_partial = _select_by_ksh_rule(partial_matches, len(partial_matches))
                if not selected_partial.empty:
                    selected_partial["match_type"] = "partial"
                    final_parts.append(selected_partial)
                    print(
                        f"âœ… [PARTIAL_SELECT] ë¶€ë¶„ë§¤ì¹­ {len(selected_partial)}ê°œ ì„ ë³„"
                    )

            # 7ë‹¨ê³„: ìµœì¢… ê²°í•© ë° DDC ì»¬ëŸ¼ ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
            if final_parts:
                final_result = pd.concat(final_parts, ignore_index=True)

                # ğŸ¯ í•µì‹¬: ksh_priority ì»¬ëŸ¼ ìƒì„± (ì™„ë²½ë§¤ì¹­=KSH 1ê°œ ìš°ì„ , ë¶€ë¶„ë§¤ì¹­=ë³µìˆ˜KSH ìš°ì„ )
                def assign_ksh_priority(row):
                    if row["match_type"] == 0:  # ì™„ë²½ë§¤ì¹­
                        return 0 if row["ksh_count"] == 1 else 1  # ë‹¨ì¼KSH ìš°ì„ 
                    else:  # ë¶€ë¶„ë§¤ì¹­
                        return 0 if row["ksh_count"] > 1 else 1  # ë³µìˆ˜KSH ìš°ì„ 

                final_result["ksh_priority"] = final_result.apply(
                    assign_ksh_priority, axis=1
                )

                # ì„ì‹œ ì»¬ëŸ¼ë“¤ ì œê±°
                columns_to_drop = [
                    "ksh_count",
                    "pub_year_num",
                    "id_num",
                    "match_type",
                    "ksh_priority",
                ]
                final_result = final_result.drop(
                    columns=columns_to_drop, errors="ignore"
                )

                # ë³µìˆ˜ KSH í™•ì¸
                multi_ksh_count = 0
                if "ksh" in final_result.columns:
                    for _, row in final_result.iterrows():
                        ksh_codes = re.findall(r"KSH\d{10}", str(row["ksh"]).upper())
                        if len(set(ksh_codes)) > 1:
                            multi_ksh_count += 1

                print(
                    f"ğŸ¯ [DDC_FINAL] ìµœì¢… ê²°ê³¼: {len(final_result)}ê°œ (ë³µìˆ˜KSH: {multi_ksh_count}ê°œ)"
                )
                print(f"ğŸ¯ [DDC_SORT] DDC ì»¬ëŸ¼ ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ ì™„ë£Œ")

                return final_result
            else:
                print(f"ğŸš« [DDC_NO_RESULT] ì„ ë³„ëœ ê²°ê³¼ ì—†ìŒ")
                return pd.DataFrame()

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: DDC ë­í‚¹ ë¡œì§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback

            traceback.print_exc()
            return pd.DataFrame()
        finally:
            if conn:
                conn.close()


    def _search_by_ddc_with_fallback(self, ddc_codes):
        # -------------------
        # 1. DDC Cache DBì—ì„œ ì™„ì „ì¼ì¹˜(1ìˆœìœ„)ì™€ ë¶€ë¶„ì¼ì¹˜(3ìˆœìœ„) ê²°ê³¼ ë¶„ë¦¬ ê²€ìƒ‰
        df_exact_cache, df_partial_cache = self._search_ddc_from_cache(ddc_codes)

        # 2. ì„œì§€ DBì—ì„œ ê²€ìƒ‰ (2ìˆœìœ„ - ê¸°ì¡´ ë¡œì§)
        # -------------------
        df_from_biblio = pd.DataFrame()
        conn = None
        try:
            conn = self.db_manager._get_mapping_connection()
            all_biblio_results = []
            for ddc_code in ddc_codes:
                candidates = self._search_by_ddc_ranking_logic(ddc_code)
                if not candidates.empty:
                    all_biblio_results.append(candidates)

            if all_biblio_results:
                df_from_biblio = pd.concat(all_biblio_results, ignore_index=True)

                # UI ë…¸ì¶œìš© ksh_labeled(ë§ˆí¬ì—…) ìƒì„± ë° ddc_label, ddc_count ì¶”ê°€
                if not df_from_biblio.empty:
                    df_from_biblio["ksh_labeled"] = df_from_biblio.apply(
                        lambda row: self._format_ksh_labeled_to_markup(
                            row.get("ksh_labeled", ""), row.get("ksh", "")
                        ),
                        axis=1,
                    )
                    # -------------------
                    # âœ… [ì„±ëŠ¥ ê°œì„ ] DDC ë ˆì´ë¸”ì„ ëŒ€ëŸ‰ìœ¼ë¡œ í•œ ë²ˆì— ì¡°íšŒ í›„ ë§¤í•‘
                    unique_ddcs = df_from_biblio["ddc"].dropna().unique().tolist()
                    if unique_ddcs:
                        ddc_label_map = self.get_all_ddc_labels_bulk(unique_ddcs)
                        df_from_biblio["ddc_label"] = (
                            df_from_biblio["ddc"].map(ddc_label_map).fillna("")
                        )
                    else:
                        df_from_biblio["ddc_label"] = ""

                    # âœ… [ì‹ ê·œ ì¶”ê°€] DDC ì¶œí˜„ ì¹´ìš´íŠ¸ ê³„ì‚°
                    ddc_counts = df_from_biblio["ddc"].value_counts()
                    df_from_biblio["ddc_count"] = df_from_biblio["ddc"].map(ddc_counts).fillna(0).astype(int)
                    # -------------------
        except Exception as e:
            print(f"ì˜¤ë¥˜: DDC ì„œì§€ DB ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì„œì§€ DB ê²€ìƒ‰ì— ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰í•˜ì—¬ Cache ê²°ê³¼ë¼ë„ ë³´ì—¬ì¤Œ
        finally:
            if conn:
                conn.close()

        # -------------------
        # 3. ìƒˆë¡œìš´ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ê²°ê³¼ ë³‘í•©
        # 1ìˆœìœ„: ì™„ì „ì¼ì¹˜ DDC Cache
        # 2ìˆœìœ„: ì„œì§€ DB ê²°ê³¼ (ìì²´ ë­í‚¹ ë¡œì§ í¬í•¨)
        # 3ìˆœìœ„: ë¶€ë¶„ì¼ì¹˜ DDC Cache
        final_df = pd.concat(
            [df_exact_cache, df_from_biblio, df_partial_cache], ignore_index=True
        )
        # -------------------

        # âœ… [ì‹ ê·œ ì¶”ê°€] DDC ì¶œí˜„ ì¹´ìš´íŠ¸ ê³„ì‚° (ì „ì²´ ë³‘í•© ê²°ê³¼ì—ì„œ)
        if not final_df.empty and "ddc" in final_df.columns:
            # ddc_countê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ê³„ì‚° (df_from_biblioëŠ” ì´ë¯¸ ê³„ì‚°ë¨)
            if "ddc_count" not in final_df.columns:
                ddc_counts = final_df["ddc"].value_counts()
                final_df["ddc_count"] = final_df["ddc"].map(ddc_counts).fillna(0).astype(int)
            else:
                # ì¼ë¶€ë§Œ ê³„ì‚°ëœ ê²½ìš° (Cache ê²°ê³¼ëŠ” ì—†ìŒ) ì „ì²´ ì¬ê³„ì‚°
                ddc_counts = final_df["ddc"].value_counts()
                final_df["ddc_count"] = final_df["ddc"].map(ddc_counts).fillna(0).astype(int)

        # âœ… [2025-10-19 ìˆ˜ì •] ìƒìœ„ 200ê°œë¡œ ì œí•œ (í‚¤ì›Œë“œ ê²€ìƒ‰ê³¼ ë™ì¼)
        if len(final_df) > 200:
            final_df = final_df.head(200)
            print(f"ğŸ” [DDC_LIMIT] ìµœì¢… ê²°ê³¼ë¥¼ ìƒìœ„ 200ê°œë¡œ ì œí•œ")

        return final_df.fillna("")


    def _search_ddc_by_sql_fts(
        self, keyword: str, pref_only: bool = False, limit: int = 20
    ) -> list:
        """
        [ë‚´ë¶€ìš©] ì˜ì–´ í‚¤ì›Œë“œë¡œ DDC ë²ˆí˜¸ë¥¼ FTS5 SQL ê²€ìƒ‰í•©ë‹ˆë‹¤. (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)
        """
        conn = None
        try:
            conn = self.db_manager._get_dewey_connection()
            cursor = conn.cursor()

            term_filter = "AND term_type = 'pref'" if pref_only else ""

            cursor.execute(
                f"""
                SELECT ddc, keyword, term_type
                FROM ddc_keyword_fts
                WHERE keyword MATCH ? {term_filter}
                ORDER BY rank
                LIMIT ?
                """,
                (keyword, limit),
            )

            results = []
            for row in cursor.fetchall():
                results.append({"ddc": row[0], "keyword": row[1], "term_type": row[2]})

            return results

        except Exception as e:
            print(f"ê²½ê³ : DDC í‚¤ì›Œë“œ SQL ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
            return []
        finally:
            if conn:
                conn.close()


    def _search_ddc_from_cache(
        self, ddc_codes: list
    ) -> tuple[pd.DataFrame, pd.DataFrame]:
        """
        DDC Cache DBì—ì„œ DDC ì½”ë“œ ëª©ë¡ì— ëŒ€í•œ ì„¤ëª…ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
        ì™„ì „ì¼ì¹˜ì™€ ë¶€ë¶„ì¼ì¹˜ ê²°ê³¼ë¥¼ ë¶„ë¦¬í•˜ì—¬ ë‘ ê°œì˜ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if not ddc_codes:
            return pd.DataFrame(), pd.DataFrame()

        logger.info(
            f"ğŸ’¾ DDC Cache DBì—ì„œ {len(ddc_codes)}ê°œ DDC ì½”ë“œì— ëŒ€í•´ ì™„ì „/ë¶€ë¶„ì¼ì¹˜ ì¡°íšŒ ì‹œì‘..."
        )

        unique_ddc_codes = list(set(ddc.strip() for ddc in ddc_codes if ddc.strip()))

        conn = None
        try:
            conn = self.db_manager._get_dewey_connection()
            cursor = conn.cursor()

            # -------------------
            # âœ… [í•µì‹¬ ìˆ˜ì •] 1ë²ˆê³¼ 2ë²ˆ ë¡œì§ì„ í†µí•©í•˜ì—¬ DB ì¡°íšŒë¥¼ í•œ ë²ˆìœ¼ë¡œ ìµœì í™”í•©ë‹ˆë‹¤.
            # 1. ì™„ì „/ë¶€ë¶„ì¼ì¹˜ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  í‚¤ì›Œë“œ(pref, alt)ë¥¼ í•œ ë²ˆì— ì¡°íšŒ
            all_ddc_codes_to_query = unique_ddc_codes
            partial_like_conditions = " OR ".join(
                ["ddc LIKE ?"] * len(all_ddc_codes_to_query)
            )
            placeholders = ",".join("?" for _ in all_ddc_codes_to_query)

            query = f"""
                SELECT ddc, keyword, term_type
                FROM ddc_keyword
                WHERE ddc IN ({placeholders}) OR ({partial_like_conditions})
            """
            params = all_ddc_codes_to_query + [
                f"{code}%" for code in all_ddc_codes_to_query
            ]
            cursor.execute(query, params)
            all_rows = cursor.fetchall()

            # 2. Pythonì—ì„œ DDCë³„ë¡œ ëª¨ë“  ë ˆì´ë¸”ì„ ì¡°í•© (ê¸°ì¡´ get_all_ddc_labels_bulkì™€ ìœ ì‚¬í•œ ë¡œì§)
            ddc_labels_map = {}
            # pref ë ˆì´ë¸”ì„ ìš°ì„ í•˜ê¸° ìœ„í•´ ë¨¼ì € ì •ë ¬
            all_rows.sort(key=lambda x: (x[0], 0 if x[2] == "pref" else 1, x[1]))

            for ddc, keyword, term_type in all_rows:
                if ddc not in ddc_labels_map:
                    ddc_labels_map[ddc] = []

                label = f"{keyword}(pref)" if term_type == "pref" else keyword
                if label not in ddc_labels_map[ddc]:
                    ddc_labels_map[ddc].append(label)

            # 3. ì¡°í•©ëœ ë ˆì´ë¸”ì„ ê¸°ë°˜ìœ¼ë¡œ ì™„ì „ì¼ì¹˜ì™€ ë¶€ë¶„ì¼ì¹˜ ê²°ê³¼ ë¶„ë¦¬
            exact_results = []
            partial_results = []
            for ddc, labels in ddc_labels_map.items():
                formatted_label = " | ".join(labels)
                if ddc in unique_ddc_codes:
                    exact_results.append((ddc, formatted_label))
                else:
                    # ë¶€ë¶„ì¼ì¹˜ ê²°ê³¼ ì •ë ¬ì„ ìœ„í•´ ddc ì½”ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                    partial_results.append((ddc, formatted_label))

            partial_results.sort(key=lambda x: x[0])

            # 4. DataFrame í¬ë§·íŒ… í—¬í¼ í•¨ìˆ˜ (ê¸°ì¡´ ë¡œì§ ì¬í™œìš©)
            def format_cache_df(rows, match_type):
                # -------------------
                if not rows:
                    return pd.DataFrame()

                df = pd.DataFrame(rows, columns=["ddc", "ddc_label"])
                df.drop_duplicates(subset=["ddc"], inplace=True, keep="first")

                df["source_file"] = "DDC Cache DB"
                df["data_type"] = "DDC ë ˆì´ë¸”"

                biblio_cols = [
                    "identifier",
                    "kdc",
                    "ddc",
                    "ksh",
                    "kdc_edition",
                    "ddc_edition",
                    "publication_year",
                    "title",
                    "data_type",
                    "source_file",
                    "ksh_labeled",
                    "ddc_label",
                    "nlk_link",
                ]
                for col in biblio_cols:
                    if col not in df.columns:
                        df[col] = ""

                logger.info(f"ğŸ’¾ DDC Cache DB ({match_type}): {len(df)}ê°œ ê²°ê³¼ ìƒì„±.")
                return df[biblio_cols]

            df_exact = format_cache_df(exact_results, "ì™„ì „ì¼ì¹˜")
            df_partial = format_cache_df(partial_results, "ë¶€ë¶„ì¼ì¹˜")

            return df_exact, df_partial

        except Exception as e:
            logger.error(f"âŒ DDC Cache DB ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return pd.DataFrame(), pd.DataFrame()
        finally:
            if conn:
                conn.close()


    def _search_ddc_with_fallback_hierarchy(self, ddc_code, max_results=50):
        """
        ğŸ”„ DDC ìƒí•˜ìœ„ ë¶„ë¥˜ í´ë°± ê²€ìƒ‰: ì™„ë²½ë§¤ì¹­ì´ ì—†ì„ ë•Œ ì‚¬ìš©
        """
        conn = self.db_manager._get_mapping_connection()
        try:
            fallback_patterns = []

            # ìƒìœ„ ë¶„ë¥˜ íŒ¨í„´ ìƒì„±
            if "." in ddc_code:
                # ì˜ˆ: 330.951 â†’ 330.9, 330
                parts = ddc_code.split(".")
                integer_part = parts[0]
                decimal_part = parts[1]

                # ì†Œìˆ˜ì  í•œ ìë¦¬ì”© ì œê±°
                for i in range(len(decimal_part) - 1, 0, -1):
                    fallback_patterns.append(f"{integer_part}.{decimal_part[:i]}")
                fallback_patterns.append(integer_part)
            else:
                # ì˜ˆ: 330 â†’ 33, 3
                for i in range(len(ddc_code) - 1, 0, -1):
                    fallback_patterns.append(ddc_code[:i])

            # í•˜ìœ„ ë¶„ë¥˜ íŒ¨í„´ (í˜„ì¬ ì½”ë“œë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ê²ƒ)
            fallback_patterns.append(f"{ddc_code}")  # ìê¸° ìì‹ ë„ í¬í•¨

            print(f"ğŸ”„ [DDC_FALLBACK] í´ë°± íŒ¨í„´ë“¤: {fallback_patterns}")

            all_results = []
            for pattern in fallback_patterns:
                # âœ… [ì„±ëŠ¥ ê°œì„ ] í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¡°íšŒ
                query = """
                    SELECT
                        identifier,
                        kdc,
                        ddc,
                        ksh,
                        ksh_korean,
                        ksh_labeled,
                        kdc_edition,
                        ddc_edition,
                        publication_year,
                        title,
                        source_file,
                        ? as fallback_level
                    FROM mapping_data
                    WHERE ddc LIKE ? AND ksh IS NOT NULL AND ksh != ''
                    LIMIT ?
                """
                level = fallback_patterns.index(pattern)
                df_pattern = pd.read_sql_query(
                    query,
                    conn,
                    params=(
                        level,
                        f"{pattern}%",
                        max_results // len(fallback_patterns) + 10,
                    ),
                )

                if not df_pattern.empty:
                    all_results.append(df_pattern)

                # ì¶©ë¶„í•œ ê²°ê³¼ê°€ ëª¨ì´ë©´ ì¤‘ë‹¨
                total_found = sum(len(df) for df in all_results)
                if total_found >= max_results:
                    break

            if all_results:
                combined = pd.concat(all_results, ignore_index=True)

                # ì¤‘ë³µ ì œê±° í›„ ì •ë ¬
                combined = combined.drop_duplicates(subset=["identifier"], keep="first")
                combined = combined.sort_values(
                    ["fallback_level", "publication_year"], ascending=[True, False]
                )
                combined = combined.drop("fallback_level", axis=1)

                result = combined.head(max_results)
                print(f"ğŸ”„ [DDC_FALLBACK] í´ë°± ê²€ìƒ‰ ê²°ê³¼: {len(result)}ê°œ")
                return result
            else:
                print(f"ğŸ”„ [DDC_FALLBACK] í´ë°± ê²€ìƒ‰ë„ ê²°ê³¼ ì—†ìŒ")
                return pd.DataFrame()

        except Exception as e:
            print(f"ì˜¤ë¥˜: DDC í´ë°± ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return pd.DataFrame()
        finally:
            if conn:
                conn.close()


    def get_all_ddc_labels_bulk(self, ddc_numbers: list[str]) -> dict:
        """
        âœ… [ì‹ ê·œ ìµœì í™”] DDC ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ëª¨ë“  ê´€ë ¨ ë ˆì´ë¸”ì„ ë‹¨ì¼ ì¿¼ë¦¬ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
        - ë°˜í™˜ê°’: {ddc_number: "label1 | label2 | ...", ...} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        """
        if not ddc_numbers:
            return {}

        # 1. ì¤‘ë³µëœ DDC ë²ˆí˜¸ ë° ê³µë°± ì œê±°
        unique_ddc_numbers = list(
            set(d.strip() for d in ddc_numbers if d and d.strip())
        )
        if not unique_ddc_numbers:
            return {}

        # 2. ë‹¨ì¼ SQL ì¿¼ë¦¬ ì‹¤í–‰ (IN ì—°ì‚°ì í™œìš©)
        # database_managerì— ì‹¤ì œ DBì— ì ‘ê·¼í•˜ëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.
        # ì´ ì¿¼ë¦¬ëŠ” (ddc, keyword, term_type) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
        all_labels_list = self.db_manager.get_all_ddc_keywords_by_numbers(
            unique_ddc_numbers
        )

        # 3. Pythonì—ì„œ ë°ì´í„° ì¬êµ¬ì„± (DB ë¶€í•˜ ìµœì†Œí™”)
        ddc_labels_map = {}
        # pref ë ˆì´ë¸”ì„ ìš°ì„ í•˜ê¸° ìœ„í•´ ë¨¼ì € ì •ë ¬
        all_labels_list.sort(key=lambda x: (x[0], 0 if x[2] == "pref" else 1, x[1]))

        for ddc, keyword, term_type in all_labels_list:
            if ddc not in ddc_labels_map:
                ddc_labels_map[ddc] = []

            label = f"{keyword}(pref)" if term_type == "pref" else keyword
            if label not in ddc_labels_map[ddc]:
                ddc_labels_map[ddc].append(label)

        # 4. ìµœì¢…ì ìœ¼ë¡œ '|'ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        result_map = {ddc: " | ".join(labels) for ddc, labels in ddc_labels_map.items()}
        return result_map


    def get_ddc_description_cached(self, ddc_code: str) -> str | None:
        """
        DDC ë²ˆí˜¸ë¡œ ìºì‹œì—ì„œ ì„¤ëª…(prefLabel, title ë“±)ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
        (database_manager.pyì—ì„œ ì´ê´€ë¨)
        """
        try:
            # âœ… [ìˆ˜ì •] ê°™ì€ í´ë˜ìŠ¤ ë‚´ì˜ get_dewey_by_notation ë©”ì„œë“œë¥¼ ì§ì ‘ í˜¸ì¶œí•©ë‹ˆë‹¤.
            cached_data = self.get_dewey_by_notation(ddc_code)
            if cached_data:
                try:
                    ddc_json = json.loads(cached_data)

                    # âœ… [ê°œì„ ] prefLabel ìš°ì„  ì¡°íšŒ, ì—†ìœ¼ë©´ title ì¡°íšŒ
                    label_data = ddc_json.get("prefLabel")
                    if isinstance(label_data, dict):
                        # ë‹¤êµ­ì–´ ì²˜ë¦¬ (en ìš°ì„ )
                        title = label_data.get(
                            "en", next(iter(label_data.values()), f"DDC {ddc_code}")
                        )
                    elif isinstance(label_data, str):
                        title = label_data
                    else:
                        title = ddc_json.get("title", f"DDC {ddc_code}")

                    return title
                except (json.JSONDecodeError, StopIteration) as e:
                    logger.warning(f"DDC {ddc_code} ìºì‹œ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                    return f"DDC {ddc_code} (ìºì‹œ íŒŒì‹± ì˜¤ë¥˜)"
            return None
        except Exception as e:
            logger.warning(f"DDC {ddc_code} ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None


    def get_ddc_labels(self, ddc_numbers: str) -> str:
        """
        DDC ë²ˆí˜¸(ë“¤)ì— ëŒ€í•œ ë ˆì´ë¸”ì„ ì¡°íšŒí•˜ì—¬ í¬ë§·íŒ…ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            ddc_numbers: DDC ë²ˆí˜¸ ë¬¸ìì—´ (ì˜ˆ: "320.011" ë˜ëŠ” "320.011 | 320.01")

        Returns:
            í¬ë§·íŒ…ëœ ë ˆì´ë¸” ë¬¸ìì—´
            - ë‹¨ì¼ DDC: "Male gods(pref); Male gods(alt); Gods--male(alt)"
            - ë³µìˆ˜ DDC: "202.113; Male gods(pref); ...; 202.114; Female goddesses(pref); ..."
            - ë°ì´í„° ì—†ìŒ: ""
        """
        if not ddc_numbers or not str(ddc_numbers).strip():
            return ""

        # -------------------
        # âœ… [ìˆ˜ì •] ì»´ë§ˆ(,) ë˜ëŠ” íŒŒì´í”„(|)ë¡œ êµ¬ë¶„ëœ ë³µìˆ˜ DDC ë²ˆí˜¸ ì²˜ë¦¬
        import re

        ddc_list = [
            ddc.strip() for ddc in re.split(r"[,|]", str(ddc_numbers)) if ddc.strip()
        ]
        # -------------------

        if not ddc_list:
            return ""

        conn = None
        try:
            conn = self.db_manager._get_dewey_connection()
            cursor = conn.cursor()

            result_parts = []

            for ddc in ddc_list:
                # ê° DDC ë²ˆí˜¸ì— ëŒ€í•œ ë ˆì´ë¸” ì¡°íšŒ
                cursor.execute(
                    """
                    SELECT keyword, term_type
                    FROM ddc_keyword
                    WHERE ddc = ?
                    ORDER BY
                        CASE term_type
                            WHEN 'pref' THEN 1
                            WHEN 'alt' THEN 2
                            ELSE 3
                        END,
                        keyword
                    """,
                    (ddc,),
                )

                rows = cursor.fetchall()

                if rows:
                    # ë³µìˆ˜ DDCì¸ ê²½ìš°ì—ë§Œ DDC ë²ˆí˜¸ë¥¼ ì•ì— ì¶”ê°€
                    if len(ddc_list) > 1:
                        result_parts.append(ddc)

                    # âœ… [ê°œì„ ] (pref)ë§Œ í‘œì‹œ, (alt)ëŠ” íƒœê·¸ ì œê±°
                    # prefê°€ ì—†ìœ¼ë©´ ëª¨ë‘ altë¡œ ê°„ì£¼
                    labels = []
                    for keyword, term_type in rows:
                        if term_type == "pref":
                            labels.append(f"{keyword}(pref)")
                        else:
                            # altëŠ” íƒœê·¸ ì—†ì´ keywordë§Œ í‘œì‹œ
                            labels.append(keyword)
                    result_parts.extend(labels)

            if not result_parts:
                return ""

            # -------------------
            # âœ… [ìˆ˜ì •] íŒŒì´í”„ë¡œ êµ¬ë¶„í•˜ì—¬ ë°˜í™˜
            return " | ".join(result_parts)
            # -------------------

        except Exception as e:
            logger.warning(f"DDC ë ˆì´ë¸” ì¡°íšŒ ì‹¤íŒ¨ ({ddc_numbers}): {e}")
            return ""
        finally:
            if conn:
                conn.close()


    def get_dewey_by_notation(self, ddc_code: str) -> str | None:
        """
        DDC ì½”ë“œë¡œ ìºì‹œì—ì„œ raw_jsonì„ ì¡°íšŒí•©ë‹ˆë‹¤.
        get_dewey_cache_entryì˜ ë˜í¼ í•¨ìˆ˜ì…ë‹ˆë‹¤.
        """
        entry = self.get_dewey_cache_entry(ddc_code)
        return entry[0] if entry else None


    def get_dewey_cache_entry(self, ddc_code: str) -> tuple | None:
        """
        DDC ì½”ë“œë¡œ ìºì‹œì—ì„œ raw_jsonê³¼ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì¼ì‹œë¥¼ í•¨ê»˜ ì¡°íšŒí•©ë‹ˆë‹¤.
        Returns:
            (raw_json, last_updated) íŠœí”Œ ë˜ëŠ” None
        """
        if not ddc_code:
            return None
        conn = None
        try:
            conn = self.db_manager._get_dewey_connection()
            cur = conn.cursor()
            cur.execute(
                """
                SELECT raw_json, last_updated
                  FROM dewey_cache
                 WHERE ddc_code = ?
              ORDER BY last_updated DESC
                 LIMIT 1
                """,
                (ddc_code,),
            )
            row = cur.fetchone()
            return (row["raw_json"], row["last_updated"]) if row else None
        except Exception as e:
            logger.error(f"ì˜¤ë¥˜: get_dewey_cache_entry ì‹¤íŒ¨: {e}")
            return None
        finally:
            if conn:
                conn.close()


    def get_dewey_from_cache(self, iri: str) -> str | None:
        """
        âœ… [ë™ì‹œì„± ê°œì„ ] DDC ì „ìš© DBì—ì„œ ìºì‹œ ì¡°íšŒ (ì½ê¸° ì „ìš©)
        íˆíŠ¸ ì¹´ìš´íŠ¸ëŠ” ë°°ì¹˜ ì—…ë°ì´íŠ¸ë¡œ ì²˜ë¦¬í•˜ì—¬ ë½ ì¶©ëŒ ë°©ì§€
        """
        conn = None
        try:
            conn = self.db_manager._get_dewey_connection()
            cursor = conn.cursor()

            # ìºì‹œ ì¡°íšŒ (ì½ê¸° ì „ìš© - UPDATE ì œê±°ë¡œ ë½ ì¶©ëŒ í•´ì†Œ)
            cursor.execute("SELECT raw_json FROM dewey_cache WHERE iri = ?", (iri,))
            result = cursor.fetchone()

            if result:
                # âœ… [ë™ì‹œì„± ê°œì„ ] íˆíŠ¸ ì¹´ìš´íŠ¸ë¥¼ ë©”ëª¨ë¦¬ì— ëˆ„ì ë§Œ í•˜ê³  ì¦‰ì‹œ ë°˜í™˜
                # ì‹¤ì œ DB ì—…ë°ì´íŠ¸ëŠ” 3ì´ˆë§ˆë‹¤ ë°°ì¹˜ë¡œ ì²˜ë¦¬ (database_managerì—ì„œ)
                self.db_manager._schedule_hit_count_update(iri)
                return result[0]

            return None

        except Exception as e:
            print(f"ê²½ê³ : DDC ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
        finally:
            if conn:
                conn.close()


    def get_multiple_ddcs_descriptions(self, ddc_list: list) -> dict:
        """
        ì—¬ëŸ¬ DDC ë²ˆí˜¸ì˜ ì„¤ëª…ì„ ìºì‹œì—ì„œ í•œ ë²ˆì— ì¡°íšŒí•©ë‹ˆë‹¤.
        (database_manager.pyì—ì„œ ì´ê´€ë¨)
        """
        descriptions = {}

        if not ddc_list:
            return descriptions

        # DDC ì½”ë“œ ì •ë¦¬ ë° ì¤‘ë³µ ì œê±°
        clean_ddcs = []
        for ddc in ddc_list:
            if ddc and str(ddc).strip():
                clean_ddc = str(ddc).strip()
                # DDC í˜•ì‹ ê²€ì¦ (ìˆ«ìì™€ ì ë§Œ í¬í•¨)
                if re.match(r"^\d+\.?\d*$", clean_ddc):
                    clean_ddcs.append(clean_ddc)

        unique_ddcs = list(set(clean_ddcs))

        # ê° DDCì— ëŒ€í•´ ìºì‹œì—ì„œ ì¡°íšŒ
        for ddc in unique_ddcs:
            try:
                # âœ… [ìˆ˜ì •] ì´ì œ ê°™ì€ í´ë˜ìŠ¤ ë‚´ì˜ ë©”ì„œë“œë¥¼ ì •ìƒì ìœ¼ë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤.
                desc = self.get_ddc_description_cached(ddc)
                if desc:
                    descriptions[ddc] = desc
                else:
                    # ìºì‹œì— ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
                    descriptions[ddc] = f"DDC {ddc} (ì˜ë¯¸ ì¡°íšŒ í•„ìš”)"

            except Exception as e:
                logger.warning(f"DDC {ddc} ì¡°íšŒ ì‹¤íŒ¨: {e}")
                descriptions[ddc] = f"DDC {ddc} (ì¡°íšŒ ì‹¤íŒ¨)"

        logger.info(f"ğŸ’¾ DDC ì„¤ëª… ì¼ê´„ ì¡°íšŒ ì™„ë£Œ: {len(descriptions)}ê°œ")
        return descriptions

    # --- ìš©ì–´ì§‘(Glossary) ë°ì´í„° ê´€ë ¨ í•¨ìˆ˜ ---

    # ========================================
    # DDC ìºì‹œ ê´€ë ¨ í•¨ìˆ˜ë“¤
    # ========================================


    def save_dewey_to_cache(self, iri: str, ddc_code: str, raw_json: str):
        """
        âœ… [ë™ì‹œì„± ê°œì„ ] DDC ë°ì´í„°ë¥¼ ì“°ê¸° íë¥¼ í†µí•´ ì €ì¥í•©ë‹ˆë‹¤.
        ì—¬ëŸ¬ ìŠ¤ë ˆë“œì—ì„œ ë™ì‹œì— í˜¸ì¶œí•´ë„ ì•ˆì „í•©ë‹ˆë‹¤ (database is locked ì˜¤ë¥˜ í•´ê²°).
        """
        try:
            # 1. ë©”ì¸ ìºì‹œ ì €ì¥ì€ íë¥¼ í†µí•´ ë¹„ë™ê¸° ì²˜ë¦¬
            self.db_manager.enqueue_dewey_cache_write(iri, ddc_code, raw_json)

            # 2. í‚¤ì›Œë“œ ì¶”ì¶œì€ ë³„ë„ë¡œ ì¦‰ì‹œ ì²˜ë¦¬ (ë¹ˆë„ê°€ ë‚®ê³  ë…ë¦½ì )
            # âœ… Negative Cache ({"exists": false})ëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ ìŠ¤í‚µ
            try:
                payload = json.loads(raw_json)
                if payload.get("exists") is not False:
                    # Positive Cacheë§Œ í‚¤ì›Œë“œ ì¶”ì¶œ
                    self._save_keywords_separately(iri, ddc_code, raw_json)
            except (json.JSONDecodeError, TypeError):
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„
                self._save_keywords_separately(iri, ddc_code, raw_json)

            # âœ… ì„±ê³µ ë¡œê·¸ (ì•± í™”ë©´ì—ë„ í‘œì‹œ)
            logger.info(f"âœ… DDC ìºì‹œ ì €ì¥ ìš”ì²­: {ddc_code} (íì— ì¶”ê°€ë¨)")

        except Exception as e:
            error_msg = f"ê²½ê³ : DDC ìºì‹œ ì €ì¥ ì‹¤íŒ¨ (IRI: {iri}): {e}"
            logger.warning(error_msg)


    def search_ddc_by_keyword(
        self, keyword: str, pref_only: bool = False, limit: int = 20
    ) -> list:
        """
        [í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰] SQL(FTS)ì˜ ì •í™•ì„±ê³¼ ë²¡í„° ê²€ìƒ‰ì˜ ì˜ë¯¸ í™•ì¥ì„±ì„ ê²°í•©í•©ë‹ˆë‹¤.
        """
        # --- 1ë‹¨ê³„: SQL(FTS) ê²€ìƒ‰ (ì •í™•ì„± ë‹´ë‹¹) ---
        sql_results = self._search_ddc_by_sql_fts(keyword, pref_only, limit)

        # --- 2ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ (ì˜ë¯¸ í™•ì¥ ë‹´ë‹¹) ---
        vector_results = []
        if hasattr(self.db_manager, "search_ddc_by_vector"):
             vector_results = self.db_manager.search_ddc_by_vector(keyword, top_k=limit)
        
        # --- 3ë‹¨ê³„: ê²°ê³¼ ë³‘í•© ë° ìˆœìœ„ ì¬ì¡°ì • ---
        if not sql_results and not vector_results:
            return []

        # DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì‰½ê²Œ ì²˜ë¦¬
        df_sql = pd.DataFrame(sql_results)
        df_vec = pd.DataFrame(vector_results)

        # SQL ê²°ê³¼ì— ìµœê³  ì ìˆ˜(2.0)ì™€ íƒ€ì… ë¶€ì—¬
        if not df_sql.empty:
            df_sql['score'] = 2.0 
            df_sql['match_type'] = 'exact'

        # ë²¡í„° ê²°ê³¼ì—ëŠ” ìœ ì‚¬ë„ ì ìˆ˜ì™€ íƒ€ì… ë¶€ì—¬ (ë²¡í„° ì ìˆ˜ëŠ” 0~1 ì‚¬ì´)
        if not df_vec.empty:
            df_vec.rename(columns={'distance': 'score'}, inplace=True)
            df_vec['match_type'] = 'semantic'

        # ë‘ ê²°ê³¼ ë³‘í•©
        df_combined = pd.concat([df_sql, df_vec], ignore_index=True)
        
        # 'ddc'ì™€ 'keyword' ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (SQL ê²°ê³¼ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ìœ ì§€)
        df_combined.drop_duplicates(subset=['ddc', 'keyword'], keep='first', inplace=True)
        
        # ìµœì¢… ì •ë ¬: score ë†’ì€ ìˆœ (ì •í™• ì¼ì¹˜ > ì˜ë¯¸ ìœ ì‚¬)
        df_combined.sort_values(by=['score'], ascending=False, inplace=True)
        
        return df_combined.head(limit).to_dict('records')


    def search_ddc_by_multiple_keywords(
        self, keywords: str, pref_only: bool = False, max_results_per_level: int = 3
    ) -> list:
        """
        ë³µìˆ˜ í‚¤ì›Œë“œ(ì½¤ë§ˆ êµ¬ë¶„)ë¡œ DDC Cache DBë¥¼ ê²€ìƒ‰í•˜ì—¬ DDC ë¹ˆë„ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ê²°ê³¼ ë°˜í™˜

        Args:
            keywords: ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ ë¬¸ìì—´ (ì˜ˆ: "artificial intelligence, machine learning, neural networks")
            pref_only: Trueë©´ ìš°ì„ ì–´(prefLabel)ë§Œ ê²€ìƒ‰, Falseë©´ ë™ì˜ì–´(altLabel) í¬í•¨ ì „ì²´ ê²€ìƒ‰
            max_results_per_level: ìµœì¢…ì ìœ¼ë¡œ ë°˜í™˜í•  ìƒìœ„ DDC ìˆœìœ„ ê°œìˆ˜ (ê¸°ë³¸ê°’ 3)

        Returns:
            DDC ë¹ˆë„ ìˆœìœ¼ë¡œ ì •ë ¬ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            [{"ddc": "006.3", "count": 15, "keyword": "artificial intelligence", "term_type": "pref"}, ...]
        """
        # --- 1. ì…ë ¥ ìœ íš¨ì„± ê²€ì‚¬ ---
        # keywords ë¬¸ìì—´ì´ ë¹„ì–´ìˆê±°ë‚˜ ê³µë°±ë§Œ ìˆëŠ” ê²½ìš°, ë¶ˆí•„ìš”í•œ ì‘ì—…ì„ ë§‰ê¸° ìœ„í•´ ì¦‰ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        if not keywords or not keywords.strip():
            return []

        # --- 2. ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬ ---
        # ì…ë ¥ëœ ë¬¸ìì—´ì„ ì½¤ë§ˆ(,) ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ê° í‚¤ì›Œë“œì˜ ì•ë’¤ ê³µë°±ì„ ì œê±°í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤.
        keyword_list = [kw.strip() for kw in keywords.split(",") if kw.strip()]
        # ì „ì²˜ë¦¬ í›„ ë‚¨ì€ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
        if not keyword_list:
            return []

        # --- 3. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ---
        conn = None # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê°ì²´ë¥¼ ë‹´ì„ ë³€ìˆ˜ ì´ˆê¸°í™”
        try:
            # DatabaseManagerë¥¼ í†µí•´ DDC Cache DBì— ëŒ€í•œ ì—°ê²°ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            conn = self.db_manager._get_dewey_connection()
            cursor = conn.cursor()

            # --- 4. DDC í›„ë³´êµ° ì§‘ê³„ ---
            # ê° DDC ë²ˆí˜¸ê°€ ëª‡ ë²ˆì´ë‚˜ ê²€ìƒ‰ ê²°ê³¼ì— ë“±ì¥í–ˆëŠ”ì§€(ë¹ˆë„ìˆ˜)ì™€ ê´€ë ¨ ì •ë³´ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤.
            # êµ¬ì¡° ì˜ˆì‹œ: {'006.3': {'count': 2, 'keywords': {'AI', 'ML'}, ...}}
            ddc_aggregation = {}

            # pref_only í”Œë˜ê·¸ì— ë”°ë¼ SQL ì¿¼ë¦¬ì— ì¶”ê°€í•  í•„í„° ì¡°ê±´ì„ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
            # Trueì´ë©´ 'pref' íƒ€ì…ì˜ ìš©ì–´ë§Œ ê²€ìƒ‰ ëŒ€ìƒì— í¬í•¨ì‹œí‚µë‹ˆë‹¤.
            term_filter = "AND term_type = 'pref'" if pref_only else ""

            # --- 5. ê° í‚¤ì›Œë“œë³„ ìˆœì°¨ ê²€ìƒ‰ ë° ê²°ê³¼ ëˆ„ì  ---
            # ë¶„ë¦¬ëœ ê° í‚¤ì›Œë“œì— ëŒ€í•´ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ì„ ë°˜ë³µ ì‹¤í–‰í•©ë‹ˆë‹¤.
            for keyword in keyword_list:
                
                # FTS5(Full-Text Search)ëŠ” ê´„í˜¸ë‚˜ í•˜ì´í”ˆ ê°™ì€ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì—°ì‚°ìë¡œ ì˜¤ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                # ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´, ì•ŒíŒŒë²³, ìˆ«ì, ê³µë°±ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤.
                # ì˜ˆ: "Unicode (Character set)" -> "Unicode  Character set"
                sanitized_keyword = re.sub(r"[^\w\s]", " ", keyword)
                # ì—¬ëŸ¬ ê°œì˜ ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ í•©ì¹©ë‹ˆë‹¤.
                # ì˜ˆ: "Unicode  Character set" -> "Unicode Character set"
                sanitized_keyword = " ".join(sanitized_keyword.split())

                # ddc_keyword_fts í…Œì´ë¸”ì—ì„œ ì •ì œëœ í‚¤ì›Œë“œë¡œ FTS ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                # FTSì˜ ìˆœìœ„(rank)ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ 100ê°œ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                cursor.execute(
                    f"""
                    SELECT ddc, keyword, term_type
                    FROM ddc_keyword_fts
                    WHERE keyword MATCH ? {term_filter}
                    ORDER BY rank
                    LIMIT 100
                    """,
                    (sanitized_keyword,),
                )
                
                # --- 6. ê²€ìƒ‰ ê²°ê³¼ ì§‘ê³„ (Aggregation) ---
                # í•œ í‚¤ì›Œë“œì— ëŒ€í•œ ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìˆœíšŒí•˜ë©° ddc_aggregation ë”•ì…”ë„ˆë¦¬ì— ëˆ„ì í•©ë‹ˆë‹¤.
                for row in cursor.fetchall():
                    ddc, matched_keyword, term_type = row

                    # ì´ DDC ë²ˆí˜¸ê°€ ì²˜ìŒ ë°œê²¬ëœ ê²½ìš°, ì§‘ê³„ë¥¼ ìœ„í•œ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
                    if ddc not in ddc_aggregation:
                        ddc_aggregation[ddc] = {
                            "count": 0,          # ì´ DDCê°€ ëª‡ ë²ˆ ë“±ì¥í–ˆëŠ”ì§€ ì¹´ìš´íŠ¸
                            "keywords": [],      # ì–´ë–¤ í‚¤ì›Œë“œë“¤ê³¼ ë§¤ì¹­ë˜ì—ˆëŠ”ì§€ ëª©ë¡
                            "term_types": set(), # ë§¤ì¹­ëœ ìš©ì–´ì˜ íƒ€ì… (pref, alt ë“±)
                            "original_keyword": keyword, # ì´ DDCë¥¼ ì²˜ìŒ ì°¾ê²Œ í•œ ì›ë³¸ ê²€ìƒ‰ì–´
                        }

                    # í˜„ì¬ DDCì˜ ë“±ì¥ íšŸìˆ˜(ë¹ˆë„)ë¥¼ 1 ì¦ê°€ì‹œí‚µë‹ˆë‹¤.
                    ddc_aggregation[ddc]["count"] += 1
                    # ë§¤ì¹­ëœ DB í‚¤ì›Œë“œê°€ ëª©ë¡ì— ì—†ë‹¤ë©´ ì¤‘ë³µì„ í”¼í•´ ì¶”ê°€í•©ë‹ˆë‹¤.
                    if matched_keyword not in ddc_aggregation[ddc]["keywords"]:
                        ddc_aggregation[ddc]["keywords"].append(matched_keyword)
                    # ë§¤ì¹­ëœ ìš©ì–´ íƒ€ì…ì„ setì— ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ì—†ì´ ì €ì¥í•©ë‹ˆë‹¤. (ì˜ˆ: {'pref', 'alt'})
                    ddc_aggregation[ddc]["term_types"].add(term_type)

            # --- 7. ê²°ê³¼ ì •ë ¬ ë° í•„í„°ë§ ---
            # ì§‘ê³„ëœ DDC ëª©ë¡ì„ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì •ë ¬í•©ë‹ˆë‹¤.
            # 1ìˆœìœ„: ë“±ì¥ íšŸìˆ˜(count)ê°€ ë§ì€ ìˆœì„œ (ë‚´ë¦¼ì°¨ìˆœ)
            # 2ìˆœìœ„: íšŸìˆ˜ê°€ ê°™ë‹¤ë©´ DDC ë²ˆí˜¸ê°€ ë‚®ì€ ìˆœì„œ (ì˜¤ë¦„ì°¨ìˆœ, ì¼ê´€ëœ ì •ë ¬ì„ ìœ„í•´)
            # ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ìê°€ ìš”ì²­í•œ ê°œìˆ˜(max_results_per_level)ë§Œí¼ë§Œ ì˜ë¼ëƒ…ë‹ˆë‹¤.
            sorted_ddcs = sorted(
                ddc_aggregation.items(),
                key=lambda x: (-x[1]["count"], x[0]),
            )[:max_results_per_level]

            # --- 8. ìµœì¢… ê²°ê³¼ í¬ë§·íŒ… ---
            # ì •ë ¬ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ìµœì¢… ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ê°€ê³µí•©ë‹ˆë‹¤.
            results = []
            for ddc, info in sorted_ddcs:
                results.append(
                    {
                        "ddc": ddc,
                        "ddc_count": info["count"],
                        "keyword": ", ".join(info["keywords"][:3]), # ë§¤ì¹­ëœ í‚¤ì›Œë“œëŠ” ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ í‘œì‹œ
                        "term_type": ", ".join(sorted(info["term_types"])),
                        "search_keywords": keywords, # ì‚¬ìš©ìê°€ ì…ë ¥í–ˆë˜ ì›ë³¸ ê²€ìƒ‰ì–´ ì „ì²´
                    }
                )

            return results

        except Exception as e:
            # í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì–´ë–¤ ì¢…ë¥˜ì˜ ì˜¤ë¥˜ë¼ë„ ë°œìƒí•˜ë©´ ë¡œê·¸ë¥¼ ë‚¨ê¸°ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
            logger.error(f"ë³µí•© í‚¤ì›Œë“œ DDC ê²€ìƒ‰ ì‹¤íŒ¨ ({keywords}): {e}")
            return []
        finally:
            # try ë¸”ë¡ì˜ ì½”ë“œê°€ ì„±ê³µí•˜ë“  ì‹¤íŒ¨í•˜ë“  ê´€ê³„ì—†ì´ í•­ìƒ ì‹¤í–‰ë©ë‹ˆë‹¤.
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—´ë ¤ ìˆë‹¤ë©´, ë¦¬ì†ŒìŠ¤ë¥¼ í•´ì œí•˜ê¸° ìœ„í•´ ì•ˆì „í•˜ê²Œ ë‹«ìŠµë‹ˆë‹¤.
            if conn:
                conn.close()

